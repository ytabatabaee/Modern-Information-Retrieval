[
{"id": "The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf", "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "authors": ["Jonathan Frankle", "Michael Carbin"], "date": "2018", "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. \nThis paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. \nThe lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.", "references": ["https://www.semanticscholar.org/paper/Successfully-Applying-the-Stabilized-Lottery-Ticket-Brix-Bahar/27a04669fef49981b91a6a148e549360ad309761", "https://www.semanticscholar.org/paper/Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/308e0a4523537e9fdc25a6afff67f9d214738d76", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Neural-Rejuvenation%3A-Improving-Deep-Network-by-Qiao-Lin/3ba9fdcd0d10f2a130fcbd678cebcbb5e8c6bd5f", "https://www.semanticscholar.org/paper/SiPPing-Neural-Networks%3A-Sensitivity-informed-of-Baykal-Liebenwein/65dca1da228dd3cedc4431d1caa6c30038516966", "https://www.semanticscholar.org/paper/SuperNet-An-efficient-method-of-neural-networks-Bukowski-Dzwinel/dfa8e748c16995f88134e6df44b1a6817d2bd6cd", "https://www.semanticscholar.org/paper/Learning-Sparse-Networks-Using-Targeted-Dropout-Gomez-Zhang/13de3c06ef6dac1c296ada45df2be590f843edb7", "https://www.semanticscholar.org/paper/Targeted-Dropout-Talwar/ea19fee3ecce6f1b17dfde38bce1ccceaeb5befb", "https://www.semanticscholar.org/paper/Over-parameterization-as-a-Catalyst-for-Better-of-Tian/7384fe1e1c5b760e5af2b2864c3c0dc6bee115aa", "https://www.semanticscholar.org/paper/Dynamic-parameter-reallocation-improves-of-deep-Mostafa-Wang/888753443f67e6c6b5ba90e0d239b341d97e3ba7", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "https://www.semanticscholar.org/paper/Understanding-Dropout-Baldi-Sadowski/cc46229a7c47f485e090857cbab6e6bf68c09811", "https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/Diversity-Networks%3A-Neural-Network-Compression-Mariet-Sra/2dfef5635c8c44431ca3576081e6cfe6d65d4862", "https://www.semanticscholar.org/paper/A-Deep-Neural-Network-Compression-Pipeline%3A-Huffman-Han-Mao/397de65a9a815ec39b3704a79341d687205bc80a", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/Optimal-Brain-Surgeon-and-general-network-pruning-Hassibi-Stork/e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724"]},
{"id": "Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "date": "2017", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "references": ["https://www.semanticscholar.org/paper/How-Much-Attention-Do-You-Need-A-Granular-Analysis-Domhan/cf0545d9f171c8e2d88aed13cd7ad60ff2ab8fb5", "https://www.semanticscholar.org/paper/A-Simple-but-Effective-Way-to-Improve-the-of-in-Liu-Zhang/bf9c369dc99076fc8bbab4436821357b752fda33", "https://www.semanticscholar.org/paper/Weighted-Transformer-Network-for-Machine-Ahmed-Keskar/3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "https://www.semanticscholar.org/paper/Transformer%2B%2B-Thapak-Hore/329484f2e34e7a58ba8aeac000ed57c6702942ee", "https://www.semanticscholar.org/paper/Joint-Source-Target-Self-Attention-with-Locality-Fonollosa-Casas/b0f0a5a21619d70748a4dc007983cc111f1b301e", "https://www.semanticscholar.org/paper/Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "https://www.semanticscholar.org/paper/Temporal-Convolutional-Attention-based-Network-For-Hao-Wang/80d15b5e95b27e224838914ad06a81b0567e6e0f", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Deep-Attention-Zhang-Xiong/ae5a56effb19dc0137c9071e5bdc14b638eb2e10", "https://www.semanticscholar.org/paper/Gated-Self-attentive-Encoder-for-Neural-Machine-Wei-Hu/237ab961a29a17f6952f4dea3a0647b2fb6d6090", "https://www.semanticscholar.org/paper/An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann/94238dead40b12735d79ed63e29ead70730261a2", "https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-in-Linear-Time-Kalchbrenner-Espeholt/98445f4172659ec5e891e031d8202c102135c644", "https://www.semanticscholar.org/paper/A-Deep-Reinforced-Model-for-Abstractive-Paulus-Xiong/032274e57f7d8b456bd255fe76b909b2c1d7458e", "https://www.semanticscholar.org/paper/Can-Active-Memory-Replace-Attention-Kaiser-Bengio/735d547fc75e0772d2a78c46a1cc5fad7da1474c", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Structured-Attention-Networks-Kim-Denton/13d9323a8716131911bfda048a40e2cde1a76a46", "https://www.semanticscholar.org/paper/Multi-task-Sequence-to-Sequence-Learning-Luong-Le/d76c07211479e233f7c6a6f32d5346c983c5598f", "https://www.semanticscholar.org/paper/Convolutional-Sequence-to-Sequence-Learning-Gehring-Auli/43428880d75b3a14257c3ee9bda054e61eb869c0", "https://www.semanticscholar.org/paper/Outrageously-Large-Neural-Networks%3A-The-Layer-Shazeer-Mirhoseini/510e26733aaff585d65701b9f1be7ca9d5afc586"]},
{"id": "BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "date": "2019", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "references": ["https://www.semanticscholar.org/paper/Extending-Answer-Prediction-for-Deep-Bi-directional-Dun/224060b48c1576e34ba9a7ca28424cadd9d27318", "https://www.semanticscholar.org/paper/Syntax-Infused-Transformer-and-BERT-models-for-and-Sundararaman-Subramanian/beb91a773677872fc21f08722bdcc737bf5917b5", "https://www.semanticscholar.org/paper/BERT-for-Question-Answering-on-SQuAD-2-.-0-Zhang/8617b501fedf65efaf82c3f911fe490407ba3650", "https://www.semanticscholar.org/paper/TRANS-BLSTM%3A-Transformer-with-Bidirectional-LSTM-Huang-Xu/c79a8fd667f59e6f1ca9d54afc34f792e9079c7e", "https://www.semanticscholar.org/paper/Improving-SQUAD-2-.-0-Performance-using-BERT-%2B-X-Takeuchi/82fd4c8e171ad03551161c6eaa9d8214b73d6128", "https://www.semanticscholar.org/paper/BERTSel%3A-Answer-Selection-with-Pre-trained-Models-Li-Yu/354ec86839539390a148ed41054eb68e0b8caa85", "https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7", "https://www.semanticscholar.org/paper/Unified-Language-Model-Pre-training-for-Natural-and-Dong-Yang/1c71771c701aadfd72c5866170a9f5d71464bb88", "https://www.semanticscholar.org/paper/Hierarchical-Transformers-for-Long-Document-Pappagari-%C5%BBelasko/46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948", "https://www.semanticscholar.org/paper/Incorporating-BERT-into-Neural-Machine-Translation-Zhu-Xia/34591cdb1e96e4ebe4bec38da9e88718f5c3d2df", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequence-Modeling-with-Cross-View-Clark-Luong/0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59", "https://www.semanticscholar.org/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101", "https://www.semanticscholar.org/paper/GLUE%3A-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh/93b8da28d006415866bf48f9a6e06b5242129195", "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600", "https://www.semanticscholar.org/paper/Character-Level-Language-Modeling-with-Deeper-Al-Rfou-Choe/b9de9599d7241459db9213b5cdd7059696f5ef8d", "https://www.semanticscholar.org/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050", "https://www.semanticscholar.org/paper/Skip-Thought-Vectors-Kiros-Zhu/6e795c6e9916174ae12349f5dc3f516570c17ce8"]},
{"id": "Dynamic-parameter-reallocation-improves-of-deep-Mostafa-Wang/888753443f67e6c6b5ba90e0d239b341d97e3ba7", "title": "Dynamic parameter reallocation improves trainability of deep convolutional networks", "authors": ["Hesham Mostafa", "Xuxu Wang"], "date": "2018", "abstract": "Network pruning has emerged as a powerful technique for reducing the size of deep neural networks. Pruning uncovers high-performance subnetworks by taking a trained dense network and gradually removing unimportant connections. Recently, alternative techniques have emerged for training sparse networks directly without having to train a large dense model beforehand, thereby achieving small memory footprints during both training and inference. These techniques are based on dynamic reallocation of non-zero parameters during training. Thus, they are in effect executing a training-time search for the optimal subnetwork. We investigate a most recent one of these techniques and conduct additional experiments to elucidate its behavior in training sparse deep convolutional networks. Dynamic parameter reallocation converges early during training to a highly trainable subnetwork. We show that neither the structure, nor the initialization of the discovered highperformance subnetwork is sufficient to explain its good performance. Rather, it is the dynamics of parameter reallocation that are responsible for successful learning. Dynamic parameter reallocation thus improves the trainability of deep convolutional networks, playing a similar role as overparameterization, without incurring the memory and computational cost of the latter.", "references": ["https://www.semanticscholar.org/paper/Stabilizing-the-Lottery-Ticket-Hypothesis-Frankle-Dziugaite/075da5ebbb890924267b4b163292ad21d0b100a0", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis-at-Scale-Frankle-Dziugaite/03e0cbeb4604262446a97cb381874c7de1cffea2", "https://www.semanticscholar.org/paper/Parameter-Efficient-Training-of-Deep-Convolutional-Mostafa-Wang/c703e42ac401ad734f440d56f6e19e6b2af86a60", "https://www.semanticscholar.org/paper/Deep-Rewiring%3A-Training-very-sparse-deep-networks-Bellec-Kappel/ccee800244908d2960830967e70ead7dd8266f7a", "https://www.semanticscholar.org/paper/Wide-Residual-Networks-Zagoruyko-Komodakis/1c4e9156ca07705531e45960b7a919dc473abb51", "https://www.semanticscholar.org/paper/Scalable-training-of-artificial-neural-networks-by-Mocanu-Mocanu/6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "https://www.semanticscholar.org/paper/To-prune%2C-or-not-to-prune%3A-exploring-the-efficacy-Zhu-Gupta/3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf", "https://www.semanticscholar.org/paper/Understanding-and-Simplifying-One-Shot-Architecture-Bender-Kindermans/45b7b5514a65126d39a51d5a68da53e7aa244c1f", "https://www.semanticscholar.org/paper/Neural-Architecture-Search%3A-A-Survey-Elsken-Metzen/114a32bc872f160b58f503aca13f887556b5006e"]},
{"id": "SiPPing-Neural-Networks%3A-Sensitivity-informed-of-Baykal-Liebenwein/65dca1da228dd3cedc4431d1caa6c30038516966", "title": "SiPPing Neural Networks: Sensitivity-informed Provable Pruning of Neural Networks", "authors": ["Cenk Baykal", "Lucas Liebenwein", "Igor Gilitschenski", "Dan Feldman", "Daniela Rus"], "date": "2019", "abstract": "We introduce a pruning algorithm that provably sparsifies the parameters of a trained model in a way that approximately preserves the model's predictive accuracy. Our algorithm uses a small batch of input points to construct a data-informed importance sampling distribution over the network's parameters, and adaptively mixes a sampling-based and deterministic pruning procedure to discard redundant weights. Our pruning method is simultaneously computationally efficient, provably accurate, and broadly applicable to various network architectures and data distributions. Our empirical comparisons show that our algorithm reliably generates highly compressed networks that incur minimal loss in performance relative to that of the original network. We present experimental results that demonstrate our algorithm's potential to unearth essential network connections that can be trained successfully in isolation, which may be of independent interest.", "references": ["https://www.semanticscholar.org/paper/Provable-Filter-Pruning-for-Efficient-Neural-Liebenwein-Baykal/9085f9462008336ec20295fbf241eafce98a55b6", "https://www.semanticscholar.org/paper/Good-Subnetworks-Provably-Exist%3A-Pruning-via-Greedy-Ye-Gong/53f5ab272f9d10047ece88f2cc957160e47f5ea9", "https://www.semanticscholar.org/paper/Data-Dependent-Coresets-for-Compressing-Neural-with-Baykal-Liebenwein/f61fc6c49d3b22f297b7ccb72bbfe6f7c1344e78", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Net-Trim%3A-Convex-Pruning-of-Deep-Neural-Networks-Aghasi-Abdi/6e1ee3bef407bf5562bedbc56e719e0829fa0a64", "https://www.semanticscholar.org/paper/Runtime-Neural-Pruning-Lin-Rao/88cd4209db62a34d9cba0b9cbe9d45d1e57d21e5", "https://www.semanticscholar.org/paper/SNIP%3A-Single-shot-Network-Pruning-based-on-Lee-Ajanthan/cf440ccce4a7a8681e238b4f26d5b95109add55d", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Finding-Sparse%2C-Frankle-Carbin/21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "https://www.semanticscholar.org/paper/An-Exploration-of-Parameter-Redundancy-in-Deep-with-Cheng-Yu/5934400081d9541339da0f16d2613263f1a4c2a2", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf", "https://www.semanticscholar.org/paper/Compression-aware-Training-of-Deep-Networks-Alvarez-Salzmann/45a154f8be8ec31821a0e409d4b69635670a2e1e", "https://www.semanticscholar.org/paper/Revisiting-hard-thresholding-for-DNN-pruning-Pitas-Davies/f2518319de450a08ab36ca2c03af7cbec26f6fdc"]},
{"id": "SuperNet-An-efficient-method-of-neural-networks-Bukowski-Dzwinel/dfa8e748c16995f88134e6df44b1a6817d2bd6cd", "title": "SuperNet - An efficient method of neural networks ensembling", "authors": ["Ludwik Bukowski", "Witold Dzwinel"], "date": "2020", "abstract": "The main flaw of neural network ensembling is that it is exceptionally demanding computationally, especially, if the individual sub-models are large neural networks, which must be trained separately. Having in mind that modern DNNs can be very accurate, they are already the huge ensembles of simple classifiers, and that one can construct more thrifty compressed neural net of a similar performance for any ensemble, the idea of designing the expensive SuperNets can be questionable. The widespread belief that ensembling increases the prediction time, makes it not attractive and can be the reason that the main stream of ML research is directed towards developing better loss functions and learning strategies for more advanced and efficient neural networks. On the other hand, all these factors make the architectures more complex what may lead to overfitting and high computational complexity, that is, to the same flaws for which the highly parametrized SuperNets ensembles are blamed. The goal of the master thesis is to speed up the execution time required for ensemble generation. Instead of training K inaccurate sub-models, each of them can represent various phases of training (representing various local minima of the loss function) of a single DNN [Huang et al., 2017; Gripov et al., 2018]. Thus, the computational performance of the SuperNet can be comparable to the maximum CPU time spent on training its single sub-model, plus usually much shorter CPU time required for training the SuperNet coupling factors.", "references": ["https://www.semanticscholar.org/paper/Snapshot-Ensembles%3A-Train-1%2C-get-M-for-free-Huang-Li/b134d0911e2e13ac169ffa5f478a39e6ef77869a", "https://www.semanticscholar.org/paper/Loss-Surfaces%2C-Mode-Connectivity%2C-and-Fast-of-DNNs-Garipov-Izmailov/f6195d8dc6aad8231e97b563246f2585842bc68b", "https://www.semanticscholar.org/paper/Residual-Networks-are-Exponential-Ensembles-of-Veit-Wilber/ec6bb39f18f20dbb504790a5f7089e40de33b169", "https://www.semanticscholar.org/paper/Rethinking-the-Inception-Architecture-for-Computer-Szegedy-Vanhoucke/23ffaa0fe06eae05817f527a47ac3291077f9e58", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Deep-Networks-with-Stochastic-Depth-Huang-Sun/51db1f3c8dfc7d4077da39c96bb90a6358128111", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/026ecf916023e13191331a354271b7f9b86e50a1", "https://www.semanticscholar.org/paper/The-Marginal-Value-of-Adaptive-Gradient-Methods-in-Wilson-Roelofs/1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c", "https://www.semanticscholar.org/paper/Adaptive-dropout-for-training-deep-neural-networks-Ba-Frey/f9f19bee621faf46f90b023f8de8248b57becbc4", "https://www.semanticscholar.org/paper/Wide-Residual-Networks-Zagoruyko-Komodakis/1c4e9156ca07705531e45960b7a919dc473abb51"]},
{"id": "Over-parameterization-as-a-Catalyst-for-Better-of-Tian/7384fe1e1c5b760e5af2b2864c3c0dc6bee115aa", "title": "Over-parameterization as a Catalyst for Better Generalization of Deep ReLU network", "authors": ["Yuandong Tian"], "date": "2019", "abstract": "To analyze deep ReLU network, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called \\emph{interpolation setting}, there exists many-to-one \\emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out weights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. Our analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fewer iterations. Both improve generalization. Experiments justify our finding. The code is released in this https URL.", "references": ["https://www.semanticscholar.org/paper/Sharp-Rate-of-Convergence-for-Deep-Neural-Network-Hu-Shang/01461fb1e5282348b91b5b6dbd292226388475cf", "https://www.semanticscholar.org/paper/Optimal-Rate-of-Convergence-for-Deep-Neural-Network-Hu-Shang/79750196c33f5cecf78cc9a4761d3390387007fc", "https://www.semanticscholar.org/paper/Implicit-Regularization-of-Normalization-Methods-Wu-Dobriban/90750b4543420cb46d7b97a60ca979bcd909e597", "https://www.semanticscholar.org/paper/Recovery-and-Generalization-in-Over-Realized-Sulam-You/9d6eb01364c80d4f3cc49ae7e2a2a8ef1afc99c0", "https://www.semanticscholar.org/paper/The-Most-Used-Activation-Functions%3A-Classic-Versus-Mercioni-Holban/78339ace7698c3911e3b18c3e8208c3e0f11c792", "https://www.semanticscholar.org/paper/Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/308e0a4523537e9fdc25a6afff67f9d214738d76", "https://www.semanticscholar.org/paper/A-Convergence-Theory-for-Deep-Learning-via-Allen-Zhu-Li/42ec3db12a2e4628885451b13035c2e975220a25", "https://www.semanticscholar.org/paper/Dynamics-of-stochastic-gradient-descent-for-neural-Goldt-Advani/0854a88273328a7c732838b8cefc9af1f9ef0d5d", "https://www.semanticscholar.org/paper/Gradient-descent-optimizes-over-parameterized-deep-Zou-Cao/313b368457e54e6a7482b008d5eb4182eb1b4d1c", "https://www.semanticscholar.org/paper/An-Improved-Analysis-of-Training-Over-parameterized-Zou-Gu/0feb7c19f98ac44703126dc46e60d166da1f118c", "https://www.semanticscholar.org/paper/The-Power-of-Interpolation%3A-Understanding-the-of-in-Ma-Bassily/f8dccc59fae4b86b955630f21f9558a194ca4f70", "https://www.semanticscholar.org/paper/Learning-Overparameterized-Neural-Networks-via-on-Li-Liang/ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6", "https://www.semanticscholar.org/paper/Recovery-Guarantees-for-One-hidden-layer-Neural-Zhong-Song/99ed21b585f4d6cbc4e20002bedca8d6c08169c6", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/On-exponential-convergence-of-SGD-in-non-convex-Bassily-Belkin/45b5765cd1847f6a370da01483a53e9ed9676557"]},
{"id": "Targeted-Dropout-Talwar/ea19fee3ecce6f1b17dfde38bce1ccceaeb5befb", "title": "Targeted Dropout", "authors": ["Kunal Talwar"], "date": "2018", "abstract": "Neural networks are extremely flexible models due to their large number of parameters, which is beneficial for learning, but also highly redundant. This makes it possible to compress neural networks without having a drastic effect on performance. We introduce targeted dropout, a strategy for post hoc pruning of neural network weights and units that builds the pruning mechanism directly into learning. At each weight update, targeted dropout selects a candidate set for pruning using a simple selection criterion, and then stochastically prunes the network via dropout applied to this set. The resulting network learns to be explicitly robust to pruning, comparing favourably to more complicated regularization schemes while at the same time being extremely simple to implement, and easy to tune.", "references": ["https://www.semanticscholar.org/paper/DropPruning-for-Model-Compression-Jia-Xiang/772599287ad4ddc2792cde8609fed9c061f9a67f", "https://www.semanticscholar.org/paper/Compressibility-Loss-for-Neural-Network-Weights-Aytekin-Cricri/b6ffedd1a893c07c1e6b842cc78023aee2c340ee", "https://www.semanticscholar.org/paper/An-Adaptive-Empirical-Bayesian-Method-for-Sparse-Deng-Zhang/e3127e7b443594b19ba6dc580295784e4e26e475", "https://www.semanticscholar.org/paper/Deep-Neural-Network-Compression-for-Image-and-Tzelepis-Asif/a5015fbe207c2c30e801e2c99549a45ba88cd3bf", "https://www.semanticscholar.org/paper/Parallel-Structure-Deep-Neural-Network-Using-CNN-an-Yao-Zhang/7e2be9a246a11328f14c3536d95a83b5ad74a998", "https://www.semanticscholar.org/paper/YNU_DYX-at-SemEval-2019-Task-5%3A-A-Stacked-BiGRU-on-Ding-Zhou/98c83297e9315c00356ae35e9881b46b2d4429ba", "https://www.semanticscholar.org/paper/Relational-Reasoning-Network-(RRN)-for-Anatomical-Torosdagli-McIntosh/20ed04b7fe94200b92af76a8fd66f24a012e99de", "https://www.semanticscholar.org/paper/Cross-Channel-Intragroup-Sparsity-Neural-Network-Yu-Wang/f88eee345eb6db3d1170736d745d7fc250cbf979", "https://www.semanticscholar.org/paper/Enhanced-Reinforcement-Learning-with-Targeted-Daday-Millado/6250a73c7a20393a89e599a5e4aa18a218b4c802", "https://www.semanticscholar.org/paper/Survey-of-Dropout-Methods-for-Deep-Neural-Networks-Labach-Salehinejad/028c64239ed932914f9fcceee403127c77fcb7e2", "https://www.semanticscholar.org/paper/NoiseOut%3A-A-Simple-Way-to-Prune-Neural-Networks-Babaeizadeh-Smaragdis/d43c746056601d514e49c9fd0ebed2ed55724eeb", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/Regularization-of-Neural-Networks-using-DropConnect-Wan-Zeiler/38f35dd624cd1cf827416e31ac5e0e0454028eca", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava/1366de5bb112746a555e9c0cd00de3ad8628aea8", "https://www.semanticscholar.org/paper/Learning-to-Prune-Filters-in-Convolutional-Neural-Huang-Zhou/2709a495f1a9d49bb852bbc512dd513ab158d0ad", "https://www.semanticscholar.org/paper/Learning-Ordered-Representations-with-Nested-Rippel-Gelbart/7a6fd5573d2679506765d461ec4892fd4017b745", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf"]},
{"id": "Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/308e0a4523537e9fdc25a6afff67f9d214738d76", "title": "Luck Matters: Understanding Training Dynamics of Deep ReLU Networks", "authors": ["Yuandong Tian", "Tina Jiang", "Qucheng Gong", "Ari S. Morcos"], "date": "2019", "abstract": "We analyze the dynamics of training deep ReLU networks and their implications on generalization capability. Using a teacher-student setting, we discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks. With this relationship and the assumption of small overlapping teacher node activations, we prove that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fan-out weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. We verify our assumption by showing that the majority of BatchNorm biases of pre-trained VGG11/16 models are negative. Experiments on (1) random deep teacher networks with Gaussian inputs, (2) teacher network pre-trained on CIFAR-10 and (3) extensive ablation studies validate our multiple theoretical predictions.", "references": ["https://www.semanticscholar.org/paper/Over-parameterization-as-a-Catalyst-for-Better-of-Tian/7384fe1e1c5b760e5af2b2864c3c0dc6bee115aa", "https://www.semanticscholar.org/paper/Compressing-BERT%3A-Studying-the-Effects-of-Weight-on-Gordon-Duh/e81ce33b7ce77215c1ee7126a7d4881b8fccfa33", "https://www.semanticscholar.org/paper/Discovering-Neural-Wirings-Wortsman-Farhadi/9c48f787f9590fcbad78707419ddfad269102cd3", "https://www.semanticscholar.org/paper/Study-of-Restrained-Network-Structures-for-Networks-Wang-Wang/b71b910fd61af9017d7435be5b732a8ca31a1bff", "https://www.semanticscholar.org/paper/Implicit-Regularization-of-Normalization-Methods-Wu-Dobriban/90750b4543420cb46d7b97a60ca979bcd909e597", "https://www.semanticscholar.org/paper/Convolution-Weight-Distribution-Assumption%3A-the-of-Huang-Wang/4115f4cc0045fa62ece3275c65cb7abec2f7bc6e", "https://www.semanticscholar.org/paper/Exploring-Weight-Importance-and-Hessian-Bias-in-Li-Sattar/627d24111c91f4a99bf2be1d84b7b18643cb8594", "https://www.semanticscholar.org/paper/Student-Specialization-in-Deep-ReLU-Networks-With-Tian/7db3d756dab942145a91598c572c93c7ffee669a", "https://www.semanticscholar.org/paper/A-theoretical-framework-for-deep-locally-connected-Tian/e8977504e5c8a62be486ffa2b9a22cc1dee6c452", "https://www.semanticscholar.org/paper/Deep-Nets-Don't-Learn-via-Memorization-Krueger-Ballas/0ebb83e5c28719c7b5cb5bc482e62f835fb0d116", "https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf", "https://www.semanticscholar.org/paper/Towards-Understanding-Generalization-of-Deep-of-Wu-Zhu/104ef009c1b2fc3b8be95dbb6bb1456631c61f94", "https://www.semanticscholar.org/paper/Sharp-Minima-Can-Generalize-For-Deep-Nets-Dinh-Pascanu/58123025178256279bb060ca5da971b62bc329ee", "https://www.semanticscholar.org/paper/Theoretical-Analysis-of-Auto-Rate-Tuning-by-Batch-Arora-Li/3dfb0a18ab5a5413c50d911e49b3c83b1a9383a3", "https://www.semanticscholar.org/paper/Comparing-Dynamics%3A-Deep-Neural-Networks-versus-Baity-Jesi-Sagun/fc998c5e7b0fd1609d5a91d700fec3ec9c72838c", "https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3", "https://www.semanticscholar.org/paper/A-Mean-Field-Theory-of-Batch-Normalization-Yang-Pennington/e5b7c1ce5a46e059fce96249c0c034afdd3c287a", "https://www.semanticscholar.org/paper/Identity-Crisis%3A-Memorization-and-Generalization-Zhang-Bengio/8f977a3831132ffaad2f13eea05c1fa46205b8ec"]},
{"id": "Neural-Rejuvenation%3A-Improving-Deep-Network-by-Qiao-Lin/3ba9fdcd0d10f2a130fcbd678cebcbb5e8c6bd5f", "title": "Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization", "authors": ["Siyuan Qiao", "Zhe L. Lin", "Jianming Zhang", "Alan L. Yuille"], "date": "2019", "abstract": "In this paper, we study the problem of improving computational resource utilization of neural networks. Deep neural networks are usually over-parameterized for their tasks in order to achieve good performances, thus are likely to have underutilized computational resources. This observation motivates a lot of research topics, e.g. network pruning, architecture search, etc. As models with higher computational costs (e.g. more parameters or more computations) usually have better performances, we study the problem of improving the resource utilization of neural networks so that their potentials can be further realized. To this end, we propose a novel optimization method named Neural Rejuvenation. As its name suggests, our method detects dead neurons and computes resource utilization in real time, rejuvenates dead neurons by resource reallocation and reinitialization, and trains them with new training schemes. By simply replacing standard optimizers with Neural Rejuvenation, we are able to improve the performances of neural networks by a very large margin while using similar training efforts and maintaining their original resource usages. The code is available here: https://github.com/joe-siyuan-qiao/NeuralRejuvenation-CVPR19", "references": ["https://www.semanticscholar.org/paper/Differentiable-Mask-Pruning-for-Neural-Networks-Ramakrishnan-Sari/59ad64a006290f3a8bb51e17731adedffd821d82", "https://www.semanticscholar.org/paper/Differentiable-Mask-for-Pruning-Convolutional-and-Ramakrishnan-Sari/26d7d8bacd4dbf2cf9c582a9c8ca68642e5cc095", "https://www.semanticscholar.org/paper/Understanding-the-Effects-of-Pre-Training-for-via-Shinya-Simo-Serra/cdbcc8a38bba2e69e5adb6954d7eb416c104507e", "https://www.semanticscholar.org/paper/A-Multigrid-Method-for-Efficiently-Training-Video-Wu-Girshick/b8cc28e11900e4d191651ae7234b4dbd129b1007", "https://www.semanticscholar.org/paper/MMA-Regularization%3A-Decorrelating-Weights-of-Neural-Wang-Xiang/01a5a65451f5343bca8f3c75e583b7c4c89187dd", "https://www.semanticscholar.org/paper/SemifreddoNets%3A-Partially-Frozen-Neural-Networks-Isikdogan-Nayak/083e8ecf819854b54aa9dcca3982bf57e6579018", "https://www.semanticscholar.org/paper/Efficient-Architecture-Search-by-Network-Cai-Chen/84e65a5bdb735d62eef4f72c2f01af354b2285ba", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/Efficient-Neural-Architecture-Search-via-Parameter-Pham-Guan/fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Neural-Architecture-Search-with-Reinforcement-Zoph-Le/67d968c7450878190e45ac7886746de867bf673d", "https://www.semanticscholar.org/paper/NISP%3A-Pruning-Networks-Using-Neuron-Importance-Yu-Li/af03709f0893a7ff1c2656b73249d60030bab996", "https://www.semanticscholar.org/paper/Progressive-Neural-Architecture-Search-Liu-Zoph/5f79398057bf0bbda9ff50067bc1f2950c2a2266", "https://www.semanticscholar.org/paper/Designing-Neural-Network-Architectures-using-Baker-Gupta/6cd5dfccd9f52538b19a415e00031d0ee4e5b181", "https://www.semanticscholar.org/paper/Binarized-Neural-Networks%3A-Training-Deep-Neural-and-Courbariaux-Hubara/6eecc808d4c74e7d0d7ef6b8a4112c985ced104d"]},
{"id": "Learning-Sparse-Networks-Using-Targeted-Dropout-Gomez-Zhang/13de3c06ef6dac1c296ada45df2be590f843edb7", "title": "Learning Sparse Networks Using Targeted Dropout", "authors": ["Aidan N. Gomez", "Ivan Zhang", "Kevin Swersky", "Yarin Gal", "Geoffrey E. Hinton"], "date": "2019", "abstract": "Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout, a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update, targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune.", "references": ["https://www.semanticscholar.org/paper/Grouped-sparse-projection-Gillis-Ohib/7b6f91d5ff12bb914f5c1a14952b0c688a8711f1", "https://www.semanticscholar.org/paper/AutoPrune%3A-Automatic-Network-Pruning-by-Auxiliary-Xiao-Wang/ff77078964bcce683e1583d939148881dfa7e1ba", "https://www.semanticscholar.org/paper/Gradient-Monitored-Reinforcement-Learning-Hameed-Chadha/456668d42e2cc428feac6621697c850daa5ef950", "https://www.semanticscholar.org/paper/Reinforcement-Learning-with-Chromatic-Networks-for-Song-Choromanski/c6b9b691babc5be1d81079ac7f8edfc487f9d196", "https://www.semanticscholar.org/paper/Discovering-Neural-Wirings-Wortsman-Farhadi/9c48f787f9590fcbad78707419ddfad269102cd3", "https://www.semanticscholar.org/paper/Reinforcement-Learning-with-Chromatic-Networks-Song-Choromanski/35098c03c30c601b93a6203bc5d69666c18135b8", "https://www.semanticscholar.org/paper/Impact-of-deep-learning-based-dropout-on-shallow-to-Piotrowski-Napiorkowski/123120600adbcc6dd5282b3589ae28a650e3953d", "https://www.semanticscholar.org/paper/Out-of-the-box-channel-pruned-networks-Venkatesan-Swaminathan/6cf32ac2248bb9102743ed2bba3a088866a44dc7", "https://www.semanticscholar.org/paper/Neural-Network-Compression-Framework-for-fast-model-Kozlov-Lazarevich/66cb331ba7f145f470869bd39dcef8ff535503a9", "https://www.semanticscholar.org/paper/CARS%3A-Continuous-Evolution-for-Efficient-Neural-Yang-Wang/a18938ca00d7c1299b914ee81a802c383da04d68", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Adaptive-dropout-for-training-deep-neural-networks-Ba-Frey/f9f19bee621faf46f90b023f8de8248b57becbc4", "https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/Pruning-neural-networks%3A-is-it-time-to-nip-it-in-Crowley-Turner/91c96082aed524662e9429540ae1dc0a5f561bc9", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/NoiseOut%3A-A-Simple-Way-to-Prune-Neural-Networks-Babaeizadeh-Smaragdis/d43c746056601d514e49c9fd0ebed2ed55724eeb", "https://www.semanticscholar.org/paper/Excitation-Dropout%3A-Encouraging-Plasticity-in-Deep-Zunino-Bargal/b7c2227258ce57f46b761b3e2a1e89dc86c2ce89", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Regularization-of-Neural-Networks-using-DropConnect-Wan-Zeiler/38f35dd624cd1cf827416e31ac5e0e0454028eca"]},
{"id": "Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "title": "Rethinking the Value of Network Pruning", "authors": ["Zhuang Liu", "Mingjie Sun", "Tinghui Zhou", "Gao Huang", "Trevor Darrell"], "date": "2019", "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.", "references": ["https://www.semanticscholar.org/paper/Pruning-from-Scratch-Wang-Zhang/454655f4e14e3892bfb574bd283ac5d4184847f4", "https://www.semanticscholar.org/paper/Gradual-Channel-Pruning-while-Training-using-Scores-Aketi-Roy/d1f7c85c7bff8337b5d942879d6d6b18435b6d8a", "https://www.semanticscholar.org/paper/AutoPrune%3A-Automatic-Network-Pruning-by-Auxiliary-Xiao-Wang/ff77078964bcce683e1583d939148881dfa7e1ba", "https://www.semanticscholar.org/paper/A-Closer-Look-at-Structured-Pruning-for-Neural-Crowley-Turner/cd8767219e6ab832ebaa227dfc94c9e7f346647b", "https://www.semanticscholar.org/paper/Pruning-Filters-while-Training-for-Efficiently-Deep-Roy-Panda/71cf7f19755bed67edc4c42e583acd2c8a357289", "https://www.semanticscholar.org/paper/Dynamic-Model-Pruning-with-Feedback-Lin-Stich/c71da533053d79ad267b1d74814a43dda7c584fb", "https://www.semanticscholar.org/paper/Importance-Estimation-for-Neural-Network-Pruning-Molchanov-Mallya/a6f4917d043494d2ebaebe6b65cb35e6a07fda41", "https://www.semanticscholar.org/paper/PICKING-WINNING-TICKETS-BEFORE-TRAINING/e08557d60dc47af675b48688a3524a7b0a6eac84", "https://www.semanticscholar.org/paper/CGaP%3A-Continuous-Growth-and-Pruning-for-Efficient-Du-Li/438b3860ee945fb47984ab417249f1d19a65f9c3", "https://www.semanticscholar.org/paper/Network-Pruning-via-Transformable-Architecture-Dong-Yang/817e265620f2a62ed3171b2f7d57d56b9d601a1f", "https://www.semanticscholar.org/paper/Runtime-Neural-Pruning-Lin-Rao/88cd4209db62a34d9cba0b9cbe9d45d1e57d21e5", "https://www.semanticscholar.org/paper/To-prune%2C-or-not-to-prune%3A-exploring-the-efficacy-Zhu-Gupta/3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "https://www.semanticscholar.org/paper/Network-Trimming%3A-A-Data-Driven-Neuron-Pruning-Deep-Hu-Peng/60ae4f18cb53efff0174e3fea7064049737e1e67", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/Layer-compensated-Pruning-for-Resource-constrained-Chin-Zhang/0f0a58958e7c2c4fe59127aff3f10c9dfd7a473e", "https://www.semanticscholar.org/paper/Efficient-Neural-Architecture-Search-via-Parameter-Pham-Guan/fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "https://www.semanticscholar.org/paper/Recovering-from-Random-Pruning%3A-On-the-Plasticity-Mittal-Bhardwaj/d3f6b3ce8f7b67c1e112a79b3fe9764242c655f5", "https://www.semanticscholar.org/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "https://www.semanticscholar.org/paper/Rethinking-the-Smaller-Norm-Less-Informative-in-of-Ye-Lu/60464c4bd94a14b63898e322f9ea651830e54ae0", "https://www.semanticscholar.org/paper/NISP%3A-Pruning-Networks-Using-Neuron-Importance-Yu-Li/af03709f0893a7ff1c2656b73249d60030bab996"]},
{"id": "Temporal-Convolutional-Attention-based-Network-For-Hao-Wang/80d15b5e95b27e224838914ad06a81b0567e6e0f", "title": "Temporal Convolutional Attention-based Network For Sequence Modeling", "authors": ["Hongyan Hao", "Yan Wang", "Yudi Xia", "Jian Zhao", "Shen Furao"], "date": "2020", "abstract": "With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 26.92 on word-level PTB, 1.043 on character-level PTB, and 6.66 on WikiText-2.", "references": ["https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/An-Empirical-Evaluation-of-Generic-Convolutional-Bai-Kolter/921196c32213a229245a9705ee4768bc941e7a26", "https://www.semanticscholar.org/paper/Trellis-Networks-for-Sequence-Modeling-Bai-Kolter/a14af711aaa3ae83eb64d1f517b024b8c3094a8a", "https://www.semanticscholar.org/paper/Regularizing-and-Optimizing-LSTM-Language-Models-Merity-Keskar/58c6f890a1ae372958b7decf56132fe258152722", "https://www.semanticscholar.org/paper/Independently-Recurrent-Neural-Network-(IndRNN)%3A-A-Li-Li/565ab57eede8bf6ef9c42df51216b9f85287c234", "https://www.semanticscholar.org/paper/Improving-Neural-Language-Modeling-via-Adversarial-Wang-Gong/e84d754564c9e2ce993596370e0a1493c9c6e4b1", "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe", "https://www.semanticscholar.org/paper/Sharp-Nearby%2C-Fuzzy-Far-Away%3A-How-Neural-Language-Khandelwal-He/fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd", "https://www.semanticscholar.org/paper/Zoneout%3A-Regularizing-RNNs-by-Randomly-Preserving-Krueger-Maharaj/9f0687bcd0a7d7fc91b8c5d36c003a38b8853105"]},
{"id": "An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann/94238dead40b12735d79ed63e29ead70730261a2", "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation", "authors": ["Alessandro Raganato", "J\u00f6rg Tiedemann"], "date": "2018", "abstract": "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.", "references": ["https://www.semanticscholar.org/paper/Improving-Neural-Machine-Translation-with-Bugliarello-Okazaki/b26bfc6fa8aff11181cf1231e36752e8c33e080c", "https://www.semanticscholar.org/paper/Incorporating-Source-Syntax-into-Transformer-Based-Currey-Heafield/0f4a3b7f835737c7e004073f4a873e356c0063b4", "https://www.semanticscholar.org/paper/Fixed-Encoder-Self-Attention-Patterns-in-Machine-Raganato-Scherrer/57f123c95ecf9d901be3a53291f53302740451e2", "https://www.semanticscholar.org/paper/The-Bottom-up-Evolution-of-Representations-in-the-A-Voita-Sennrich/112fd54ee193237b24f2ce7fce79e399609a29c5", "https://www.semanticscholar.org/paper/Cross-Aggregation-of-Multi-head-Attention-for-Cao-Hai/4d68c1b4167f858979c6a8e8b9ad0b484cd48c63", "https://www.semanticscholar.org/paper/Analyzing-the-Structure-of-Attention-in-a-Language-Vig-Belinkov/a039ea239e37f53a2cb60c68e0a1967994353166", "https://www.semanticscholar.org/paper/Abstractive-Text-Summarization-based-on-Language-Aksenov-Moreno-Schneider/a750d10fef892f63c7a52eb70f2a5f081bfebc82", "https://www.semanticscholar.org/paper/Attention-Module-is-Not-Only-a-Weight%3A-Analyzing-Kobayashi-Kuribayashi/2a8e42995caaedadc9dc739d85bed2c57fc78568", "https://www.semanticscholar.org/paper/On-the-Linguistic-Representational-Power-of-Neural-Belinkov-Durrani/2d94a995267903535f78c661a341ee34eabc1ec0", "https://www.semanticscholar.org/paper/Multi-Granularity-Self-Attention-for-Neural-Machine-Hao-Wang/91066eed5b158e58b004038fcb6cf1186b20791b", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Evaluating-Layers-of-Representation-in-Neural-on-Belinkov-Villodre/dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d", "https://www.semanticscholar.org/paper/Fine-grained-Analysis-of-Sentence-Embeddings-Using-Adi-Kermany/e44da7d8c71edcc6e575fa7faadd5e75785a7901", "https://www.semanticscholar.org/paper/What-do-Neural-Machine-Translation-Models-Learn-Belinkov-Durrani/82364428995c29b3dcb60c1835548eeff4adcd20", "https://www.semanticscholar.org/paper/A-Convolutional-Encoder-Model-for-Neural-Machine-Gehring-Auli/f958d4921951e394057a1c4ec33bad9a34e5dad1", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Exploring-the-Syntactic-Abilities-of-RNNs-with-Enguehard-Goldberg/b64a0511b5e802b4dafef63b81800cf64f359eb1", "https://www.semanticscholar.org/paper/Exploiting-Linguistic-Resources-for-Neural-Machine-Niehues-Cho/2f9700e197e5bdfd6fffa9dc1badd25f224739fe", "https://www.semanticscholar.org/paper/Transfer-Learning-for-Low-Resource-Neural-Machine-Zoph-Yuret/1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6"]},
{"id": "Successfully-Applying-the-Stabilized-Lottery-Ticket-Brix-Bahar/27a04669fef49981b91a6a148e549360ad309761", "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture", "authors": ["Christopher Brix", "Parnia Bahar", "Hermann Ney"], "date": "2020", "abstract": "Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter's initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.", "references": ["https://www.semanticscholar.org/paper/Deconstructing-Lottery-Tickets%3A-Zeros%2C-Signs%2C-and-Zhou-Lan/f7c410ab241bc972cda2f47993124ea8483003b6", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis-at-Scale-Frankle-Dziugaite/03e0cbeb4604262446a97cb381874c7de1cffea2", "https://www.semanticscholar.org/paper/Playing-the-lottery-with-rewards-and-multiple-in-RL-Yu-Edunov/387e0b95d56e9ecec60a1037ddf7cc57b2851835", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf", "https://www.semanticscholar.org/paper/To-prune%2C-or-not-to-prune%3A-exploring-the-efficacy-Zhu-Gupta/3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "https://www.semanticscholar.org/paper/The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen/26384278cf5d575fc32cb92c303fb648fa0d5217", "https://www.semanticscholar.org/paper/Exploring-Sparsity-in-Recurrent-Neural-Networks-Narang-Diamos/0a5265d5f4a2b59bde18c258ad5acd26bc680769", "https://www.semanticscholar.org/paper/Fast-Sparse-ConvNets-Elsen-Dukhan/db2d3dc613169b519f1a2dd35e0473dc2e848025", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/EIE%3A-Efficient-Inference-Engine-on-Compressed-Deep-Han-Liu/2e2b189f668cf2c06ebc44dc9b166648256cf457"]},
{"id": "Gated-Self-attentive-Encoder-for-Neural-Machine-Wei-Hu/237ab961a29a17f6952f4dea3a0647b2fb6d6090", "title": "Gated Self-attentive Encoder for Neural Machine Translation", "authors": ["Xiangpeng Wei", "Yue Hu", "Luxi Xing"], "date": "2019", "abstract": "Neural Machine Translation (NMT) has become a popular technology in recent years, and the RNN-based encoder-decoder model is its actual translation framework. However, it remains a major challenge for RNNs to handle long-range dependencies. To address this limitation, we propose a gated self-attentive encoder (GSAE) for NMT that aims to directly capture the dependency relationship between any two words of source-side regardless of their distance. The proposed GSAE gains access to a wider context and ensures the better representation in the encoder. We extensively evaluate the proposed model using three language pairs. Experiments on WMT\u201914 English-German and WMT\u201914 English-French tasks show that our shallow model achieves BLEU = 26.54 and BLEU = 38.94 respectively, which are comparable with the state-of-the-art deep models. On WMT\u201917 English-Chinese translation task, our GSAE-NMT outperforms two strong baselines by 1.61 and 1.15 BLEU points, respectively.", "references": ["https://www.semanticscholar.org/paper/Improved-Neural-Machine-Translation-with-Source-Wu-Zhou/9f291ce2d0fc1d76206139a40a859283674d8f65", "https://www.semanticscholar.org/paper/Deep-Neural-Machine-Translation-with-Linear-Unit-Wang-Lu/ff4dde4e30236747fa2b3539a4bfcc786f1ae6bc", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff", "https://www.semanticscholar.org/paper/Modeling-Coverage-for-Neural-Machine-Translation-Tu-Lu/33108287fbc8d94160787d7b2c7ef249d3ad6437", "https://www.semanticscholar.org/paper/Deep-Semantic-Role-Labeling-with-Self-Attention-Tan-Wang/6ed376a26045ff0048ec2b216785d396960d6ed1"]},
{"id": "Neural-Machine-Translation-with-Deep-Attention-Zhang-Xiong/ae5a56effb19dc0137c9071e5bdc14b638eb2e10", "title": "Neural Machine Translation with Deep Attention", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "date": "2020", "abstract": "Deepening neural models has been proven very successful in improving the model's capacity when solving complex learning tasks, such as the machine translation task. Previous efforts on deep neural machine translation mainly focus on the encoder and the decoder, while little on the attention mechanism. However, the attention mechanism is of vital importance to induce the translation correspondence between different languages where shallow neural networks are relatively insufficient, especially when the encoder and decoder are deep. In this paper, we propose a deep attention model (DeepAtt). Based on the low-level attention information, DeepAtt is capable of automatically determining what should be passed or suppressed from the corresponding encoder layer so as to make the distributed representation appropriate for high-level attention and translation. We conduct experiments on NIST Chinese-English, WMT English-German, and WMT English-French translation tasks, where, with five attention layers, DeepAtt yields very competitive performance against the state-of-the-art results. We empirically find that with an adequate increase of attention layers, DeepAtt tends to produce more accurate attention weights. An in-depth analysis on the translation of important context words further reveals that DeepAtt significantly improves the faithfulness of system translations.", "references": ["https://www.semanticscholar.org/paper/Multiscale-Collaborative-Deep-Models-for-Neural-Wei-Yu/9e9ec3dba37f2b1a47b84a7ad7d6fef11ef34b6b", "https://www.semanticscholar.org/paper/Future-Aware-Knowledge-Distillation-for-Neural-Zhang-Xiong/4a546aef476b1e5375b0fe3a500a226f693c1d25", "https://www.semanticscholar.org/paper/Improving-Deep-Transformer-with-Depth-Scaled-and-Zhang-Titov/4cf963e5fd88825ac62ad6cce364447e5d2dfb2b", "https://www.semanticscholar.org/paper/REMOVED%3A-Equipping-recurrent-neural-network-with-of-Usama-Ahmad/7adad86420738efbb5926035198ff003944ab548", "https://www.semanticscholar.org/paper/Multimodal-Neural-Machine-Translation-With-Weakly-Heo-Kang/7f867ac51b100eedd2ed4e46852fcf4a06c2c87c", "https://www.semanticscholar.org/paper/Machine-Translation-with-Image-Context-from-Chinese-Johnson/19ac88e9d729e2f4c33f3a5ce5fbce958d7b869c", "https://www.semanticscholar.org/paper/Context-in-Neural-Machine-Translation%3A-A-Review-of-Popescu-Belis/3d35f6876451ee9de37f7dda628d108a8442fea4", "https://www.semanticscholar.org/paper/Progressive-Self-Supervised-Attention-Learning-for-Tang-Lu/db4da4ac33210c26bd440bfb894ea4e175c81238", "https://www.semanticscholar.org/paper/Symmetry-Encoder-Decoder-Network-with-Attention-for-Guo-Zhang/9703ec378d5d5e505ec2b7a2cbff5aa70ccfe28e", "https://www.semanticscholar.org/paper/A-Lightweight-Recurrent-Network-for-Sequence-Korhonen-M%C3%A0rquez/474bb7ca4ed0f26df0ef9c195c20c797d2e39fca", "https://www.semanticscholar.org/paper/Recurrent-Neural-Machine-Translation-Zhang-Xiong/65161b91378d0ee020f79a467c3646c801c24a0f", "https://www.semanticscholar.org/paper/Deep-Neural-Machine-Translation-with-Linear-Unit-Wang-Lu/ff4dde4e30236747fa2b3539a4bfcc786f1ae6bc", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Deep-Neural-Machine-Translation-with-Units-Gangi-Federico/a33b1e5c67be044005d5d48be4eb05c6da22aa77", "https://www.semanticscholar.org/paper/Asynchronous-Bidirectional-Decoding-for-Neural-Zhang-Su/0a2b9345a8028f92ce0340b7d5b947352f580a09", "https://www.semanticscholar.org/paper/Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "https://www.semanticscholar.org/paper/A-GRU-Gated-Attention-Model-for-Neural-Machine-Zhang-Xiong/a3143ea1bc45933da261af5b01fd4040ac47b0b6"]},
{"id": "Transformer%2B%2B-Thapak-Hore/329484f2e34e7a58ba8aeac000ed57c6702942ee", "title": "Transformer++", "authors": ["Prakhar Thapak", "Prodip Hore"], "date": "2020", "abstract": "Recent advancements in attention mechanisms have replaced recurrent neural networks and its variants for machine translation tasks. Transformer using attention mechanism solely achieved state-of-the-art results in sequence modeling. Neural machine translation based on the attention mechanism is parallelizable and addresses the problem of handling long-range dependencies among words in sentences more effectively than recurrent neural networks. One of the key concepts in attention is to learn three matrices, query, key, and value, where global dependencies among words are learned through linearly projecting word embeddings through these matrices. Multiple query, key, value matrices can be learned simultaneously focusing on a different subspace of the embedded dimension, which is called multi-head in Transformer. We argue that certain dependencies among words could be learned better through an intermediate context than directly modeling word-word dependencies. This could happen due to the nature of certain dependencies or lack of patterns that lend them difficult to be modeled globally using multi-head self-attention. In this work, we propose a new way of learning dependencies through a context in multi-head using convolution. This new form of multi-head attention along with the traditional form achieves better results than Transformer on the WMT 2014 English-to-German and English-to-French translation tasks. We also introduce a framework to learn POS tagging and NER information during the training of encoder which further improves results achieving a new state-of-the-art of 32.1 BLEU, better than existing best by 1.4 BLEU, on the WMT 2014 English-to-German and 44.6 BLEU, better than existing best by 1.1 BLEU, on the WMT 2014 English-to-French translation tasks. We call this Transformer++.", "references": ["https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Can-Active-Memory-Replace-Attention-Kaiser-Bengio/735d547fc75e0772d2a78c46a1cc5fad7da1474c", "https://www.semanticscholar.org/paper/MUSE%3A-Parallel-Multi-Scale-Attention-for-Sequence-Zhao-Sun/3bc53c49ae68adacf2d5be2fa795bcb879e2717a", "https://www.semanticscholar.org/paper/Attention-Based-Models-for-Speech-Recognition-Chorowski-Bahdanau/b624504240fa52ab76167acfe3156150ca01cf3b", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Pay-Less-Attention-with-Lightweight-and-Dynamic-Wu-Fan/ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f", "https://www.semanticscholar.org/paper/Exploiting-Linguistic-Resources-for-Neural-Machine-Niehues-Cho/2f9700e197e5bdfd6fffa9dc1badd25f224739fe", "https://www.semanticscholar.org/paper/Outrageously-Large-Neural-Networks%3A-The-Layer-Shazeer-Mirhoseini/510e26733aaff585d65701b9f1be7ca9d5afc586"]},
{"id": "Joint-Source-Target-Self-Attention-with-Locality-Fonollosa-Casas/b0f0a5a21619d70748a4dc007983cc111f1b301e", "title": "Joint Source-Target Self Attention with Locality Constraints", "authors": ["Jos\u00e9 A. R. Fonollosa", "Noe Casas", "Marta R. Costa-juss\u00e0"], "date": "2019", "abstract": "The dominant neural machine translation models are based on the encoder-decoder structure, and many of them rely on an unconstrained receptive field over source and target sequences. In this paper we study a new architecture that breaks with both conventions. Our simplified architecture consists in the decoder part of a transformer model, based on self-attention, but with locality constraints applied on the attention receptive field. As input for training, both source and target sentences are fed to the network, which is trained as a language model. At inference time, the target tokens are predicted autoregressively starting with the source sequence as previous tokens. The proposed model achieves a new state of the art of 35.7 BLEU on IWSLT'14 German-English and matches the best reported results in the literature on the WMT'14 English-German and WMT'14 English-French translation benchmarks.", "references": ["https://www.semanticscholar.org/paper/Fixed-Encoder-Self-Attention-Patterns-in-Machine-Raganato-Scherrer/57f123c95ecf9d901be3a53291f53302740451e2", "https://www.semanticscholar.org/paper/Explicit-Sparse-Transformer%3A-Concentrated-Attention-Zhao-Lin/b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "https://www.semanticscholar.org/paper/Pointwise-Conv-Projection-Shared-Projection-Output/9538ae749417eb39af04658e15f65e05ee3012f3", "https://www.semanticscholar.org/paper/MUSE%3A-Parallel-Multi-Scale-Attention-for-Sequence-Zhao-Sun/3bc53c49ae68adacf2d5be2fa795bcb879e2717a", "https://www.semanticscholar.org/paper/The-TALP-UPC-Machine-Translation-Systems-for-WMT19-Casas-Fonollosa/0748409634261095d4edfac664e057eede1d00cc", "https://www.semanticscholar.org/paper/Masked-Translation-Model-Nix-Kim/93800f8d7dd7647167d2a41bc479fa8cab7b24f3", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Weighted-Transformer-Network-for-Machine-Ahmed-Keskar/3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "https://www.semanticscholar.org/paper/Training-Deeper-Neural-Machine-Translation-Models-Bapna-Chen/8fec5d6ac57e90f459e7330775165f2671abc445", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f", "https://www.semanticscholar.org/paper/Pervasive-Attention%3A-2D-Convolutional-Neural-for-Elbayad-Besacier/9e35cd34c87332796ed9d1480068ed8bb275bd45", "https://www.semanticscholar.org/paper/Pay-Less-Attention-with-Lightweight-and-Dynamic-Wu-Fan/ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Layer-Wise-Coordination-between-Encoder-and-Decoder-He-Tan/b12ccd118974839db290f15c989649b2b5188636", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},
{"id": "Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "title": "Accelerating Neural Transformer via an Average Attention Network", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "date": "2018", "abstract": "With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.", "references": ["https://www.semanticscholar.org/paper/Accelerating-Transformer-Decoding-via-a-Hybrid-of-Wang-Wu/9de89c5d9be19681da51ca9ca833b51b80859524", "https://www.semanticscholar.org/paper/Sharing-Attention-Weights-for-Fast-Transformer-Xiao-Li/5b3a37c1bd06bf3b0062e327a04b4a7af13d4e6f", "https://www.semanticscholar.org/paper/Improving-Deep-Transformer-with-Depth-Scaled-and-Zhang-Titov/4cf963e5fd88825ac62ad6cce364447e5d2dfb2b", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Deep-Attention-Zhang-Xiong/ae5a56effb19dc0137c9071e5bdc14b638eb2e10", "https://www.semanticscholar.org/paper/Language-Modeling-with-Transformer-Zhang-Li/1455b871c8e8a6c45ec541f08759cb589d019125", "https://www.semanticscholar.org/paper/Fast-Transformer-Decoding%3A-One-Write-Head-is-All-Shazeer/dc52b09089704ebd6f471177474bc29741c50023", "https://www.semanticscholar.org/paper/Analyzing-Word-Translation-of-Transformer-Layers-Xu-Genabith/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Simplifying-Neural-Machine-Translation-with-Zhang-Xiong/e7fa0af1fef0b219e122bbf66d792b131d0da42b", "https://www.semanticscholar.org/paper/Context-Aware-Self-Attention-Networks-Yang-Li/5d1a6d27d67e0b8d4a049f7f5dc3995f837d7976", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/DiSAN%3A-Directional-Self-Attention-Network-for-Shen-Zhou/adc276e6eae7051a027a4c269fb21dae43cadfed", "https://www.semanticscholar.org/paper/Recurrent-Neural-Machine-Translation-Zhang-Xiong/65161b91378d0ee020f79a467c3646c801c24a0f", "https://www.semanticscholar.org/paper/Structured-Attention-Networks-Kim-Denton/13d9323a8716131911bfda048a40e2cde1a76a46", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Convolutional-Sequence-to-Sequence-Learning-Gehring-Auli/43428880d75b3a14257c3ee9bda054e61eb869c0", "https://www.semanticscholar.org/paper/A-GRU-Gated-Attention-Model-for-Neural-Machine-Zhang-Xiong/a3143ea1bc45933da261af5b01fd4040ac47b0b6", "https://www.semanticscholar.org/paper/A-Convolutional-Encoder-Model-for-Neural-Machine-Gehring-Auli/f958d4921951e394057a1c4ec33bad9a34e5dad1", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454"]},
{"id": "Neural-Architecture-Search%3A-A-Survey-Elsken-Metzen/114a32bc872f160b58f503aca13f887556b5006e", "title": "Neural Architecture Search: A Survey", "authors": ["Thomas Elsken", "Jan Hendrik Metzen", "Frank Hutter"], "date": "2019", "abstract": "Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.", "references": ["https://www.semanticscholar.org/paper/Neural-Architecture-Search-Elsken-Metzen/f959413cf1914ab5d9046c72cbfccd46485cb016", "https://www.semanticscholar.org/paper/Improving-Neural-Architecture-Search-with-Learning/3a12106528c50b72cc0d437d93d120471429f582", "https://www.semanticscholar.org/paper/A-technical-view-on-neural-architecture-search-Hu-Yu/6b4f6971aaba912f04ad27a297e6fbfb59a96fcb", "https://www.semanticscholar.org/paper/Analysis-of-Efficient-Neural-Architecture-Search-Singh/fa5d44db9143980bf34805378e72f0e6b1c2405c", "https://www.semanticscholar.org/paper/A-Study-of-the-Learning-Progress-in-Neural-Search-Singh-Jacobs/b15f2c0d34e820729fee86b563c0cadd1f56f490", "https://www.semanticscholar.org/paper/CNASV%3A-A-Convolutional-Neural-Architecture-for-Task-Zhou-Weng/52349f4890c59a36ed3d3a3c1329edf8cd31640b", "https://www.semanticscholar.org/paper/Deep-neural-network-architecture-search-using-Kwasigroch-Grochowski/5872294900248407c193c939c18b434c5a7596d6", "https://www.semanticscholar.org/paper/CELL-BASED-NEURAL-ARCHITECTURE-SEARCH-Shu-Wang/a062b730749f10a0197b20858d08c6c90b7034a2", "https://www.semanticscholar.org/paper/Part-of-Speech-Tagging-with-Neural-Architecture-Bingham/d65400fc9316368117d9cec8e02ebc322b8793de", "https://www.semanticscholar.org/paper/Automated-Architecture-Design-for-Deep-Neural-Abreu/475315fa75357c116e845ee98d7a1410dd07887e", "https://www.semanticscholar.org/paper/Evolving-Deep-Neural-Networks-Miikkulainen-Liang/cd7c02cab3ec4e59150657bd6660eab97def9a3a", "https://www.semanticscholar.org/paper/Simple-And-Efficient-Architecture-Search-for-Neural-Elsken-Metzen/0e0ee672ebd9ec0019c414d1c0524f3bb888dd6d", "https://www.semanticscholar.org/paper/DARTS%3A-Differentiable-Architecture-Search-Liu-Simonyan/c1f457e31b611da727f9aef76c283a18157dfa83", "https://www.semanticscholar.org/paper/Neural-Architecture-Search-with-Reinforcement-Zoph-Le/67d968c7450878190e45ac7886746de867bf673d", "https://www.semanticscholar.org/paper/Understanding-and-Simplifying-One-Shot-Architecture-Bender-Kindermans/45b7b5514a65126d39a51d5a68da53e7aa244c1f", "https://www.semanticscholar.org/paper/Progressive-Neural-Architecture-Search-Liu-Zoph/5f79398057bf0bbda9ff50067bc1f2950c2a2266", "https://www.semanticscholar.org/paper/Efficient-Architecture-Search-by-Network-Cai-Chen/84e65a5bdb735d62eef4f72c2f01af354b2285ba", "https://www.semanticscholar.org/paper/Towards-Automated-Deep-Learning%3A-Efficient-Joint-Zela-Klein/b9e942942306d1d4b7a5640d8ed3c0cdcdc34078", "https://www.semanticscholar.org/paper/DeepArchitect%3A-Automatically-Designing-and-Training-Negrinho-Gordon/71a80e7342e56f33fd120246e907151a0cf1b4d0", "https://www.semanticscholar.org/paper/Learning-Transferable-Architectures-for-Scalable-Zoph-Vasudevan/d0611891b9e8a7c5731146097b6f201578f47b2f"]},
{"id": "Understanding-and-Simplifying-One-Shot-Architecture-Bender-Kindermans/45b7b5514a65126d39a51d5a68da53e7aa244c1f", "title": "Understanding and Simplifying One-Shot Architecture Search", "authors": ["Gabriel Bender", "Pieter-Jan Kindermans", "Barret Zoph", "Vijay Vasudevan", "Quoc V. Le"], "date": "2018", "abstract": "There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive, requiring thousands of different architectures to be trained from scratch. Recent work has explored weight sharing across models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hypernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.", "references": ["https://www.semanticscholar.org/paper/Efficient-Novelty-Driven-Neural-Architecture-Search-Zhang-Li/93c07b1aaaaa617709024f3ee2654c91efde8260", "https://www.semanticscholar.org/paper/DARTS%3A-Differentiable-Architecture-Search-Liu-Simonyan/c1f457e31b611da727f9aef76c283a18157dfa83", "https://www.semanticscholar.org/paper/Analysis-of-Efficient-Neural-Architecture-Search-Singh/fa5d44db9143980bf34805378e72f0e6b1c2405c", "https://www.semanticscholar.org/paper/A-Survey-on-Neural-Architecture-Search-Wistuba-Rawat/9c919f435d6a27706e1c55ca180b96ee7c122609", "https://www.semanticscholar.org/paper/BigNAS%3A-Scaling-Up-Neural-Architecture-Search-with-Yu-Jin/a8c0ac6588012d91c81b83b6cbd16c40e2e5edd2", "https://www.semanticscholar.org/paper/Variational-Depth-Search-in-ResNets-Antoran-Allingham/41eece1fda44d36b14c0e50c21d2bb594f102b17", "https://www.semanticscholar.org/paper/ONE-SHOT-NEURAL-ARCHITECTURE-SEARCH-Siems-Hutter/ea633caefacaa8af7428f53e5200ceed47e32b7e", "https://www.semanticscholar.org/paper/BayesNAS%3A-A-Bayesian-Approach-for-Neural-Search-Zhou-Yang/93c4478a5f6e22925ac0582105e4fb5a94e787a7", "https://www.semanticscholar.org/paper/Efficient-Decoupled-Neural-Architecture-Search-by-Lee-Kim/ff3d4846d6a8e3f28a3613a3e39e9ca72580c8e6", "https://www.semanticscholar.org/paper/A-Review-of-Meta-Reinforcement-Learning-for-Deep-Ja%C3%A2fra-Laurent/529c3f5247d6f8763d2ca614c7e1cb591073b330", "https://www.semanticscholar.org/paper/SMASH%3A-One-Shot-Model-Architecture-Search-through-Brock-Lim/e56b10f7cd4bf037beac84da5925dc4544fab974", "https://www.semanticscholar.org/paper/Reinforcement-Learning-for-Architecture-Search-by-Cai-Chen/4e7c28bd51d75690e166769490ed718af9736faa", "https://www.semanticscholar.org/paper/Progressive-Neural-Architecture-Search-Liu-Zoph/5f79398057bf0bbda9ff50067bc1f2950c2a2266", "https://www.semanticscholar.org/paper/Neural-Architecture-Search-with-Reinforcement-Zoph-Le/67d968c7450878190e45ac7886746de867bf673d", "https://www.semanticscholar.org/paper/Simple-And-Efficient-Architecture-Search-for-Neural-Elsken-Metzen/0e0ee672ebd9ec0019c414d1c0524f3bb888dd6d", "https://www.semanticscholar.org/paper/Designing-Neural-Network-Architectures-using-Baker-Gupta/6cd5dfccd9f52538b19a415e00031d0ee4e5b181", "https://www.semanticscholar.org/paper/Hierarchical-Representations-for-Efficient-Search-Liu-Simonyan/856451974cce2d353d5d8a5a72104984a252375c", "https://www.semanticscholar.org/paper/Neural-Optimizer-Search-with-Reinforcement-Learning-Bello-Zoph/168b7d0ab57a331a228ce21ffd1becbb93066f79", "https://www.semanticscholar.org/paper/Learned-Optimizers-that-Scale-and-Generalize-Wichrowska-Maheswaranathan/b8ff7e02ffa1577d125acd3e998e8ce76a9059dc", "https://www.semanticscholar.org/paper/Searching-for-Activation-Functions-Ramachandran-Zoph/c8c4ab59ac29973a00df4e5c8df3773a3c59995a"]},
{"id": "An-Exploration-of-Parameter-Redundancy-in-Deep-with-Cheng-Yu/5934400081d9541339da0f16d2613263f1a4c2a2", "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections", "authors": ["Yu Cheng", "Felix X. Yu", "Rog\u00e9rio Schmidt Feris", "Sanjiv Kumar", "Alok N. Choudhary", "Shih-Fu Chang"], "date": "2015", "abstract": "We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d2) to O(dlogd) and space complexity from O(d2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.", "references": ["https://www.semanticscholar.org/paper/On-the-Expressive-Power-of-Deep-Fully-Circulant-Araujo-N%C3%A9grevergne/e736f68f07d5d5ae3e36b4764668918074f354f5", "https://www.semanticscholar.org/paper/On-the-Expressive-Power-of-Deep-Diagonal-Circulant-Araujo-N%C3%A9grevergne/ce5746d2d8b800e9547e52154469d0b312564869", "https://www.semanticscholar.org/paper/CirCNN%3A-Accelerating-and-Compressing-Deep-Neural-Ding-Liao/a818050961d1d88ad9ccbfcb2abcfdb5f75dfcd9", "https://www.semanticscholar.org/paper/Transformed-%F0%9D%93%811-Regularization-for-Learning-Sparse-Ma-Miao/33722da70530687e87f39bf9ec4d7944e37bfcd8", "https://www.semanticscholar.org/paper/CircConv%3A-A-Structured-Convolution-with-Low-Liao-Li/fb45445db753c4234cdcae065d0aa9c8ee460823", "https://www.semanticscholar.org/paper/Deep-Neural-Network-Approximation-using-Tensor-Kasiviswanathan-Narodytska/fa4d6f83ac667113c636a273175abc1fae2b7819", "https://www.semanticscholar.org/paper/Cascaded-Projection%3A-End-To-End-Network-Compression-Minnehan-Savakis/88cb92934f9ba8a9e7052870b1dd8d0bd9ad7b3d", "https://www.semanticscholar.org/paper/Reducing-the-Model-Order-of-Deep-Neural-Networks-Tu-Berisha/07e95172ed3a862ca27667328133bc9a51e8132d", "https://www.semanticscholar.org/paper/Compression-of-fully-connected-layer-in-neural-by-Wu/9c6a63ed20988fc20f51a253c6fd1e871fe02bb3", "https://www.semanticscholar.org/paper/Regularization-of-Deep-Neural-Networks-with-Dropout-Khan-Hayat/eddefcaae5df88acf6719f16f2b2528921448fb3", "https://www.semanticscholar.org/paper/Memory-Bounded-Deep-Convolutional-Networks-Collins-Kohli/2a4117849c88d4728c33b1becaa9fb6ed7030725", "https://www.semanticscholar.org/paper/Circulant-Binary-Embedding-Yu-Kumar/46abc8d639f1b8595ad241e44ac5dae61075ea80", "https://www.semanticscholar.org/paper/Deep-Fried-Convnets-Yang-Moczulski/27a99c21a1324f087b2f144adc119f04137dfd87", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Low-rank-matrix-factorization-for-Deep-Neural-with-Sainath-Kingsbury/5cea23330c76994cb626df20bed31cc2588033df", "https://www.semanticscholar.org/paper/Network-In-Network-Lin-Chen/5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Kernel-methods-match-Deep-Neural-Networks-on-TIMIT-Huang-Avron/b3c879b2430a61d8c4393436685c85bcfbd6c6d8", "https://www.semanticscholar.org/paper/FitNets%3A-Hints-for-Thin-Deep-Nets-Romero-Ballas/cd85a549add0c7c7def36aca29837efd24b24080"]},
{"id": "Survey-of-Dropout-Methods-for-Deep-Neural-Networks-Labach-Salehinejad/028c64239ed932914f9fcceee403127c77fcb7e2", "title": "Survey of Dropout Methods for Deep Neural Networks", "authors": ["Alex Labach", "Hojjat Salehinejad", "Shahrokh Valaee"], "date": "2019", "abstract": "Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.", "references": ["https://www.semanticscholar.org/paper/A-Comparison-of-Few-Shot-Learning-Methods-for-and-Ochal-Vazquez/88032676dcfa3b7cfe548c11d643418272d437e6", "https://www.semanticscholar.org/paper/A-Framework-for-Neural-Network-Pruning-Using-Gibbs-Labach-Valaee/fbb052d2f8018ec7058d38e06d2ee2513e7de43e", "https://www.semanticscholar.org/paper/Assessing-Intelligence-in-Artificial-Neural-Schaub-Hotaling/77f84aa48df0685584fcbd17df8d316133c9b377", "https://www.semanticscholar.org/paper/Distributed-Learning-of-Deep-Neural-Networks-using-Yuan-Kyrillidis/ad9acdd2d95186da6814fdb61229813ebbea0aa9", "https://www.semanticscholar.org/paper/Dual-attention-Guided-Dropblock-Module-for-Weakly-Yin-Zhang/dae051f6372892288b551bd197701846b481b696", "https://www.semanticscholar.org/paper/EDropout%3A-Energy-Based-Dropout-and-Pruning-of-Deep-Salehinejad-Valaee/6dbf2ca5d97189a784e8c9179864832451593236", "https://www.semanticscholar.org/paper/Impact-of-deep-learning-based-dropout-on-shallow-to-Piotrowski-Napiorkowski/123120600adbcc6dd5282b3589ae28a650e3953d", "https://www.semanticscholar.org/paper/Ising-Dropout-with-Node-Grouping-for-Training-and-Salehinejad-Wang/ef3f86783e98abe72c691516da1534bb3aa9e1ce", "https://www.semanticscholar.org/paper/Online-Tuning-of-Artificial-Neural-Networks-Using-a-Michel/5e6e0e2d12db912f75daf525912fc80b7550f083", "https://www.semanticscholar.org/paper/Pairwise-coupling-of-convolutional-neural-networks-Such-Tar%C3%A1bek/56824c3bb9a02ec254e59a7477ad6efaaede8904", "https://www.semanticscholar.org/paper/Analysis-on-the-Dropout-Effect-in-Convolutional-Park-Kwak/38367ae9f0f70ab16d4914e9cd6d24872eca67bc", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Fast-dropout-training-Wang-Manning/ec92efde21707ddf4b81f301cd58e2051c1a2443", "https://www.semanticscholar.org/paper/Improved-Regularization-of-Convolutional-Neural-Devries-Taylor/eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "https://www.semanticscholar.org/paper/A-Theoretically-Grounded-Application-of-Dropout-in-Gal-Ghahramani/0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "https://www.semanticscholar.org/paper/Dropout-as-a-Bayesian-Approximation%3A-Representing-Gal-Ghahramani/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "https://www.semanticscholar.org/paper/Effective-and-Efficient-Dropout-for-Deep-Neural-Cai-Gao/ea0d2d6feaef4d32aa193f7c09ca322ee830c108", "https://www.semanticscholar.org/paper/Efficient-object-localization-using-Convolutional-Tompson-Goroshin/ebcea2d842d3d4e320500086aff0deb4cb4412ff", "https://www.semanticscholar.org/paper/Adaptive-dropout-for-training-deep-neural-networks-Ba-Frey/f9f19bee621faf46f90b023f8de8248b57becbc4", "https://www.semanticscholar.org/paper/Adversarial-Dropout-Regularization-Saito-Ushiku/8c04688425fa3e03c24d08b09faad49e33f2cc30"]},
{"id": "Wide-Residual-Networks-Zagoruyko-Komodakis/1c4e9156ca07705531e45960b7a919dc473abb51", "title": "Wide Residual Networks", "authors": ["Sergey Zagoruyko", "Nikos Komodakis"], "date": "2016", "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL", "references": ["https://www.semanticscholar.org/paper/Deep-Pyramidal-Residual-Networks-Han-Kim/5bdf07c9897ca70788fff61dec56178a2bd0c29c", "https://www.semanticscholar.org/paper/Multi-Residual-Networks%3A-Improving-the-Speed-and-of-Abdi-Nahavandi/bd82a021f11da031837e4155c099e09ae946315c", "https://www.semanticscholar.org/paper/cient-Recurrent-Residual-Networks-Improved-by-MSc-Delft/a4a0b5f08198f6d7ea2d1e81bd97fea21afe3fc3", "https://www.semanticscholar.org/paper/PolyNet%3A-A-Pursuit-of-Structural-Diversity-in-Very-Zhang-Li/aad34665649953fa4bbacdc6eff4edb5408df6b3", "https://www.semanticscholar.org/paper/Sequentially-Aggregated-Convolutional-Networks-Huang-Ou/cefd98a7eb04843ae7dcdee7dc68f46c089a8eb1", "https://www.semanticscholar.org/paper/Residual-Networks-of-Residual-Networks%3A-Multilevel-Zhang-Sun/5543b8f38cdb1b6bc618f3e25c5f10e57ec01d15", "https://www.semanticscholar.org/paper/Deep-Anchored-Convolutional-Neural-Networks-Huang-Dwivedi/70cd3d0dd222e45b3ff7a628181de80fc1b5fdca", "https://www.semanticscholar.org/paper/Multi-Residual-Networks-Abdi-Nahavandi/904b322a61d9be9c0b1023946320f9245533085e", "https://www.semanticscholar.org/paper/Truncating-Wide-Networks-Using-Binary-Tree-Zhang-Ozay/22f4753dc8aafdd6cbe41cfa9e4b7c61e42d35a4", "https://www.semanticscholar.org/paper/Parameters-Sharing-in-Residual-Neural-Networks-Dai-Yu/b9dd59a7a05c9a0e3fac39768b8800b3368f49a5", "https://www.semanticscholar.org/paper/Deep-Networks-with-Stochastic-Depth-Huang-Sun/51db1f3c8dfc7d4077da39c96bb90a6358128111", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d", "https://www.semanticscholar.org/paper/Inception-v4%2C-Inception-ResNet-and-the-Impact-of-on-Szegedy-Ioffe/b5c26ab8767d046cb6e32d959fdf726aee89bb62", "https://www.semanticscholar.org/paper/Identity-Mappings-in-Deep-Residual-Networks-He-Zhang/77f0a39b8e02686fd85b01971f8feb7f60971f80", "https://www.semanticscholar.org/paper/FitNets%3A-Hints-for-Thin-Deep-Nets-Romero-Ballas/cd85a549add0c7c7def36aca29837efd24b24080", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Understanding-the-difficulty-of-training-deep-Glorot-Bengio/b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Deeply-Supervised-Nets-Lee-Xie/fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd"]},
{"id": "PICKING-WINNING-TICKETS-BEFORE-TRAINING/e08557d60dc47af675b48688a3524a7b0a6eac84", "title": "PICKING WINNING TICKETS BEFORE TRAINING", "authors": [], "date": "2019", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.", "references": ["https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Finding-Sparse%2C-Frankle-Carbin/21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "https://www.semanticscholar.org/paper/SNIP%3A-Single-shot-Network-Pruning-based-on-Lee-Ajanthan/cf440ccce4a7a8681e238b4f26d5b95109add55d", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis-at-Scale-Frankle-Dziugaite/03e0cbeb4604262446a97cb381874c7de1cffea2", "https://www.semanticscholar.org/paper/A-Signal-Propagation-Perspective-for-Pruning-Neural-Lee-Ajanthan/e7907e7e7d470a12bdab5e6381ad12c4f832ea49", "https://www.semanticscholar.org/paper/Fine-Grained-Analysis-of-Optimization-and-for-Arora-Du/14558cb69319eed0d5bfc5648aafcd09d882f443", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "https://www.semanticscholar.org/paper/Sparse-Networks-from-Scratch%3A-Faster-Training-Dettmers-Zettlemoyer/60ed82ca3ec8fbfef4d52e98e49ab687ce501a0c"]},
{"id": "Dynamic-Model-Pruning-with-Feedback-Lin-Stich/c71da533053d79ad267b1d74814a43dda7c584fb", "title": "Dynamic Model Pruning with Feedback", "authors": ["Tao Lin", "Sebastian U. Stich", "Luis Barba", "Daniil S. Dmitriev", "Martin Jaggi"], "date": "2020", "abstract": "Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and further that their performance surpasses all previously proposed pruning schemes (that come without feedback mechanisms).", "references": ["https://www.semanticscholar.org/paper/Provable-Filter-Pruning-for-Efficient-Neural-Liebenwein-Baykal/9085f9462008336ec20295fbf241eafce98a55b6", "https://www.semanticscholar.org/paper/Masking-as-an-Efficient-Alternative-to-Finetuning-Zhao-Lin/7fb301ea25f02dc7f4f7ee1360137503ee942c8c", "https://www.semanticscholar.org/paper/Ensemble-Distillation-for-Robust-Model-Fusion-in-Lin-Kong/b5c04070033a102d77603f5ae7a690e8297fe18b", "https://www.semanticscholar.org/paper/Soft-Threshold-Weight-Reparameterization-for-Kusupati-Ramanujan/a01048b7381e7bce7b3a11c8c01cf862d07c5b11", "https://www.semanticscholar.org/paper/Now-that-I-can-see%2C-I-can-improve%3A-Enabling-of-CNNs-Rajagopal-Bouganis/ef116eb8d38af689f55a208eb29740fb4d5c3fcc", "https://www.semanticscholar.org/paper/Progressive-Skeletonization%3A-Trimming-more-fat-from-Jorge-Sanyal/86622b19a571fe994ac3f6ae1207703551aff763", "https://www.semanticscholar.org/paper/WoodFisher%3A-Efficient-second-order-approximations-Singh-Alistarh/6c86ca1bc8ff857af2674ea69d016f9cc4d23216", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Parameter-Efficient-Training-of-Deep-Convolutional-Mostafa-Wang/c703e42ac401ad734f440d56f6e19e6b2af86a60", "https://www.semanticscholar.org/paper/Compression-aware-Training-of-Deep-Networks-Alvarez-Salzmann/45a154f8be8ec31821a0e409d4b69635670a2e1e", "https://www.semanticscholar.org/paper/To-prune%2C-or-not-to-prune%3A-exploring-the-efficacy-Zhu-Gupta/3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "https://www.semanticscholar.org/paper/Dynamic-Network-Surgery-for-Efficient-DNNs-Guo-Yao/c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9", "https://www.semanticscholar.org/paper/The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen/26384278cf5d575fc32cb92c303fb648fa0d5217", "https://www.semanticscholar.org/paper/SNIP%3A-Single-shot-Network-Pruning-based-on-Lee-Ajanthan/cf440ccce4a7a8681e238b4f26d5b95109add55d", "https://www.semanticscholar.org/paper/Exploring-Sparsity-in-Recurrent-Neural-Networks-Narang-Diamos/0a5265d5f4a2b59bde18c258ad5acd26bc680769", "https://www.semanticscholar.org/paper/Rethinking-the-Smaller-Norm-Less-Informative-in-of-Ye-Lu/60464c4bd94a14b63898e322f9ea651830e54ae0", "https://www.semanticscholar.org/paper/Deep-Networks-with-Stochastic-Depth-Huang-Sun/51db1f3c8dfc7d4077da39c96bb90a6358128111"]},
{"id": "Importance-Estimation-for-Neural-Network-Pruning-Molchanov-Mallya/a6f4917d043494d2ebaebe6b65cb35e6a07fda41", "title": "Importance Estimation for Neural Network Pruning", "authors": ["Pavlo Molchanov", "Arun Mallya", "Stephen Tyree", "Iuri Frosio", "Jan Kautz"], "date": "2019", "abstract": "Structural pruning of neural network parameters reduces computational, energy, and memory transfer costs during inference. We propose a novel method that estimates the contribution of a neuron (filter) to the final loss and iteratively removes those with smaller scores. We describe two variations of our method using the first and second-order Taylor expansions to approximate a filter's contribution. Both methods scale consistently across any network layer without requiring per-layer sensitivity analysis and can be applied to any kind of layer, including skip connections. For modern networks trained on ImageNet, we measured experimentally a high (>93%) correlation between the contribution computed by our methods and a reliable estimate of the true importance. Pruning with the proposed methods led to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a 40% FLOPS reduction by removing 30% of the parameters, with a loss of 0.02% in the top-1 accuracy on ImageNet.", "references": ["https://www.semanticscholar.org/paper/Gradual-Channel-Pruning-while-Training-using-Scores-Aketi-Roy/d1f7c85c7bff8337b5d942879d6d6b18435b6d8a", "https://www.semanticscholar.org/paper/Pruning-by-Explaining%3A-A-Novel-Criterion-for-Deep-Yeom-Seegerer/ec68678a737ce0e4ee51560fdc9aa86a8a917aee", "https://www.semanticscholar.org/paper/Dependency-Aware-Filter-Pruning-Zhao-Zhang/52823a69576348639d56f34940d39dd956e11bc8", "https://www.semanticscholar.org/paper/Neural-Network-Pruning-with-Residual-Connections-Luo-Wu/97ed581635de2f9c0b284c4211447eb34b812470", "https://www.semanticscholar.org/paper/Filter-Pruning-Without-Damaging-Networks-Capacity-Zuo-Chen/4a264143ba712fecd8e6c3de33fe01097ffdbdd4", "https://www.semanticscholar.org/paper/Knapsack-Pruning-with-Inner-Distillation-Aflalo-Noy/a6b92c00c79d0d426a01980bcced6d205d68c700", "https://www.semanticscholar.org/paper/Network-Pruning-via-Transformable-Architecture-Dong-Yang/817e265620f2a62ed3171b2f7d57d56b9d601a1f", "https://www.semanticscholar.org/paper/Discrimination-aware-Network-Pruning-for-Deep-Model-Liu-Zhuang/846cc70e137dd6e090f58d1fedac8677a6ec609b", "https://www.semanticscholar.org/paper/use-pruning-together-with-layer-wise-quantization-Kautz/ad3e7d2b9633a646ce3f33010c522079ae385f5a", "https://www.semanticscholar.org/paper/A-Low-Effort-Approach-to-Structured-CNN-Design-PCA-Garg-Panda/a17de970aef314acd060a1307b91774161a08c09", "https://www.semanticscholar.org/paper/NISP%3A-Pruning-Networks-Using-Neuron-Importance-Yu-Li/af03709f0893a7ff1c2656b73249d60030bab996", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/026ecf916023e13191331a354271b7f9b86e50a1", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Rethinking-the-Smaller-Norm-Less-Informative-in-of-Ye-Lu/60464c4bd94a14b63898e322f9ea651830e54ae0", "https://www.semanticscholar.org/paper/Data-Driven-Sparse-Structure-Selection-for-Deep-Huang-Wang/2adb616a77fe28b49be2a2d66cccf2d7400e4a04", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/Channel-Pruning-for-Accelerating-Very-Deep-Neural-He-Zhang/ee53c9480132fc0d09b1192226cb2c460462fd6d", "https://www.semanticscholar.org/paper/Progressive-Deep-Neural-Networks-Acceleration-via-He-Dong/80f72cf7a0df940ef26fc99f543100cb257d0ca8"]},
{"id": "Network-Pruning-via-Transformable-Architecture-Dong-Yang/817e265620f2a62ed3171b2f7d57d56b9d601a1f", "title": "Network Pruning via Transformable Architecture Search", "authors": ["Xuanyi Dong", "Yi Yang"], "date": "2019", "abstract": "Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned network to pruned networks. To break the structure limitation of the pruned networks, we propose to apply neural architecture search to search directly for a network with flexible channel and layer sizes. The number of the channels/layers is learned by minimizing the loss of the pruned networks. The feature map of the pruned network is an aggregation of K feature map fragments (generated by K networks of different sizes), which are sampled based on the probability distribution.The loss can be back-propagated not only to the network weights, but also to the parameterized distribution to explicitly tune the size of the channels/layers. Specifically, we apply channel-wise interpolation to keep the feature map with different channel sizes aligned in the aggregation procedure. The maximum probability for the size in each distribution serves as the width and depth of the pruned network, whose parameters are learned by knowledge transfer, e.g., knowledge distillation, from the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate the effectiveness of our new perspective of network pruning compared to traditional network pruning algorithms. Various searching and knowledge transfer approaches are conducted to show the effectiveness of the two components. Code is at: this https URL.", "references": ["https://www.semanticscholar.org/paper/Knapsack-Pruning-with-Inner-Distillation-Aflalo-Noy/a6b92c00c79d0d426a01980bcced6d205d68c700", "https://www.semanticscholar.org/paper/Network-Adjustment%3A-Channel-Search-Guided-by-FLOPs-Chen-Niu/d9e540ff9f360d62c1eef7d3db13fcf8b200f916", "https://www.semanticscholar.org/paper/Densely-Connected-Search-Space-for-More-Flexible-Fang-Sun/a7445517c52143cc633c7adca9c245db885264ed", "https://www.semanticscholar.org/paper/A-Feature-map-Discriminant-Perspective-for-Pruning-Hou-Kung/959e6c5ed9f706d91aad8658787cdeac22d5c153", "https://www.semanticscholar.org/paper/Pruning-Algorithms-to-Accelerate-Convolutional-for-Liu-Tripathi/e6665dc56449f1b7d7e4e2c3db03b862b4ee02ed", "https://www.semanticscholar.org/paper/Rethinking-Class-Discrimination-Based-CNN-Channel-Liu-Wentzlaff/bf86a3d8f7d7390857db14b1b95a2948f13dc1a5", "https://www.semanticscholar.org/paper/One-Shot-Neural-Architecture-Search-via-Template-Dong-Yang/643ef3197bb9da791f2cf9e99b407e0aea12ec23", "https://www.semanticscholar.org/paper/Joint-Progressive-Knowledge-Distillation-and-Domain-Nguyen-Meidine-Granger/0054a11ba67e1ac4c6243f7fb67e202868ef98d7", "https://www.semanticscholar.org/paper/Multi-scale-multi-patch-person-re-identification-Wang-Song/10c109e86c94a241e14842b8b810b0a527295edf", "https://www.semanticscholar.org/paper/Teacher-Supervises-Students-How-to-Learn-From-for-Dong-Yang/6741eb1ce7837eed6da6402b456b55cf9e626d63", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Importance-Estimation-for-Neural-Network-Pruning-Molchanov-Mallya/a6f4917d043494d2ebaebe6b65cb35e6a07fda41", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Efficient-Architecture-Search-by-Network-Cai-Chen/84e65a5bdb735d62eef4f72c2f01af354b2285ba", "https://www.semanticscholar.org/paper/Efficient-Neural-Architecture-Search-via-Parameter-Pham-Guan/fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "https://www.semanticscholar.org/paper/Cascaded-Projection%3A-End-To-End-Network-Compression-Minnehan-Savakis/88cb92934f9ba8a9e7052870b1dd8d0bd9ad7b3d", "https://www.semanticscholar.org/paper/AutoSlim%3A-Towards-One-Shot-Architecture-Search-for-Yu-Huang/fdaf68d310a0dd6a83c7753cb1bf209d2f1d4af5", "https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8"]},
{"id": "CGaP%3A-Continuous-Growth-and-Pruning-for-Efficient-Du-Li/438b3860ee945fb47984ab417249f1d19a65f9c3", "title": "CGaP: Continuous Growth and Pruning for Efficient Deep Learning", "authors": ["Xiaocong Du", "Zheng Li", "Yu Cao"], "date": "2019", "abstract": "Today a canonical approach to reduce the computation cost of Deep Neural Networks (DNNs) is to pre-define an over-parameterized model before training to guarantee the learning capacity, and then prune unimportant learning units (filters and neurons) during training to improve model compactness. We argue it is unnecessary to introduce redundancy at the beginning of the training but then reduce redundancy for the ultimate inference model. In this paper, we propose a Continuous Growth and Pruning (CGaP) scheme to minimize the redundancy from the beginning. CGaP starts the training from a small network seed, then expands the model continuously by reinforcing important learning units, and finally prunes the network to obtain a compact and accurate model. As the growth phase favors important learning units, CGaP provides a clear learning purpose to the pruning phase. Experimental results on representative datasets and DNN architectures demonstrate that CGaP outperforms previous pruning-only approaches that deal with pre-defined structures. For VGG-19 on CIFAR-100 and SVHN datasets, CGaP reduces the number of parameters by 78.9% and 85.8%, FLOPs by 53.2% and 74.2%, respectively; For ResNet-110 On CIFAR-10, CGaP reduces 64.0% number of parameters and 63.3% FLOPs.", "references": ["https://www.semanticscholar.org/paper/AutoGrow%3A-Automatic-Layer-Growing-in-Deep-Networks-Wen-Yan/fd40e8654761ef573113bb55511f96bac890162e", "https://www.semanticscholar.org/paper/PID-Controller-Based-Stochastic-Optimization-for-Wang-Luo/c6c90998970c104d0881c688667650caf3b9b1ab", "https://www.semanticscholar.org/paper/Computation-on-Sparse-Neural-Networks%3A-an-for-Sun-Qin/0887156ff863fbea508d4c442898cf91c781b8d6", "https://www.semanticscholar.org/paper/Network-Trimming%3A-A-Data-Driven-Neuron-Pruning-Deep-Hu-Peng/60ae4f18cb53efff0174e3fea7064049737e1e67", "https://www.semanticscholar.org/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d", "https://www.semanticscholar.org/paper/Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree/3db8730c203f88d7f08a6a99e8c02a077dc9b011", "https://www.semanticscholar.org/paper/NeST%3A-A-Neural-Network-Synthesis-Tool-Based-on-a-Dai-Yin/cc2bc56b30e283bc6e7193dd42e66e56658b4875", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Soft-Filter-Pruning-for-Accelerating-Deep-Neural-He-Kang/52ff452c2c38d082c07eb434996e07a8c242a692", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "https://www.semanticscholar.org/paper/Dynamic-Network-Surgery-for-Efficient-DNNs-Guo-Yao/c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9", "https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3"]},
{"id": "Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "title": "Learning Structured Sparsity in Deep Neural Networks", "authors": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "date": "2016", "abstract": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in this https URL", "references": ["https://www.semanticscholar.org/paper/Learning-Sparse-Convolutional-Neural-Network-via-Long-Ben/3ad7db356576363c8f958a5d52e2318b39964eb1", "https://www.semanticscholar.org/paper/Joint-Optimization-of-Quantization-and-Structured-Srivastava-Kadetotad/d0c85c0204aeb13c57adea30555c3fa37acf9ebb", "https://www.semanticscholar.org/paper/A-Unified-Approximation-Framework-for-Deep-Neural-Ma-Chen/1f4ed3458a4025c80580f3bb28dadcc2a2d57b75", "https://www.semanticscholar.org/paper/A-Unified-Approximation-Framework-for-Compressing-Ma-Chen/5e125146b807793968bcb63c1ed44be74b7c1e8a", "https://www.semanticscholar.org/paper/Learning-Sparse-Patterns-in-Deep-Neural-Networks-Wen-Yang/90298494b5c150cec15f387b9e48c809c4601d0a", "https://www.semanticscholar.org/paper/Parameterized-Structured-Pruning-for-Deep-Neural-Schindler-Roth/06f8a9023f34c931e5de8c388101d84e9f9ec08a", "https://www.semanticscholar.org/paper/%F0%9D%93%810-Regularized-Structured-Sparsity-Convolutional-Bui-Park/00bfba768981126f7ae6642ed746489316e8e22a", "https://www.semanticscholar.org/paper/Toward-Compact-ConvNets-via-Structure-Sparsity-Lin-Ji/689aabd92ed086f6485489e386ac3cd897e29f35", "https://www.semanticscholar.org/paper/A-Unified-Approximation-Framework-for-Non-Linear-Ma-Chen/b89395ecbf2ab0a597142c44aee3513b32fa6d1f", "https://www.semanticscholar.org/paper/Trained-Rank-Pruning-for-Efficient-Deep-Neural-Xu-Li/ae846d1c60fea77c2344be8d1d513b6869ab9e6f", "https://www.semanticscholar.org/paper/Sparse-Convolutional-Neural-Networks-Liu-Wang/d559dd84fc473fca7e91b9075675750823935afa", "https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/Convolutional-neural-networks-with-low-rank-Tai-Xiao/d5b4721c8188269b120d3d06149a04435753e755", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Learning-the-Structure-of-Deep-Convolutional-Feng-Darrell/6cf7f474eb493b0e5aae74ccfd9cdc79e506060e", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d"]},
{"id": "Multi-Granularity-Self-Attention-for-Neural-Machine-Hao-Wang/91066eed5b158e58b004038fcb6cf1186b20791b", "title": "Multi-Granularity Self-Attention for Neural Machine Translation", "authors": ["Jie Hao", "Xing Wang", "Shuming Shi", "Jinfeng Zhang", "Zhaopeng Tu"], "date": "2019", "abstract": "Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases. In this work, we present multi-granularity self-attention (Mg-Sa): a neural network that combines multi-head self-attention and phrase modeling. Specifically, we train several attention heads to attend to phrases in either n-gram or syntactic formalism. Moreover, we exploit interactions among phrases to enhance the strength of structure modeling - a commonly-cited weakness of self-attention. Experimental results on WMT14 English-to-German and NIST Chinese-to-English translation tasks show the proposed approach consistently improves performance. Targeted linguistic analysis reveals that Mg-Sa indeed captures useful phrase information at various levels of granularities.", "references": ["https://www.semanticscholar.org/paper/Reweighted-Proximal-Pruning-for-Large-Scale-Guo-Liu/59b9c79b849f068ba4eea8ace6252070b8c9f209", "https://www.semanticscholar.org/paper/Towards-Better-Modeling-Hierarchical-Structure-for-Hao/64e2a920b8b070fc02b06a5c3f870f0a915b82ab", "https://www.semanticscholar.org/paper/Towards-Better-Modeling-Hierarchical-Structure-for-Hao-Wang/a940e88117d19160f276e9b6ae014df699495a3f", "https://www.semanticscholar.org/paper/Image-Captioning-through-Image-Transformer-He-Liao/a4d75d6cac5e87548ef44f84d5769efa17f4bf6f", "https://www.semanticscholar.org/paper/Phrase-level-Self-Attention-Networks-for-Universal-Wu-Wang/cd0bb48f0f26f146759654bdc2dd0aad87e917de", "https://www.semanticscholar.org/paper/Linguistically-Informed-Self-Attention-for-Semantic-Strubell-Verga/060ff1aad5619a7d6d6cdfaf8be5da29bff3808c", "https://www.semanticscholar.org/paper/Phrase-Based-Attentions-Nguyen-Joty/ed758547ff806b913525aba901d4279acf4d6c93", "https://www.semanticscholar.org/paper/Tree-to-Sequence-Attentional-Neural-Machine-Eriguchi-Hashimoto/db02cd07726371790a825208cec377ec15f5b5f1", "https://www.semanticscholar.org/paper/An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann/94238dead40b12735d79ed63e29ead70730261a2", "https://www.semanticscholar.org/paper/Syntax-Enhanced-Neural-Machine-Translation-with-Zhang-Li/9e9d9bea25b030e760e2057b70d95e846587ece3", "https://www.semanticscholar.org/paper/Phrase-Table-as-Recommendation-Memory-for-Neural-Zhao-Wang/13447e2ed4d95232e73e2d0f2c77b5f23a1d0a0e", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff", "https://www.semanticscholar.org/paper/Translating-Phrases-in-Neural-Machine-Translation-Wang-Tu/9af39292b9407888b98df2c697d92be229be9a07", "https://www.semanticscholar.org/paper/Analyzing-Multi-Head-Self-Attention%3A-Specialized-Do-Voita-Talbot/07a64686ce8e43ac475a8d820a8a9f1d87989583"]},
{"id": "On-the-Linguistic-Representational-Power-of-Neural-Belinkov-Durrani/2d94a995267903535f78c661a341ee34eabc1ec0", "title": "On the Linguistic Representational Power of Neural Machine Translation Models", "authors": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "date": "2020", "abstract": "Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.", "references": ["https://www.semanticscholar.org/paper/What-do-Neural-Machine-Translation-Models-Learn-Belinkov-Durrani/82364428995c29b3dcb60c1835548eeff4adcd20", "https://www.semanticscholar.org/paper/Linguistic-Knowledge-and-Transferability-of-Liu-Gardner/f6fbb6809374ca57205bd2cf1421d4f4fa04f975", "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59", "https://www.semanticscholar.org/paper/On-internal-language-representations-in-deep-an-of-Belinkov/0c43dfe8a834fce0467ba6a74b2daeebb5bb8b53", "https://www.semanticscholar.org/paper/Evaluating-Layers-of-Representation-in-Neural-on-Belinkov-Villodre/dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d", "https://www.semanticscholar.org/paper/Language-Modeling-Teaches-You-More-Syntax-than-Task-Zhang-Bowman/26f7305e4cf293b3daa672f0f75c1b0bac1e873a", "https://www.semanticscholar.org/paper/An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann/94238dead40b12735d79ed63e29ead70730261a2", "https://www.semanticscholar.org/paper/What-Is-One-Grain-of-Sand-in-the-Desert-Analyzing-Dalvi-Durrani/9c2156bc35c6f8e68aa21d4b2f339134a4d28708", "https://www.semanticscholar.org/paper/Evaluating-Discourse-Phenomena-in-Neural-Machine-Bawden-Sennrich/5503ed99ad2a536e00508432e87553786e1eaa3f", "https://www.semanticscholar.org/paper/The-Lazy-Encoder%3A-A-Fine-Grained-Analysis-of-the-of-Bisazza-Tump/af96e98684c278b66769b4656af069585ac6df85"]},
{"id": "Context-Aware-Self-Attention-Networks-Yang-Li/5d1a6d27d67e0b8d4a049f7f5dc3995f837d7976", "title": "Context-Aware Self-Attention Networks", "authors": ["Baosong Yang", "Jian Li", "Derek F. Wong", "Lidia S. Chao", "Xing Wang", "Zhaopeng Tu"], "date": "2019", "abstract": "Self-attention model have shown its flexibility in parallel computation and the effectiveness on modeling both long- and short-term dependencies. However, it calculates the dependencies between representations without considering the contextual information, which have proven useful for modeling dependencies among neural representations in various natural language tasks. In this work, we focus on improving self-attention networks through capturing the richness of context. To maintain the simplicity and flexibility of the self-attention networks, we propose to contextualize the transformations of the query and key layers, which are used to calculates the relevance between elements. Specifically, we leverage the internal representations that embed both global and deep contexts, thus avoid relying on external resources. Experimental results on WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed methods. Furthermore, we conducted extensive analyses to quantity how the context vectors participate in the self-attention model.", "references": ["https://www.semanticscholar.org/paper/Leveraging-Local-and-Global-Patterns-for-Networks-Xu-Wong/2353f65b42e9f445425568088c5adef300a7f573", "https://www.semanticscholar.org/paper/Convolutional-Self-Attention-Networks-Yang-Wang/bdc046e65bc80cf13929ca0c3934d6faee830723", "https://www.semanticscholar.org/paper/Self-Attention-with-Structural-Position-Wang-Tu/4ca4e7ed290617aab8b4d99294d10b0ac8372967", "https://www.semanticscholar.org/paper/Bidirectional-Context-Aware-Hierarchical-Attention-Remy-Tixier/b67e44fb9f92f9c331d52363315a9366bbf812d6", "https://www.semanticscholar.org/paper/Improving-Neural-Machine-Translation-with-Bugliarello-Okazaki/b26bfc6fa8aff11181cf1231e36752e8c33e080c", "https://www.semanticscholar.org/paper/Cross-Aggregation-of-Multi-head-Attention-for-Cao-Hai/4d68c1b4167f858979c6a8e8b9ad0b484cd48c63", "https://www.semanticscholar.org/paper/Normalized-and-Geometry-Aware-Self-Attention-for-Guo-Liu/833560cd68a3e3d1be1bc650756dd6c679798551", "https://www.semanticscholar.org/paper/On-the-localness-modeling-for-the-self-attention-Yang-Lu/ac0e0e6cfb5bac417c7769867b1366bd64d1b8bd", "https://www.semanticscholar.org/paper/Modeling-Recurrence-for-Transformer-Hao-Wang/68a47a65c1ffd5d1540f12adde1a7300594c9969", "https://www.semanticscholar.org/paper/Sequential-Recommendation-with-Relation-Aware-Ji-Joo/3c9fe50e8237ffb083c2cd36551ecf5f4bcf124d", "https://www.semanticscholar.org/paper/Modeling-Localness-for-Self-Attention-Networks-Yang-Tu/1af138dc72fa855cc3bc9c0b83750b461c26e29d", "https://www.semanticscholar.org/paper/Convolutional-Self-Attention-Network-Yang-Wang/611a2dc033e5c02460b3ebc26c5297b4a1d0eb52", "https://www.semanticscholar.org/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f", "https://www.semanticscholar.org/paper/DiSAN%3A-Directional-Self-Attention-Network-for-Shen-Zhou/adc276e6eae7051a027a4c269fb21dae43cadfed", "https://www.semanticscholar.org/paper/Why-Self-Attention-A-Targeted-Evaluation-of-Neural-Tang-M%C3%BCller/e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1", "https://www.semanticscholar.org/paper/Self-Attentional-Acoustic-Models-Sperber-Niehues/c52ac453e154953abdb06fc041023e327ea609a4", "https://www.semanticscholar.org/paper/Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Towards-Bidirectional-Hierarchical-Representations-Yang-Wong/e4edecc44960c0d1c0ed63658251af9830c23097", "https://www.semanticscholar.org/paper/A-Structured-Self-attentive-Sentence-Embedding-Lin-Feng/204a4a70428f3938d2c538a4d74c7ae0416306d8"]},
{"id": "Simplifying-Neural-Machine-Translation-with-Zhang-Xiong/e7fa0af1fef0b219e122bbf66d792b131d0da42b", "title": "Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su", "Qian Lin", "Huiji Zhang"], "date": "2018", "abstract": "In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English- German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.", "references": ["https://www.semanticscholar.org/paper/A-Lightweight-Recurrent-Network-for-Sequence-Zhang-Sennrich/60396a7643fd25fba52d73028bc563f5ad651bb6", "https://www.semanticscholar.org/paper/A-Lightweight-Recurrent-Network-for-Sequence-Korhonen-M%C3%A0rquez/474bb7ca4ed0f26df0ef9c195c20c797d2e39fca", "https://www.semanticscholar.org/paper/A-Survey-of-Deep-Learning-Techniques-for-Neural-Yang-Wang/3fa8d2a9e9a9cf3ee9626424a157888580dcfaba", "https://www.semanticscholar.org/paper/Improving-text-simplification-by-corpus-expansion-Katsuta-Yamamoto/eeef24e3f0385959ad0e5cf1a7217b2fe117074c", "https://www.semanticscholar.org/paper/Deep-Neural-Machine-Translation-with-Linear-Unit-Wang-Lu/ff4dde4e30236747fa2b3539a4bfcc786f1ae6bc", "https://www.semanticscholar.org/paper/Deep-Neural-Machine-Translation-with-Units-Gangi-Federico/a33b1e5c67be044005d5d48be4eb05c6da22aa77", "https://www.semanticscholar.org/paper/A-Hierarchy-to-Sequence-Attentional-Neural-Machine-Su-Zeng/1a53cceae70964af9d901542820eec2474118179", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff", "https://www.semanticscholar.org/paper/A-Convolutional-Encoder-Model-for-Neural-Machine-Gehring-Auli/f958d4921951e394057a1c4ec33bad9a34e5dad1", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/A-GRU-Gated-Attention-Model-for-Neural-Machine-Zhang-Xiong/a3143ea1bc45933da261af5b01fd4040ac47b0b6", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Asynchronous-Bidirectional-Decoding-for-Neural-Zhang-Su/0a2b9345a8028f92ce0340b7d5b947352f580a09"]},
{"id": "Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "title": "Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation", "authors": ["Chenze Shao", "Yang Feng", "Jinchao Zhang", "Fandong Meng", "Xilin Chen", "Jie Zhou"], "date": "2019", "abstract": "Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup.", "references": ["https://www.semanticscholar.org/paper/Minimizing-the-Bag-of-Ngrams-Difference-for-Neural-Shao-Zhang/775f6da764a134e7bd0e361c27f7d7411afef4d3", "https://www.semanticscholar.org/paper/Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "https://www.semanticscholar.org/paper/NON-AUTOREGRESSIVE-MACHINE-TRANSLATION-Zhou-Gu/dec6bb3c7bb671c86296a2a089e0e38aa3f69279", "https://www.semanticscholar.org/paper/Understanding-Knowledge-Distillation-in-Machine-Zhou-Neubig/1e5b826ddf0754f6e93234ba1260bd939c255e7f", "https://www.semanticscholar.org/paper/Analyzing-Word-Translation-of-Transformer-Layers-Xu-Genabith/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9", "https://www.semanticscholar.org/paper/Non-autoregressive-Transformer-by-Position-Learning-Bao-Zhou/34e75b1eb986ceab79e5c463cf1a82bcf4e87944", "https://www.semanticscholar.org/paper/Masked-Translation-Model-Nix-Kim/93800f8d7dd7647167d2a41bc479fa8cab7b24f3", "https://www.semanticscholar.org/paper/Neural-Machine-Translation%3A-Challenges%2C-Progress-Zhang-Zong/ed4195aeddec775ead19569205a174f2dd8f2528", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/Semi-Autoregressive-Neural-Machine-Translation-Wang-Zhang/6a1c61a0da5f56a3fdfca5515767cfd74529524a", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Fast-Decoding-in-Sequence-Models-using-Discrete-Kaiser-Roy/2d08ed53491053d84b6de89aedbf2178b9c8cf84", "https://www.semanticscholar.org/paper/Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "https://www.semanticscholar.org/paper/Greedy-Search-with-Probabilistic-N-gram-Matching-Shao-Feng/b35ea8aca56f2587379ceaf33b9203a5bd8d264d", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Trainable-Greedy-Decoding-for-Neural-Machine-Gu-Cho/137347897a3f46fa2d67d925ec2bb3e71cde1f27", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},
{"id": "Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation", "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "date": "2015", "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1", "references": ["https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Bilingual-History-Xue-Feng/1d95aa40663a49ba4c58c6b1cc60ed676dc437fd", "https://www.semanticscholar.org/paper/Look-Ahead-Attention-for-Generation-in-Neural-Zhou-Zhang/84a90877a317dd28d836fcb0e5c1fb292d3b91bc", "https://www.semanticscholar.org/paper/Transferring-Emphasis-in-Speech-Translation-Using-Do-Sakti/ddfd297531f56121b8383bd1eb2bb09189ab2e2b", "https://www.semanticscholar.org/paper/Syntax-Directed-Attention-for-Neural-Machine-Chen-Wang/4ddd5d6c973632f977ff3a92c3233e41f097b096", "https://www.semanticscholar.org/paper/Multiway-Attention-for-Neural-Machine-Translation-Liu-Wang/440eecd93ed991c4bf417e27395968b44448562e", "https://www.semanticscholar.org/paper/An-Effective-Coverage-Approach-for-Attention-based-Nguyen-Nguyen/3e858cc107ef38a25837fdafcef06763dbb68c5a", "https://www.semanticscholar.org/paper/Improving-Neural-Machine-Translation-with-Bugliarello-Okazaki/b26bfc6fa8aff11181cf1231e36752e8c33e080c", "https://www.semanticscholar.org/paper/Temporal-Attention-Model-for-Neural-Machine-Sankaran-Mi/2f160ce71f01ac2043de67536ff0e413ff6f58c5", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Decoding-History-Wang-Xie/558272dd8e4d840560a5a247b0e989c48f8a97aa", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Dynamic-Selection-Han-Mu/e080487556381239ae57f77e7fd82c7a48e974a2", "https://www.semanticscholar.org/paper/Addressing-the-Rare-Word-Problem-in-Neural-Machine-Luong-Sutskever/1956c239b3552e030db1b78951f64781101125ed", "https://www.semanticscholar.org/paper/On-Using-Very-Large-Target-Vocabulary-for-Neural-Jean-Cho/1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e", "https://www.semanticscholar.org/paper/Show%2C-Attend-and-Tell%3A-Neural-Image-Caption-with-Xu-Ba/4d8f2d14af5991d4f0d050d22216825cac3157bd", "https://www.semanticscholar.org/paper/End-to-end-Continuous-Speech-Recognition-using-NN%3A-Chorowski-Bahdanau/47d2dc34e1d02a8109f5c04bb6939725de23716d", "https://www.semanticscholar.org/paper/N-gram-Counts-and-Language-Models-from-the-Common-Buck-Heafield/8e4fb17fff38a7834af5b4eaafcbbde02bf00975"]},
{"id": "Part-of-Speech-Tagging-with-Neural-Architecture-Bingham/d65400fc9316368117d9cec8e02ebc322b8793de", "title": "Part of Speech Tagging with Neural Architecture Search", "authors": ["Garrett Bingham"], "date": "2019", "abstract": "Recent neural architecture search techniques have achieved state-of-the-art performance in image classification and language modeling, but applications to other machine learning tasks have been lacking. We present the first application of neural architecture search to part of speech tagging. We adapt the DARTS approach to neural architecture search (Liu et al., 2018) to part of speech tagging and achieve encouraging performance. We improve this by introducing BiDARTS, a bidirectional version of DARTS. BiDARTS consistently outperforms DARTS and is competitive with a state-of-the-art part of speech tagger.", "references": ["https://www.semanticscholar.org/paper/Neural-Architecture-Search%3A-A-Survey-Elsken-Metzen/114a32bc872f160b58f503aca13f887556b5006e", "https://www.semanticscholar.org/paper/Neural-Architecture-Search-with-Reinforcement-Zoph-Le/67d968c7450878190e45ac7886746de867bf673d", "https://www.semanticscholar.org/paper/DARTS%3A-Differentiable-Architecture-Search-Liu-Simonyan/c1f457e31b611da727f9aef76c283a18157dfa83", "https://www.semanticscholar.org/paper/An-improved-neural-network-model-for-joint-POS-and-Nguyen-Verspoor/710cad18de252b51368895e85751683f0a10d8fb", "https://www.semanticscholar.org/paper/Resource-Efficient-Neural-Architect-Zhou-Ebrahimi/f7892f3a6d096cd65de0321a1901720d886c3e62", "https://www.semanticscholar.org/paper/From-Nodes-to-Networks%3A-Evolving-Recurrent-Neural-Rawal-Miikkulainen/4707fc778ca4c923553fe4baa6c23c4150220d3f", "https://www.semanticscholar.org/paper/Learning-Transferable-Architectures-for-Scalable-Zoph-Vasudevan/d0611891b9e8a7c5731146097b6f201578f47b2f", "https://www.semanticscholar.org/paper/Practical-Block-Wise-Neural-Network-Architecture-Zhong-Yan/8a1ce657dd41a4f49990a4769000dc8049b83404", "https://www.semanticscholar.org/paper/Regularized-Evolution-for-Image-Classifier-Search-Real-Aggarwal/50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "https://www.semanticscholar.org/paper/Designing-Neural-Network-Architectures-using-Baker-Gupta/6cd5dfccd9f52538b19a415e00031d0ee4e5b181"]},
{"id": "Automated-Architecture-Design-for-Deep-Neural-Abreu/475315fa75357c116e845ee98d7a1410dd07887e", "title": "Automated Architecture Design for Deep Neural Networks", "authors": ["Steven Abreu"], "date": "2019", "abstract": "Machine learning has made tremendous progress in recent years and received large amounts of public attention. Though we are still far from designing a full artificially intelligent agent, machine learning has brought us many applications in which computers solve human learning tasks remarkably well. Much of this progress comes from a recent trend within machine learning, called deep learning. Deep learning models are responsible for many state-of-the-art applications of machine learning. Despite their success, deep learning models are hard to train, very difficult to understand, and often times so complex that training is only possible on very large GPU clusters. Lots of work has been done on enabling neural networks to learn efficiently. However, the design and architecture of such neural networks is often done manually through trial and error and expert knowledge. This thesis inspects different approaches, existing and novel, to automate the design of deep feedforward neural networks in an attempt to create less complex models with good performance that take away the burden of deciding on an architecture and make it more efficient to design and train such deep networks.", "references": ["https://www.semanticscholar.org/paper/On-the-Complexity-of-Neural-Network-Classifiers%3A-A-Bianchini-Scarselli/2a21d9a484167c0c1dca6a42077afb7d5fe552a2", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf", "https://www.semanticscholar.org/paper/Neural-Architecture-Search%3A-A-Survey-Elsken-Metzen/114a32bc872f160b58f503aca13f887556b5006e", "https://www.semanticscholar.org/paper/Deep-Learning-Goodfellow-Bengio/a4cec122a08216fe8a3bc19b22e78fbaea096256", "https://www.semanticscholar.org/paper/Regularized-Evolution-for-Image-Classifier-Search-Real-Aggarwal/50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d", "https://www.semanticscholar.org/paper/An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan/b8012351bc5ebce4a4b3039bbbba3ce393bc3315", "https://www.semanticscholar.org/paper/Forward-Thinking%3A-Building-and-Training-Neural-One-Hettinger-Christensen/7344a30ca24fe213325a6570a1bc74a138915879", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},
{"id": "Pairwise-coupling-of-convolutional-neural-networks-Such-Tar%C3%A1bek/56824c3bb9a02ec254e59a7477ad6efaaede8904", "title": "Pairwise coupling of convolutional neural networks for better explicability of classification systems", "authors": ["Ondrej Such", "Peter Tar\u00e1bek", "Katar\u00edna Bachrat\u00e1", "Andrea Tinajov\u00e1"], "date": "2019", "abstract": "We examine several aspects of explicability of a classification system built from neural networks. The first aspect is the pairwise explicability, which is the ability to provide the most accurate prediction when the range of possibilities is narrowed to just two. Next we consider explicability in development, which means ability to make incremental improvement in prediction accuracy based on observed deficiency of the system. Inherent stochasticity of neural network based classifiers can be interpreted using likelihood randomness explicability. Finally, sureness explicability indicates confidence of the classifying system to make any prediction at all. \nThese concepts are examined in the framework of pairwise coupling, which is a non-trainable metamodel that originated during development of support vector machines. Several methodologies are evaluated, of which the key one is shown to be the choice of the pairwise coupling method. We compare two methods: the established Wu-Lin-Weng method with the recently proposed Bayes covariant method. Our experiments indicate that the Wu-Lin-Weng method gives more weight to a single pairwise classifier, whereas the latter tries to balance information from the whole matrix of pairwise likelihoods. This translates into higher accuracy, and better sureness predictions for the Bayes covariant method. \nPairwise coupling methodology has its costs, especially in terms of the number of parameters (but not necessarily in terms of training costs). However, when additional explicability aspects beyond accuracy are desired in an application, the pairwise coupling models are a promising alternative to the established methodology.", "references": ["https://www.semanticscholar.org/paper/Neural-Pairwise-Classification-Models-Created-by-Such-Kontsek/5ce27861ec38dd18abd0852afa7b680b9c7f59d6", "https://www.semanticscholar.org/paper/Hierarchical-Multi-label-Classification-using-Fully-Zhang-Shah/9ec3176b35a716f58724688ce4018e7e1302b47c", "https://www.semanticscholar.org/paper/Multiclass-Classification-with-Pairwise-Coupled-or-Mayoraz/d0d55c208d13209151d90ab4e0b78c0d58a37a76", "https://www.semanticscholar.org/paper/Pairwise-Neural-Network-Classifiers-with-Outputs-Price-Knerr/69205cf740895c59ff211af4d66c7b1a0c3304a0", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d", "https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3", "https://www.semanticscholar.org/paper/Energy-and-Policy-Considerations-for-Deep-Learning-Strubell-Ganesh/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "https://www.semanticscholar.org/paper/Classification-by-pairwise-coupling-of-imprecise-Quost-Destercke/f37c6a03fbfb70bd49df270b29dc5e18e0ab98c8", "https://www.semanticscholar.org/paper/Labelling-strategies-for-hierarchical-multi-label-Triguero-Vens/e1ab9b55e8b85f08913bb01af912d076095fce9a", "https://www.semanticscholar.org/paper/Classification-by-Pairwise-Coupling-Hastie-Tibshirani/f642a692da944604a7df590e9f9fa06089b7991a"]},
{"id": "Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "title": "Learning Sparse Neural Networks through L0 Regularization", "authors": ["Christos Louizos", "Max Welling", "Diederik P. Kingma"], "date": "2018", "abstract": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.", "references": ["https://www.semanticscholar.org/paper/Adaptive-Neural-Connections-for-Sparsity-Learning-Gain-Kaushik/8804369e808e7c4a2325dcef9bec176e2b56aa81", "https://www.semanticscholar.org/paper/SPARSE-DEEP-NEURAL-NETWORKS-USING-L1%2C%E2%88%9E-WEIGHT-Xu-Zheng/99c76174095f69af059dcb92bc9b4499549bdbfb", "https://www.semanticscholar.org/paper/Convergence-of-a-Relaxed-Variable-Splitting-Method-Dinh-Xin/15ba368354bda3d2a3b717953cc715a805858016", "https://www.semanticscholar.org/paper/ProxSGD%3A-Training-Structured-Neural-Networks-under-Yang-Yuan/d3a40ade460c40991b4719188f4f762518b9a0dc", "https://www.semanticscholar.org/paper/Learned-Threshold-Pruning-Azarian-Bhalgat/a0cfa5d2512740ece7c5cbcf6738b797e197da84", "https://www.semanticscholar.org/paper/%24%5Cell_0%24-Regularized-Structured-Sparsity-Neural-Bui-Park/5aff03c05e00b80c50c3455fb8766c09c3e46710", "https://www.semanticscholar.org/paper/L0-ARM%3A-Network-Sparsification-via-Stochastic-Li-Ji/4f623668949fdaa3202df73655ed2fdb88b17d89", "https://www.semanticscholar.org/paper/%F0%9D%93%810-Regularized-Structured-Sparsity-Convolutional-Bui-Park/00bfba768981126f7ae6642ed746489316e8e22a", "https://www.semanticscholar.org/paper/DeepHoyer%3A-Learning-Sparser-Neural-Network-with-Yang-Wen/8d42c1e2bf782e81b991e7251fed8134b330b04a", "https://www.semanticscholar.org/paper/Nonconvex-sparse-regularization-for-deep-neural-and-Ohn-Kim/6f3e188f2d9cb05930a111d79052c0fe11042abb", "https://www.semanticscholar.org/paper/Structured-Bayesian-Pruning-via-Log-Normal-Noise-Neklyudov-Molchanov/d773718f36ee1cc5bb9bc5b01afa8f76d09f452f", "https://www.semanticscholar.org/paper/Conditional-Computation-in-Neural-Networks-for-Bengio-Bacon/fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Neural-Variational-Inference-and-Learning-in-Belief-Mnih-Gregor/018300f5f0e679cee5241d9c69c8d88e00e8bf31", "https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8", "https://www.semanticscholar.org/paper/Estimating-or-Propagating-Gradients-Through-Neurons-Bengio-L%C3%A9onard/62c76ca0b2790c34e85ba1cce09d47be317c7235", "https://www.semanticscholar.org/paper/Variational-Dropout-and-the-Local-Trick-Blum-Haghtalab/f0ddb2bc6e5464d992ddbcdfdc7e894150fc81f2", "https://www.semanticscholar.org/paper/Variational-Dropout-Sparsifies-Deep-Neural-Networks-Molchanov-Ashukha/34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5", "https://www.semanticscholar.org/paper/A-Structured-Variational-Auto-encoder-for-Learning-Salimans/ef6d96b44a30d848aa55af842ef78fe77369a8cf", "https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02"]},
{"id": "Compression-of-fully-connected-layer-in-neural-by-Wu/9c6a63ed20988fc20f51a253c6fd1e871fe02bb3", "title": "Compression of fully-connected layer in neural network by Kronecker product", "authors": ["Jia-Nan Wu"], "date": "2015", "abstract": "In this paper we propose and study a technique to reduce the number of parameters in fully-connected layers of neural networks using Kronecker product, at a mild cost of the prediction quality. The technique proceeds by replacing fully-connected layers with so-called Kronecker fully-connected layers, where the weight matrices of the fully-connected layers are approximated by linear combinations of multiple Kronecker products of smaller matrices. Just as the Kronecker product is a generalization of the outer product from vectors to matrices, our method is a generalization of the low rank approximation method for fully-connected layers. We also use combinations of different shapes of Kronecker product to increase modelling capacity. Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve 10x reduction of number of parameters with less than 1% drop in accuracy, showing the effectiveness and efficiency of our method.", "references": ["https://www.semanticscholar.org/paper/Deep-Graph-Convolutional-Image-Denoising-Valsesia-Fracastoro/c275e2c3bedb5a424602317b3f15544f18c11c10", "https://www.semanticscholar.org/paper/Image-Denoising-with-Graph-Convolutional-Neural-Valsesia-Fracastoro/be14c73c06da4a75c7ee06baa983044d7fd1c2a9", "https://www.semanticscholar.org/paper/Pushing-the-limits-of-RNN-Compression-Thakker-Fedorov/da42d411b689637cbf1be9334fd262c761d82789", "https://www.semanticscholar.org/paper/Compressing-RNNs-for-IoT-devices-by-15-38x-using-Thakker-Beu/323d6fd137450a114483c79944fe3854390cca2f", "https://www.semanticscholar.org/paper/Establishing-Effective-Techniques-for-Increasing-Sunesson/633f3735247dd6f9bfa72214a2ac25d629448041", "https://www.semanticscholar.org/paper/An-Exploration-of-Parameter-Redundancy-in-Deep-with-Cheng-Yu/5934400081d9541339da0f16d2613263f1a4c2a2", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-Using-Lebedev-Ganin/62e348e26976c3ef77909b9af9788ebc2509009a", "https://www.semanticscholar.org/paper/Compressing-Convolutional-Neural-Networks-Chen-Wilson/2a9cf601068ce67994dc70b159be69073f89276f", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Speeding-Up-Neural-Networks-for-Large-Scale-using-Bakhtiary-Lapedriza/7de11e686c16f7dbe720bef17345bd5c4dd50b84", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Efficient-and-accurate-approximations-of-nonlinear-Zhang-Zou/b64601d509711468f5d085261d463846f36785b2", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff"]},
{"id": "Regularization-of-Deep-Neural-Networks-with-Dropout-Khan-Hayat/eddefcaae5df88acf6719f16f2b2528921448fb3", "title": "Regularization of Deep Neural Networks with Spectral Dropout", "authors": ["Salman H. Khan", "Munawar Hayat", "Fatih Murat Porikli"], "date": "2019", "abstract": "The big breakthrough on the ImageNet challenge in 2012 was partially due to the 'Dropout' technique used to avoid overfitting. Here, we introduce a new approach called 'Spectral Dropout' to improve the generalization ability of deep neural networks. We cast the proposed approach in the form of regular Convolutional Neural Network (CNN) weight layers using a decorrelation transform with fixed basis functions. Our spectral dropout method prevents overfitting by eliminating weak and 'noisy' Fourier domain coefficients of the neural network activations, leading to remarkably better results than the current regularization methods. Furthermore, the proposed is very efficient due to the fixed basis functions used for spectral transformation. In particular, compared to Dropout and Drop-Connect, our method significantly speeds up the network convergence rate during the training process (roughly \u00d72), with considerably higher neuron pruning rates (an increase of \u223c30%). We demonstrate that the spectral dropout can also be used in conjunction with other regularization approaches resulting in additional performance gains.", "references": ["https://www.semanticscholar.org/paper/A-noise-based-stabilizer-for-convolutional-neural-Geete-Pandey/211d7503d679953619cf31847863c4078eadc5d0", "https://www.semanticscholar.org/paper/Theory-of-adaptive-SVD-regularization-for-deep-Bejani-Ghatee/9a98be6276029ac2ff1166d26e12d63440b93144", "https://www.semanticscholar.org/paper/Adaptive-Low-Rank-Factorization-to-regularize-and-Bejani-Ghatee/b9f154785e6e8c9065cac1b1d881719701aa2e29", "https://www.semanticscholar.org/paper/Spectral-Pooling-Based-CNN-Model-Compression-for-Zhang-Yang/76407c847f7d13be259dcedcbe794224afa9ccad", "https://www.semanticscholar.org/paper/Impact-of-deep-learning-based-dropout-on-shallow-to-Piotrowski-Napiorkowski/123120600adbcc6dd5282b3589ae28a650e3953d", "https://www.semanticscholar.org/paper/Convolutional-regularization-methods-for-4D%2C-x-ray-Clark-Badea/c75fd68befb0bb57f0f2809bf9be01abae8703a2", "https://www.semanticscholar.org/paper/Prostate-segmentation-in-MRI-using-a-convolutional-Karimi-Samei/ba8e724b0fc076f232e61074c8e002609355d06c", "https://www.semanticscholar.org/paper/Robust-manifold-broad-learning-system-for-noisy-A-Feng-Ren/1d6bb2edc56153e368c8c33e6a5652bebdc775e3", "https://www.semanticscholar.org/paper/An-Efficient-Hardware-Oriented-Dropout-Algorithm-Yeoh-Morie/af6c30b9f272bef061ac94cba35abbab977a492c", "https://www.semanticscholar.org/paper/A-discretely-adaptive-connection-logic-network-Correll/66f068058a67d1efb9a8311276ae80b7e4244daf", "https://www.semanticscholar.org/paper/Spectral-Representations-for-Convolutional-Neural-Rippel-Snoek/0e37c8f19eefeb0c20d92f5cb4df4153077c116b", "https://www.semanticscholar.org/paper/Deep-Networks-with-Stochastic-Depth-Huang-Sun/51db1f3c8dfc7d4077da39c96bb90a6358128111", "https://www.semanticscholar.org/paper/Deeply-Supervised-Nets-Lee-Xie/fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "https://www.semanticscholar.org/paper/An-Exploration-of-Parameter-Redundancy-in-Deep-with-Cheng-Yu/5934400081d9541339da0f16d2613263f1a4c2a2", "https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3", "https://www.semanticscholar.org/paper/Stochastic-Pooling-for-Regularization-of-Deep-Zeiler-Fergus/0abb49fe138e8fb7332c26b148a48d0db39724fc", "https://www.semanticscholar.org/paper/Network-In-Network-Lin-Chen/5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},
{"id": "A-Low-Effort-Approach-to-Structured-CNN-Design-PCA-Garg-Panda/a17de970aef314acd060a1307b91774161a08c09", "title": "A Low Effort Approach to Structured CNN Design Using PCA", "authors": ["Isha Garg", "Priyadarshini Panda", "Kaushik Roy"], "date": "2020", "abstract": "Deep learning models hold state of the art performance in many fields, yet their design is still based on heuristics or grid search methods that often result in overparametrized networks. This work proposes a method to analyze a trained network and deduce an optimized, compressed architecture that preserves accuracy while keeping computational costs tractable. Model compression is an active field of research that targets the problem of realizing deep learning models in hardware. However, most pruning methodologies tend to be experimental, requiring large compute and time intensive iterations of retraining the entire network. We introduce structure into model design by proposing a single shot analysis of a trained network that serves as a first order, low effort approach to dimensionality reduction, by using PCA (Principal Component Analysis). The proposed method simultaneously analyzes the activations of each layer and considers the dimensionality of the space described by the filters generating these activations. It optimizes the architecture in terms of number of layers, and number of filters per layer without any iterative retraining procedures, making it a viable, low effort technique to design efficient networks. We demonstrate the proposed methodology on AlexNet and VGG style networks on the CIFAR-10, CIFAR-100 and ImageNet datasets, and successfully achieve an optimized architecture with a reduction of up to 3.8X and 9X in the number of operations and parameters respectively, while trading off less than 1% accuracy. We also apply the method to MobileNet, and achieve 1.7X and 3.9X reduction in the number of operations and parameters respectively, while improving accuracy by almost one percentage point.", "references": ["https://www.semanticscholar.org/paper/Gradual-Channel-Pruning-while-Training-using-Scores-Aketi-Roy/d1f7c85c7bff8337b5d942879d6d6b18435b6d8a", "https://www.semanticscholar.org/paper/Activation-Density-driven-Energy-Efficient-Pruning-Foldy-Porto-Panda/df8f28e7ca93435dea911b88e3f56724ae900259", "https://www.semanticscholar.org/paper/Structured-Compression-and-Sharing-of-Space-for-Saha-Garg/a0e8e5d6520054e867189c6df0eeb93775ce921a", "https://www.semanticscholar.org/paper/Constructing-Energy-efficient-Mixed-precision-for-Chakraborty-Roy/294d77271a39d49cae642df4d0ab1c993822510e", "https://www.semanticscholar.org/paper/Scaling-Deep-Spiking-Neural-Networks-with-Binary-Roy-Chakraborty/668b2ffffd7807fdbe527ee6c2197d3d95870be9", "https://www.semanticscholar.org/paper/Feature-Space-Saturation-during-Training-Shenk-Richter/531eaa7281d5c6f1713c343d321535fadc837d9b", "https://www.semanticscholar.org/paper/PCA-driven-Hybrid-network-design-for-enabling-at-Chakraborty-Roy/4f5d3cf86fca2a04a9cb96e582bd0aae3cb7b663", "https://www.semanticscholar.org/paper/Principal-Component-Networks%3A-Parameter-Reduction-Waleffe-Rekatsinas/fef6a936ae6b9e23c9daa9a839bc4785f305bdae", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/SNIP%3A-Single-shot-Network-Pruning-based-on-Lee-Ajanthan/cf440ccce4a7a8681e238b4f26d5b95109add55d", "https://www.semanticscholar.org/paper/Auto-Balanced-Filter-Pruning-for-Efficient-Neural-Ding-Ding/e7da662ca8da8f7f7a39f2ead041fa9c77e5d90c", "https://www.semanticscholar.org/paper/Net-Trim%3A-Convex-Pruning-of-Deep-Neural-Networks-Aghasi-Abdi/6e1ee3bef407bf5562bedbc56e719e0829fa0a64", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/NISP%3A-Pruning-Networks-Using-Neuron-Importance-Yu-Li/af03709f0893a7ff1c2656b73249d60030bab996", "https://www.semanticscholar.org/paper/Structured-Binary-Neural-Networks-for-Accurate-and-Zhuang-Shen/78e1c397fe469ae4dfce3770c8352c397d63fc43", "https://www.semanticscholar.org/paper/Compression-aware-Training-of-Deep-Networks-Alvarez-Salzmann/45a154f8be8ec31821a0e409d4b69635670a2e1e", "https://www.semanticscholar.org/paper/Efficient-and-accurate-approximations-of-nonlinear-Zhang-Zou/b64601d509711468f5d085261d463846f36785b2", "https://www.semanticscholar.org/paper/Quantized-CNN%3A-A-Unified-Approach-to-Accelerate-and-Cheng-Wu/e2adc565c677fbdd1fa546649a9fd9faa5d4b82e"]},
{"id": "Discrimination-aware-Network-Pruning-for-Deep-Model-Liu-Zhuang/846cc70e137dd6e090f58d1fedac8677a6ec609b", "title": "Discrimination-aware Network Pruning for Deep Model Compression", "authors": ["Jing Liu", "Bohan Zhuang", "Zhuangwei Zhuang", "Yong Guo", "Junzhou Huang", "Jinhui Zhu", "Mingkui Tan"], "date": "2020", "abstract": "We study network pruning which aims to remove redundant channels/kernels and hence speed up the inference of deep networks. Existing pruning methods either train from scratch with sparsity constraints or minimize the reconstruction error between the feature maps of the pre-trained models and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, while the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. In this paper, we propose a simple-yet-effective method called discrimination-aware channel pruning (DCP) to choose the channels that actually contribute to the discriminative power. Note that a channel often consists of a set of kernels. Besides the redundancy in channels, some kernels in a channel may also be redundant and fail to contribute to the discriminative power of the network, resulting in kernel level redundancy. To solve this, we propose a discrimination-aware kernel pruning (DKP) method to further compress deep networks by removing redundant kernels. To prevent DCP/DKP from selecting redundant channels/kernels, we propose a new adaptive stopping condition, which helps to automatically determine the number of selected channels/kernels and often results in more compact models with better performance. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of our methods. For example, on ILSVRC-12, the resultant ResNet-50 model with 30% reduction of channels even outperforms the baseline model by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and MobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device, respectively, with negligible performance degradation. The source code and the pre-trained models are available at this https URL.", "references": ["https://www.semanticscholar.org/paper/Multi-way-backpropagation-for-training-compact-deep-Guo-Chen/07f34571005bde02dd31886cf58279591a1ae914", "https://www.semanticscholar.org/paper/Closed-loop-Matters%3A-Dual-Regression-Networks-for-Guo-Chen/fc99c6cc77f12cf252a9dcaa4dd2f2987e029375", "https://www.semanticscholar.org/paper/Hierarchical-Neural-Architecture-Search-for-Single-Guo-Luo/52cd20bc5d50fc84e3b204ed1cea911f3b2aebb0", "https://www.semanticscholar.org/paper/Online-job-scheduling-for-distributed-machine-in-Liu-Yu/9a49af16ad9750ee707a3e1c356b7644d59626b8", "https://www.semanticscholar.org/paper/Discrimination-aware-Channel-Pruning-for-Deep-Zhuang-Tan/a8fe949f73ad7c0ca5cdabed1a0493be72c4a598", "https://www.semanticscholar.org/paper/Rethinking-the-Smaller-Norm-Less-Informative-in-of-Ye-Lu/60464c4bd94a14b63898e322f9ea651830e54ae0", "https://www.semanticscholar.org/paper/Improving-Deep-Neural-Network-Sparsity-through-Zhu-Zhou/4220214c847eadc2f559dc0f77f00f1b527ed924", "https://www.semanticscholar.org/paper/Structured-Pruning-of-Deep-Convolutional-Neural-Anwar-Hwang/7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "https://www.semanticscholar.org/paper/Gate-Decorator%3A-Global-Filter-Pruning-Method-for-You-Yan/bb3f24186972fbc6d8dcd3327dabe7da1e0e4ce8", "https://www.semanticscholar.org/paper/Toward-Compact-ConvNets-via-Structure-Sparsity-Lin-Ji/689aabd92ed086f6485489e386ac3cd897e29f35", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/Filter-Pruning-via-Geometric-Median-for-Deep-Neural-He-Liu/bb5bc0acea8d452a7999c512127b4f7b3acf8a6d", "https://www.semanticscholar.org/paper/Towards-Optimal-Structured-CNN-Pruning-via-Learning-Lin-Ji/dee5d062e70250572e50cda25f08d6a2c02b2bab"]},
{"id": "Sparse-Networks-from-Scratch%3A-Faster-Training-Dettmers-Zettlemoyer/60ed82ca3ec8fbfef4d52e98e49ab687ce501a0c", "title": "Sparse Networks from Scratch: Faster Training without Losing Performance", "authors": ["Tim Dettmers", "Luke Zettlemoyer"], "date": "2019", "abstract": "We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving performance levels competitive with dense networks. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms. Furthermore, we show that our algorithm can reliably find the equivalent of winning lottery tickets from random initialization: Our algorithm finds sparse configurations with 20% or fewer weights which perform as well, or better than their dense counterparts. Sparse momentum also decreases the training time: It requires a single training run -- no re-training is required -- and increases training speed up to 11.85x. In our analysis, we show that our sparse networks might be able to reach dense performance levels by learning more general features which are useful to a broader range of classes than dense networks.", "references": ["https://www.semanticscholar.org/paper/Dynamic-Sparse-Training%3A-Find-Efficient-Sparse-From-Liu-Xu/573cf589caa7e131c17fa5c45fa817f3dce6cfa5", "https://www.semanticscholar.org/paper/Rigging-the-Lottery%3A-Making-All-Tickets-Winners-Evci-Gale/f74c8b22131bf63c8f7708c986370ee6f512bf61", "https://www.semanticscholar.org/paper/Picking-Winning-Tickets-Before-Training-by-Gradient-Wang-Zhang/480d1a4e2ac541edebabf03aca6c2ee0d5f559ab", "https://www.semanticscholar.org/paper/Dynamic-Model-Pruning-with-Feedback-Lin-Stich/c71da533053d79ad267b1d74814a43dda7c584fb", "https://www.semanticscholar.org/paper/PICKING-WINNING-TICKETS-BEFORE-TRAINING/e08557d60dc47af675b48688a3524a7b0a6eac84", "https://www.semanticscholar.org/paper/IN-NEURAL-NETWORK-PRUNING-REWINDING-Renda/850464c9006261bd632c4203f3e630db09a32faf", "https://www.semanticscholar.org/paper/Comparing-Rewinding-and-Fine-tuning-in-Neural-Renda-Frankle/2c6f83b417e127130703b5b81e42899e0ae3693e", "https://www.semanticscholar.org/paper/SENTATION-FOR-ALL-STRUCTURED-LINEAR-MAPS-Dao-Sohoni/11d1b258c96ecbdc4bbc081404de2e4cee6f7189", "https://www.semanticscholar.org/paper/Kaleidoscope%3A-An-Efficient%2C-Learnable-For-All-Maps-Dao-Sohoni/a68c3412e60560290400d2707596f82a914b7c00", "https://www.semanticscholar.org/paper/Discovering-Neural-Wirings-Wortsman-Farhadi/9c48f787f9590fcbad78707419ddfad269102cd3", "https://www.semanticscholar.org/paper/Parameter-Efficient-Training-of-Deep-Convolutional-Mostafa-Wang/c703e42ac401ad734f440d56f6e19e6b2af86a60", "https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "https://www.semanticscholar.org/paper/Wide-Residual-Networks-Zagoruyko-Komodakis/1c4e9156ca07705531e45960b7a919dc473abb51", "https://www.semanticscholar.org/paper/Scalable-training-of-artificial-neural-networks-by-Mocanu-Mocanu/6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "https://www.semanticscholar.org/paper/The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen/26384278cf5d575fc32cb92c303fb648fa0d5217", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Exploring-Sparsity-in-Recurrent-Neural-Networks-Narang-Diamos/0a5265d5f4a2b59bde18c258ad5acd26bc680769", "https://www.semanticscholar.org/paper/SNIP%3A-Single-shot-Network-Pruning-based-on-Lee-Ajanthan/cf440ccce4a7a8681e238b4f26d5b95109add55d", "https://www.semanticscholar.org/paper/Deep-Rewiring%3A-Training-very-sparse-deep-networks-Bellec-Kappel/ccee800244908d2960830967e70ead7dd8266f7a", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d"]},
{"id": "Syntax-Enhanced-Neural-Machine-Translation-with-Zhang-Li/9e9d9bea25b030e760e2057b70d95e846587ece3", "title": "Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations", "authors": ["Meishan Zhang", "Zhenghua Li", "Guohong Fu", "Min Zhang"], "date": "2019", "abstract": "Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods.", "references": ["https://www.semanticscholar.org/paper/Improving-Neural-Machine-Translation-with-Bugliarello-Okazaki/b26bfc6fa8aff11181cf1231e36752e8c33e080c", "https://www.semanticscholar.org/paper/Incorporating-Source-Syntax-into-Transformer-Based-Currey-Heafield/0f4a3b7f835737c7e004073f4a873e356c0063b4", "https://www.semanticscholar.org/paper/Recurrent-Graph-Syntax-Encoder-for-Neural-Machine-Ding-Tao/ddbe99d66a112dd58de835462a60ebfaef401ad8", "https://www.semanticscholar.org/paper/Multi-Granularity-Self-Attention-for-Neural-Machine-Hao/137f8e5c9550ceb9ef35aea0f49ef5d2295b6b24", "https://www.semanticscholar.org/paper/Multi-Granularity-Self-Attention-for-Neural-Machine-Hao/066f99a510f31aabb482d75d331a63c77611291d", "https://www.semanticscholar.org/paper/Multi-Granularity-Self-Attention-for-Neural-Machine-Hao-Wang/91066eed5b158e58b004038fcb6cf1186b20791b", "https://www.semanticscholar.org/paper/A-Syntax-aware-Multi-task-Learning-Framework-for-Xia-Li/57636735d753afd356d5252c61f3f37c505cd4d2", "https://www.semanticscholar.org/paper/Is-POS-Tagging-Necessary-or-Even-Helpful-for-Neural-Zhang-Li/3bb577d87ae8e0d45a223f65db24ab479fbda174", "https://www.semanticscholar.org/paper/Structurally-Comparative-Hinge-Loss-for-Neural-Text-Wang-Zhou/d11ef04e3471598d94890aaad1a4bac4fd3c90a0", "https://www.semanticscholar.org/paper/Spatial-Relation-Extraction-from-Radiology-Reports-Datta-Roberts/6e3dbfd2dc06e2bf94aab425fda1f4e8a7db67ac", "https://www.semanticscholar.org/paper/Improved-Neural-Machine-Translation-with-Source-Wu-Zhou/9f291ce2d0fc1d76206139a40a859283674d8f65", "https://www.semanticscholar.org/paper/Modeling-Source-Syntax-for-Neural-Machine-Li-Xiong/ebb222fff7b71b82d1a5971e198982858abcd03d", "https://www.semanticscholar.org/paper/Graph-Convolutional-Encoders-for-Syntax-aware-Bastings-Titov/2784000e1a3554374662f4d18cb5ad52f59c8de6", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Source-Side-Latent-Hashimoto-Tsuruoka/eec15bc21ddc668e75115972e6716dff0f1944df", "https://www.semanticscholar.org/paper/Sequence-to-Dependency-Neural-Machine-Translation-Wu-Zhang/c79d8c3768b8dbde4e9fbbd8924805d4a02a1158", "https://www.semanticscholar.org/paper/Improved-Neural-Machine-Translation-with-a-Encoder-Chen-Huang/3e597e492c1ed6e7bbd539d5f2e5a6586c6074cd", "https://www.semanticscholar.org/paper/Tree-to-Sequence-Attentional-Neural-Machine-Eriguchi-Hashimoto/db02cd07726371790a825208cec377ec15f5b5f1", "https://www.semanticscholar.org/paper/Linguistic-Input-Features-Improve-Neural-Machine-Sennrich-Haddow/46d0aa6b357c5427f46c7f8ff7053617c4309649", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},
{"id": "Teacher-Supervises-Students-How-to-Learn-From-for-Dong-Yang/6741eb1ce7837eed6da6402b456b55cf9e626d63", "title": "Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection", "authors": ["Xuanyi Dong", "Yezhou Yang"], "date": "2019", "abstract": "Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data and become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.", "references": ["https://www.semanticscholar.org/paper/Pixel-In-Pixel-Net%3A-Towards-Efficient-Facial-in-the-Jin-Liao/6344fbc54002b42a2cc44f08c669ce26b518528c", "https://www.semanticscholar.org/paper/Semi-Supervised-Learning-for-Fine-Grained-With-Nartey-Yang/450149acf23636e5b006caec9e6a3cdb0facf802", "https://www.semanticscholar.org/paper/Facial-Landmark-Correlation-Analysis-Yan-Duffner/83092fbcc9bac3a88c0cd686e6616fd49c6494c4", "https://www.semanticscholar.org/paper/LUVLi-Face-Alignment%3A-Estimating-Landmarks'-and-Kumar-Marks/3fe97c4bec362afb198b9ec6262cdd6f36739f23", "https://www.semanticscholar.org/paper/Robust-Semi-Supervised-Traffic-Sign-Recognition-via-Nartey-Yang/90f82d422b4a0c2c3aa937480996aa4ed7ffc114", "https://www.semanticscholar.org/paper/Attentive-One-Dimensional-Heatmap-Regression-for-Yin-Wang/d7575268d268bc3b51109d47b24f06d6fe786efd", "https://www.semanticscholar.org/paper/Knowledge-Distillation-and-Student-Teacher-Learning-Wang-Yoon/aab0e1768bc230aca7354922b4827b5ca92fafe6", "https://www.semanticscholar.org/paper/3FabRec%3A-Fast-Few-shot-Face-alignment-by-Browatzki-Wallraven/d798d4f8d751229cb0c1ca0a751b326a2f7a6b00", "https://www.semanticscholar.org/paper/Knowledge-Distillation%3A-A-Survey-Gou-Yu/b9d291a4702faeaa0ee57cd86ee619a552c96e51", "https://www.semanticscholar.org/paper/Improving-Landmark-Localization-with-Learning-Honari-Molchanov/dcd2ac544a8336d73e4d3d80b158477c783e1e50", "https://www.semanticscholar.org/paper/Supervision-by-Registration%3A-An-Unsupervised-to-the-Dong-Yu/3534635d265932ed6c3e991cade973774e5e51ea", "https://www.semanticscholar.org/paper/Facial-Landmark-Detection-with-Tweaked-Neural-Wu-Hassner/620689ad653dfb9f50c9d9e7dbfe884a44506e25", "https://www.semanticscholar.org/paper/Teacher-and-Student-Joint-Learning-for-Compact-Lee-Baddar/c9d15b58f1a46c6d7a535f7ea1e987913aab1016", "https://www.semanticscholar.org/paper/Style-Aggregated-Network-for-Facial-Landmark-Dong-Yan/77875d6e4d8c7ed3baeb259fd5696e921f59d7ad", "https://www.semanticscholar.org/paper/A-Deep-Regression-Architecture-with-Two-Stage-for-Lv-Shao/0e7a792ef33af26c26970ffc275d0ae82ee8f5d1", "https://www.semanticscholar.org/paper/Annotated-Facial-Landmarks-in-the-Wild%3A-A-database-K%C3%B6stinger-Wohlhart/b4d2151e29fb12dbe5d164b430273de65103d39b", "https://www.semanticscholar.org/paper/HyperFace%3A-A-Deep-Multi-Task-Learning-Framework-for-Ranjan-Patel/04eed24e26d9e6aaf2ca434cad20facd5feb83d0", "https://www.semanticscholar.org/paper/The-First-Facial-Landmark-Tracking-in-the-Wild-and-Shen-Zafeiriou/07d95be4922670ef2f8b11997e0c00eb643f3fca", "https://www.semanticscholar.org/paper/300-Faces-in-the-Wild-Challenge%3A-The-First-Facial-Sagonas-Tzimiropoulos/08d4b0960e90b390db7e51b7e6ba4bc40680dfd4"]},
{"id": "Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "title": "Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input", "authors": ["Junliang Guo", "Xu Tan", "Di He", "Tao Qin", "Linli Xu", "Tie-Yan Liu"], "date": "2019", "abstract": "Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the inputs of the decoder, achieve significantly inference speedup but at the cost of inferior accuracy compared to autoregressive translation (AT) models. Previous work shows that the quality of the inputs of the decoder is important and largely impacts the model accuracy. In this paper, we propose two methods to enhance the decoder inputs so as to improve NAT models. The first one directly leverages a phrase table generated by conventional SMT approaches to translate source tokens to target tokens, which are then fed into the decoder as inputs. The second one transforms source-side word embeddings to target-side word embeddings through sentence-level alignment and word-level adversary learning, and then feeds the transformed word embeddings into the decoder as inputs. Experimental results show our method largely outperforms the NAT baseline~\\citep{gu2017non} by $5.11$ BLEU scores on WMT14 English-German task and $4.72$ BLEU scores on WMT16 English-Romanian task.", "references": ["https://www.semanticscholar.org/paper/Minimizing-the-Bag-of-Ngrams-Difference-for-Neural-Shao-Zhang/775f6da764a134e7bd0e361c27f7d7411afef4d3", "https://www.semanticscholar.org/paper/Fine-Tuning-by-Curriculum-Learning-for-Neural-Guo-Tan/1bebcbfb3b8309c9cea44de52662a911f99bc65b", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Imitation-Learning-for-Non-Autoregressive-Neural-Wei-Wang/df9e4fbe7f7a8d3a1d4804b2065308a24b8490ae", "https://www.semanticscholar.org/paper/Fast-Structured-Decoding-for-Sequence-Models-Sun-Li/182e0fe967972c19d99b83a5e8c317014fb3ae28", "https://www.semanticscholar.org/paper/Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "https://www.semanticscholar.org/paper/Syntactically-Supervised-Transformers-for-Faster-Akoury-Krishna/d6e21619df572d04b2b2d97b4c5d1fd604f185fb", "https://www.semanticscholar.org/paper/Analyzing-Word-Translation-of-Transformer-Layers-Xu-Genabith/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9", "https://www.semanticscholar.org/paper/Multilingual-Neural-Machine-Translation-with-Tan-Ren/1b24b7b4ac2427d20ab60c8451563eb8d99caf9c", "https://www.semanticscholar.org/paper/Efficient-Bidirectional-Neural-Machine-Translation-Tan-Xia/7cb1715b4e20c304ddf64964425c0bbf964f73b4", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff", "https://www.semanticscholar.org/paper/Sequence-Level-Knowledge-Distillation-Kim-Rush/57a10537978600fd33dcdd48922c791609a4851a", "https://www.semanticscholar.org/paper/Analyzing-Uncertainty-in-Neural-Machine-Translation-Ott-Auli/b887a39268ee41fea8d72f0ecd364eb72fe28a82", "https://www.semanticscholar.org/paper/Beyond-Error-Propagation-in-Neural-Machine-of-Also-Wu-Tan/c6003f7a78bcc13e4c0b18653b09cf34583dfaa8", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-in-Linear-Time-Kalchbrenner-Espeholt/98445f4172659ec5e891e031d8202c102135c644", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d"]},
{"id": "Modeling-Recurrence-for-Transformer-Hao-Wang/68a47a65c1ffd5d1540f12adde1a7300594c9969", "title": "Modeling Recurrence for Transformer", "authors": ["Jie Hao", "Xing Wang", "Baosong Yang", "Longyue Wang", "Jinfeng Zhang", "Zhaopeng Tu"], "date": "2019", "abstract": "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention and recurrent networks. Experimental results on the widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.", "references": ["https://www.semanticscholar.org/paper/Capsule-Transformer-for-Neural-Machine-Translation-Duan-Cao/876be9b226601821eeade310013506a03f023824", "https://www.semanticscholar.org/paper/Recurrent-Positional-Embedding-for-Neural-Machine-Chen-Wang/114745b95c7029c5163b745a37829d1e85fc3083", "https://www.semanticscholar.org/paper/Convolutional-Self-Attention-Networks-Yang-Wang/bdc046e65bc80cf13929ca0c3934d6faee830723", "https://www.semanticscholar.org/paper/Explicit-Reordering-for-Neural-Machine-Translation-Chen-Wang/951747a8d4f516cb011b4f8d19c764f14f155723", "https://www.semanticscholar.org/paper/Towards-Better-Modeling-Hierarchical-Structure-for-Hao-Wang/a940e88117d19160f276e9b6ae014df699495a3f", "https://www.semanticscholar.org/paper/Theoretical-Limitations-of-Self-Attention-in-Neural-Hahn/b3564be8b79f25585acb035f3deaf4ae93c26d8f", "https://www.semanticscholar.org/paper/Leveraging-Local-and-Global-Patterns-for-Networks-Xu-Wong/2353f65b42e9f445425568088c5adef300a7f573", "https://www.semanticscholar.org/paper/Towards-Better-Modeling-Hierarchical-Structure-for-Hao/64e2a920b8b070fc02b06a5c3f870f0a915b82ab", "https://www.semanticscholar.org/paper/Exploiting-Sentential-Context-for-Neural-Machine-Wang-Tu/512894bf03ec8c41bc73205cec9e648147837432", "https://www.semanticscholar.org/paper/A-Mixture-of-h-1-Heads-is-Better-than-h-Heads-Peng-Schwartz/d97cd476bef990352ae921e4c7c5ae1222e9b8da", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/The-Importance-of-Being-Recurrent-for-Modeling-Tran-Bisazza/997c55547aeca733dfc5dfebd12412612ecba022", "https://www.semanticscholar.org/paper/Exploiting-Deep-Representations-for-Neural-Machine-Dou-Tu/7e960b6479d1071b507d0014776adecb3480748d", "https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e", "https://www.semanticscholar.org/paper/Recurrent-Memory-Networks-for-Language-Modeling-Tran-Bisazza/889e57259a1d6017701fb2c2ceece82f9f4eff4c", "https://www.semanticscholar.org/paper/Multi-layer-Representation-Fusion-for-Neural-Wang-Li/66d21f5f029fcf2346196637792eaeed36b61e81", "https://www.semanticscholar.org/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f", "https://www.semanticscholar.org/paper/DiSAN%3A-Directional-Self-Attention-Network-for-Shen-Zhou/adc276e6eae7051a027a4c269fb21dae43cadfed", "https://www.semanticscholar.org/paper/Context-Aware-Self-Attention-Networks-Yang-Li/5d1a6d27d67e0b8d4a049f7f5dc3995f837d7976", "https://www.semanticscholar.org/paper/The-Best-of-Both-Worlds%3A-Combining-Recent-Advances-Chen-Firat/bb669de2fce407df2f5cb2f8c51dedee3f467e04"]},
{"id": "use-pruning-together-with-layer-wise-quantization-Kautz/ad3e7d2b9633a646ce3f33010c522079ae385f5a", "title": "use pruning together with layer-wise quantization and additional Huffman coding", "authors": ["Jan Kautz"], "date": "2019", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale computational models on resource-constrained devices. Contrary to analogous domains where large-scale systems are build as a hierarchical repetition of smallscale units, the current practice in Machine Learning largely relies on models with non-repetitive components. In the spirit of molecular composition with repeating atoms, we advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons. In other words, the same neurons with the same weights are stochastically re-positioned in subsequent layers of the network. Empirical evidence suggests that ACNs achieve compression rates of up to three orders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7 to 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy (0.15% to 5.33%). Moreover our method can yield sub-linear model complexities and permits learning deep ACNs with less parameters than a logistic regression with no decline in classification accuracy.", "references": ["https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/Model-compression-via-distillation-and-quantization-Polino-Pascanu/f6a4bf043af1a9ec7f104a7b7ab56806b241ceda", "https://www.semanticscholar.org/paper/Deep-Neural-Network-Compression-by-In-Parallel-Tung-Mori/610d0b290f0f1c22b03f220d6ba332627a5f6f3e", "https://www.semanticscholar.org/paper/Soft-Weight-Sharing-for-Neural-Network-Compression-Ullrich-Meeds/6c018075720da00d81938bec1c1d2cde42f19d7d", "https://www.semanticscholar.org/paper/Tensorizing-Neural-Networks-Novikov-Podoprikhin/e6f2f3a5cc7c7213835b9aede15715b5830520e1", "https://www.semanticscholar.org/paper/Importance-Estimation-for-Neural-Network-Pruning-Molchanov-Mallya/a6f4917d043494d2ebaebe6b65cb35e6a07fda41", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/A-Survey-of-Model-Compression-and-Acceleration-for-Cheng-Wang/8dd85e38445a5ddb5dd71cabc3c4246de30c014f", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/EfficientNet%3A-Rethinking-Model-Scaling-for-Neural-Tan-Le/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9"]},
{"id": "Sequential-Recommendation-with-Relation-Aware-Ji-Joo/3c9fe50e8237ffb083c2cd36551ecf5f4bcf124d", "title": "Sequential Recommendation with Relation-Aware Kernelized Self-Attention", "authors": ["Mingi Ji", "Weonyoung Joo", "Kyungwoo Song", "Yoon-Yeong Kim", "Il-Chul Moon"], "date": "2020", "abstract": "Recent studies identified that sequential Recommendation is improved by the attention mechanism. By following this development, we propose Relation-Aware Kernelized Self-Attention (RKSA) adopting a self-attention mechanism of the Transformer with augmentation of a probabilistic model. The original self-attention of Transformer is a deterministic measure without relation-awareness. Therefore, we introduce a latent space to the self-attention, and the latent space models the recommendation context from relation as a multivariate skew-normal distribution with a kernelized covariance matrix from co-occurrences, item characteristics, and user information. This work merges the self-attention of the Transformer and the sequential recommendation by adding a probabilistic model of the recommendation task specifics. We experimented RKSA over the benchmark datasets, and RKSA shows significant improvements compared to the recent baseline models. Also, RKSA were able to produce a latent space model that answers the reasons for recommendation.", "references": ["https://www.semanticscholar.org/paper/CSAN%3A-Contextual-Self-Attention-Network-for-User-Huang-Qian/e763c7c0d718e5796cd9142843fb81389a2b6268", "https://www.semanticscholar.org/paper/Next-Item-Recommendation-with-Self-Attentive-Metric-Zhang-Tay/814aad45aa227433256c8d3bbf62b55521ee36a0", "https://www.semanticscholar.org/paper/Context-Aware-Self-Attention-Networks-Yang-Li/5d1a6d27d67e0b8d4a049f7f5dc3995f837d7976", "https://www.semanticscholar.org/paper/Multi-Order-Attentive-Ranking-Model-for-Sequential-Yu-Zhang/0763d8f5aefd4a41d264a5392b4f4076761e2d2f", "https://www.semanticscholar.org/paper/ATRank%3A-An-Attention-Based-User-Behavior-Modeling-Zhou-Bai/063598cdaff79852c3647d074506120889c5c17b", "https://www.semanticscholar.org/paper/Sequential-Recommender-System-based-on-Hierarchical-Ying-Zhuang/0b42168f22713c48d8b0ce3bc8f680d347184d88", "https://www.semanticscholar.org/paper/Hierarchical-Context-enabled-Recurrent-Neural-for-Song-Ji/b8fbc855d491f1e458bd4c39f1eeee9a0127cf6c", "https://www.semanticscholar.org/paper/Self-Attentive-Sequential-Recommendation-Kang-McAuley/97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "https://www.semanticscholar.org/paper/Neural-Attentive-Session-based-Recommendation-Li-Ren/97de32d8ca162944e5f7e83071c596d13d168109", "https://www.semanticscholar.org/paper/Session-based-Recommendations-with-Recurrent-Neural-Hidasi-Karatzoglou/dfb1e7d1559fbc2eb63761bc170061d256496bdf"]},
{"id": "Neural-Machine-Translation-with-Dynamic-Selection-Han-Mu/e080487556381239ae57f77e7fd82c7a48e974a2", "title": "Neural Machine Translation with Dynamic Selection Network", "authors": ["Fei Han", "Jiang Mu", "Long Tian", "Mengting Luo", "Zirui Qiu", "Dejun Zhang"], "date": "2018", "abstract": "Neural Machine Translation (NMT) has made remarkable progress in recent years, and the attention mechanism has become the dominant approach with the state-of-the-art records in many language pairs. However, the attention based NMT models have two shortcomings: First, due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. Second, with the extensive application of NMT model in empirical research, its long-term weaknesses in dealing with scarce and extra vocabulary have become increasingly prominent. To address this problem, we employ our dynamic selection network which consists of context gate that dynamically controls the amount of information flowing from the source and target contexts, and dynamic vocabulary that additionally considers copying words directly from the source. Experiments are conducted on three machine translation tasks, English-to-German IWLST 2014, English-to-Vietnamese IWLST 2015 and Turkish-to-English WMT 2017. Experiments show that the proposed model outperforms the traditional NMT model with a large margin.", "references": ["https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Reconstruction-Tu-Liu/31fc1b0fd5ec43863f1a502f6fc3df2cc71b6e6f", "https://www.semanticscholar.org/paper/Context-Gates-for-Neural-Machine-Translation-Tu-Liu/8dde8967c8bf1c97a5614c70beb0eeeaf32d2e7c", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/Massive-Exploration-of-Neural-Machine-Translation-Britz-Goldie/4550a4c714920ef57d19878e31c9ebae37b049b2", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-External-Phrase-Tang-Meng/dac49d809bd9c64fa77e8f2efd3e20406cde3719", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/The-IWSLT-2015-Evaluation-Campaign-Cettolo-Niehues/b54268e3b8d148c0695ca52bebb0f80e26a4b987"]},
{"id": "Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "title": "Non-Autoregressive Machine Translation with Auxiliary Regularization", "authors": ["Yiren Wang", "Fei Tian", "Di He", "Tao Qin", "ChengXiang Zhai", "Tie-Yan Liu"], "date": "2019", "abstract": "As a new neural machine translation approach, Non-Autoregressive machine Translation (NAT) has attracted attention recently due to its high efficiency in inference. However, the high efficiency has come at the cost of not capturing the sequential dependency on the target side of translation, which causes NAT to suffer from two kinds of translation errors: 1) repeated translations (due to indistinguishable adjacent decoder hidden states), and 2) incomplete translations (due to incomplete transfer of source side information via the decoder hidden states). \nIn this paper, we propose to address these two problems by improving the quality of decoder hidden representations via two auxiliary regularization terms in the training process of an NAT model. First, to make the hidden states more distinguishable, we regularize the similarity between consecutive hidden states based on the corresponding target tokens. Second, to force the hidden states to contain all the information in the source sentence, we leverage the dual nature of translation tasks (e.g., English to German and German to English) and minimize a backward reconstruction error to ensure that the hidden states of the NAT decoder are able to recover the source side sentence. Extensive experiments conducted on several benchmark datasets show that both regularization strategies are effective and can alleviate the issues of repeated translations and incomplete translations in NAT models. The accuracy of NAT models is therefore improved significantly over the state-of-the-art NAT models with even better efficiency for inference.", "references": ["https://www.semanticscholar.org/paper/Minimizing-the-Bag-of-Ngrams-Difference-for-Neural-Shao-Zhang/775f6da764a134e7bd0e361c27f7d7411afef4d3", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Imitation-Learning-for-Non-Autoregressive-Neural-Wei-Wang/df9e4fbe7f7a8d3a1d4804b2065308a24b8490ae", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Latent-Saharia-Chan/f6127bbe33d7e5776d3c313304b35d27e1051459", "https://www.semanticscholar.org/paper/Latent-Variable-Non-Autoregressive-Neural-Machine-a-Shu-Lee/d6cfb4e345b1031040ccd3683730854c560a2b0d", "https://www.semanticscholar.org/paper/Fine-Tuning-by-Curriculum-Learning-for-Neural-Guo-Tan/1bebcbfb3b8309c9cea44de52662a911f99bc65b", "https://www.semanticscholar.org/paper/Fast-Structured-Decoding-for-Sequence-Models-Sun-Li/182e0fe967972c19d99b83a5e8c317014fb3ae28", "https://www.semanticscholar.org/paper/LAVA-NAT%3A-A-Non-Autoregressive-Translation-Model-Li-Meng/2c3859564617e910651b56373e6c0941656f32ff", "https://www.semanticscholar.org/paper/NON-AUTOREGRESSIVE-MACHINE-TRANSLATION-Zhou-Gu/dec6bb3c7bb671c86296a2a089e0e38aa3f69279", "https://www.semanticscholar.org/paper/Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Reconstruction-Tu-Liu/31fc1b0fd5ec43863f1a502f6fc3df2cc71b6e6f", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Dual-Learning-for-Machine-Translation-He-Xia/23b559b5ab27f2fca6f56c0a7b6478bcf69db509", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff", "https://www.semanticscholar.org/paper/Fast-Decoding-in-Sequence-Models-using-Discrete-Kaiser-Roy/2d08ed53491053d84b6de89aedbf2178b9c8cf84", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Modeling-Coverage-for-Neural-Machine-Translation-Tu-Lu/33108287fbc8d94160787d7b2c7ef249d3ad6437", "https://www.semanticscholar.org/paper/Achieving-Human-Parity-on-Automatic-Chinese-to-News-Hassan-Aue/d7dded37a976ebc05103a4f3785969005c37af5b"]},
{"id": "Nonconvex-sparse-regularization-for-deep-neural-and-Ohn-Kim/6f3e188f2d9cb05930a111d79052c0fe11042abb", "title": "Nonconvex sparse regularization for deep neural networks and its optimality", "authors": ["Ilsang Ohn", "Yongdai Kim"], "date": "2020", "abstract": "Recent theoretical studies proved that deep neural network (DNN) estimators obtained by minimizing empirical risk with a certain sparsity constraint can attain optimal convergence rates for regression and classification problems. However, the sparsity constraint requires to know certain properties of the true model, which are not available in practice. Moreover, computation is difficult due to the discrete nature of the sparsity constraint. In this paper, we propose a novel penalized estimation method for sparse DNNs, which resolves the aforementioned problems existing in the sparsity constraint. We establish an oracle inequality for the excess risk of the proposed sparse-penalized DNN estimator and derive convergence rates for several learning tasks. In particular, we prove that the sparse-penalized estimator can adaptively attain minimax convergence rates for various nonparametric regression problems. For computation, we develop an efficient gradient-based optimization algorithm that guarantees the monotonic reduction of the objective function.", "references": ["https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096", "https://www.semanticscholar.org/paper/Analysis-of-Multi-stage-Convex-Relaxation-for-Zhang/f518cc59864aa238cbe7dfb4539437420076ec73", "https://www.semanticscholar.org/paper/Deep-Neural-Networks-Learn-Non-Smooth-Functions-Imaizumi-Fukumizu/65305349aacab45ea652d3b6af0700c99aa2cc1f", "https://www.semanticscholar.org/paper/Nonparametric-regression-using-deep-neural-networks-Schmidt-Hieber/396bc378026271f5e5f87794004fac49996791c8", "https://www.semanticscholar.org/paper/On-deep-learning-as-a-remedy-for-the-curse-of-in-Bauer-Kohler/57fe842291f2bfc9601b1ab0ce9318168514d082", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "https://www.semanticscholar.org/paper/Fast-convergence-rates-of-deep-neural-networks-for-Kim-Ohn/4ebe1ab82123e34c572b58074d64f69711ac7deb", "https://www.semanticscholar.org/paper/Sparse-Convolutional-Neural-Networks-Liu-Wang/d559dd84fc473fca7e91b9075675750823935afa", "https://www.semanticscholar.org/paper/Optimal-approximation-of-piecewise-smooth-functions-Petersen-Voigtl%C3%A4nder/8d4f727b181e14632b7dd15435e4f756e2926ad9", "https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02"]},
{"id": "Classification-by-Pairwise-Coupling-Hastie-Tibshirani/f642a692da944604a7df590e9f9fa06089b7991a", "title": "Classification by Pairwise Coupling", "authors": ["Trevor J. Hastie", "Robert Tibshirani"], "date": "1997", "abstract": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described.", "references": ["https://www.semanticscholar.org/paper/Probability-Estimates-for-Multi-class-by-Pairwise-Wu-Lin/0d02083f5e9206ff88ed1e284da8665d4bee81ba", "https://www.semanticscholar.org/paper/Reducing-multiclass-to-binary-by-coupling-estimates-Zadrozny/af068a5fe9905674e450f144a1e01d0e8689fb42", "https://www.semanticscholar.org/paper/Combining-Pairwise-Classifiers-with-Stacking-Savick%C3%BD-F%C3%BCrnkranz/07767f3dd46cc1f546e32bfe261345a658569585", "https://www.semanticscholar.org/paper/Efficient-Pairwise-Classification-Park-F%C3%BCrnkranz/d3796b40df755ed87f5e915508a5b4259cf2cd9d", "https://www.semanticscholar.org/paper/On-the-combination-of-locally-optimal-pairwise-Szepannek-Bischl/99b01e8da1c18ffa1fd997c9b49043901d86ce86", "https://www.semanticscholar.org/paper/How-to-Do-Multi-way-Classification-with-Two-Way-Cutzu/647559d2234aca1430dcd3036f39beb787e1e798", "https://www.semanticscholar.org/paper/New-variants-of-pairwise-classification-Krzysko-Wolynski/e478f2302cfd83f0110b64b708a358b44a5e9b9a", "https://www.semanticscholar.org/paper/Using-two-class-classifiers-for-multiclass-Tax-Duin/32b9384ec5653a9910a7eb4c55d064ee13769a5e", "https://www.semanticscholar.org/paper/On-Locally-Linear-Classification-by-Pairwise-Chen-Lu/12338734142ea159d1fa0246843199695975ef88", "https://www.semanticscholar.org/paper/Efficient-Pairwise-Classification-and-Ranking-Park-F%C3%BCrnkranz/a19511548159c87c89881bb8c5b3a9de6de0650e", "https://www.semanticscholar.org/paper/On-a-Least-Squares-Adjustment-of-a-Sampled-Table-Deming-Stephan/0d3385999a7a2d905027ea9e71702e02e856196f", "https://www.semanticscholar.org/paper/The-rank-analysis-of-incomplete-block-designs.-II.-Bradley/204f2738c97d2f1202e849ba2c61f545e1684fc1"]},
{"id": "Classification-by-pairwise-coupling-of-imprecise-Quost-Destercke/f37c6a03fbfb70bd49df270b29dc5e18e0ab98c8", "title": "Classification by pairwise coupling of imprecise probabilities", "authors": ["Benjamin Quost", "S\u00e9bastien Destercke"], "date": "2018", "abstract": "We address combining imprecise classifiers in the framework of imprecise probabilities.Classifier outputs are probability intervals, which are large for irrelevant classifiers.The combination consists in computing probabilities consistent with the intervals.This approach is particularly interesting when combining few classifiers. In this paper, we are interested in making decisions by combining classifiers providing uncertain outputs, in the form of sets of probability distributions. More precisely, each classifier provides lower and upper bounds on the conditional probabilities of the associated classes. The classifiers are combined by computing the set of unconditional probability distributions compatible with these bounds, by solving linear optimization problems. When the classifier outputs are inconsistent, we propose a correcting step that restores this consistency. The experiments show the interest of our approach for solving multi-class classification problems, particularly when information is scarce (i.e., a limited number of classifiers is available). In this case, modeling the lack of information associated with classifier outputs gives good results even when they are poorly regularized or overfit the data.", "references": ["https://www.semanticscholar.org/paper/Ensembles-of-Nested-Dichotomies-with-Multiple-Leathart-Frank/0dad5309a49154c659212feb9b64c8efda26f819", "https://www.semanticscholar.org/paper/Pairwise-coupling-of-convolutional-neural-networks-Such-Tar%C3%A1bek/56824c3bb9a02ec254e59a7477ad6efaaede8904", "https://www.semanticscholar.org/paper/Evolutionary-wrapper-approaches-for-training-set-as-Verbiest-Derrac/9560d6985eb636c30f600a3bd6d36fa315cb9a6c", "https://www.semanticscholar.org/paper/IntelliMatch-%3A-A-SME-Driven-Trainable-Matching-%E2%99%A3-Zhang-Ramamoorty/abd5bd922eb4b941929df721e20e63f4ae9726da", "https://www.semanticscholar.org/paper/An-Effective-Fault-Diagnosis-Approach-Based-On-and-Peng-Zhang/466ac8548e227d3c128e213872f1fb44ea3bed41", "https://www.semanticscholar.org/paper/Classification-of-Guillain-Barr%C3%A9-Syndrome-Subtypes-Torres-V%C3%A1squez-Ch%C3%A1vez-Bosquez/0b49d95c5a67e4c03550bc014c27e48f916cb29d", "https://www.semanticscholar.org/paper/Fault-Diagnosis-Method-for-Heterogeneous-Fusion-of-Yin-Zhang/41cb3ea8eeceebf9b831eda74a9a3ed7dc7c3d31", "https://www.semanticscholar.org/paper/Improving-Dialogue-Act-Classification-for-Arabic-at-Elmadany-Abdou/c4eb9151b804bae873471dd8ee65271bc1e735a4", "https://www.semanticscholar.org/paper/SLA-based-adaptation-schemes-in-distributed-stream-Hanif-Kim/645d577d75ff7220f451ee1df0de349dda00a700", "https://www.semanticscholar.org/paper/01-Approche-g%C3%A9n%C3%A9rique-de-la-gestion-de-l%E2%80%99incertain-Appriou/22746e76fc6c3381aa7b491171dd641b25e24a22", "https://www.semanticscholar.org/paper/4-Th%C3%A9orie-de-l'%C3%A9vidence-et-cadres-de-discernement-Janez-Appriou/0088b9c4e251451d8e4a69deddc56206eb6d0290", "https://www.semanticscholar.org/paper/A-Systematic-Framework-for-Composite-Hypothesis-of-Ciuonzo-Maio/913f5ff731687dbdc31083af03268b4ed048a6d4", "https://www.semanticscholar.org/paper/A-non-parametric-approach-to-extending-generic-for-Santhanam-Morariu/3479236fa1cf8f9d2437fca546af99ab5773bb5b", "https://www.semanticscholar.org/paper/A-novel-cascade-ensemble-classifier-system-with-a-Zhang-Bui/3bc7191ab73f2b92af8a8ecd895a7428516ca7ba", "https://www.semanticscholar.org/paper/A-solution-for-facial-expression-representation-and-Dubuisson-Davoine/62f3525cca0a08ef4727b60a366104baadc4db26", "https://www.semanticscholar.org/paper/A-two-stage-linear-discriminant-analysis-via-Ye-Li/187e1ac48c318cb84f519f662f0d09a98ea2306e", "https://www.semanticscholar.org/paper/Adaptive-Directed-Acyclic-Graphs-for-Multiclass-Kijsirikul-Ussivakul/7e0bfbdd91c0e0ca713e32e91d03d140f91a1e46", "https://www.semanticscholar.org/paper/An-overview-of-ensemble-methods-for-binary-in-study-Galar-Fern%C3%A1ndez/3fae7a4e843b8bcf21b86bfc848c9b23ece56d99", "https://www.semanticscholar.org/paper/An-overview-of-robust-Bayesian-analysis-Berger-Moreno/4675bd4233ffa2f3d2ba86ed586de88b77b91170"]},
{"id": "Labelling-strategies-for-hierarchical-multi-label-Triguero-Vens/e1ab9b55e8b85f08913bb01af912d076095fce9a", "title": "Labelling strategies for hierarchical multi-label classification techniques", "authors": ["Isaac Triguero", "Celine Vens"], "date": "2016", "abstract": "Many hierarchical multi-label classification systems predict a real valued score for every (instance, class) couple, with a higher score reflecting more confidence that the instance belongs to that class. These classifiers leave the conversion of these scores to an actual label set to the user, who applies a cut-off value to the scores. The predictive performance of these classifiers is usually evaluated using threshold independent measures like precision-recall curves. However, several applications require actual label sets, and thus an automatic labelling strategy.In this paper, we present and evaluate different alternatives to perform the actual labelling in hierarchical multi-label classification. We investigate the selection of both single and multiple thresholds. Despite the existence of multiple threshold selection strategies in non-hierarchical multi-label classification, they cannot be applied directly to the hierarchical context. The proposed strategies are implemented within two main approaches: optimisation of a certain performance measure of interest (such as F-measure or hierarchical loss), and simulating training set properties (such as class distribution or label cardinality) in the predictions. We assess the performance of the proposed labelling schemes on 10 datasets from different application domains. Our results show that selecting multiple thresholds may result in an efficient and effective solution for hierarchical multi-label problems. HighlightsWe select single or multiple thresholds for hierarchical multi-label classifiers.Selecting multiple thresholds often yield better label sets in lesser time.We show that optimising H-loss tends to favor empty label sets.Multiple threshold selection is preferred for micro F-measure and HMC-loss.Imitating training set properties is a competitive approach to optimise HMC-loss.", "references": ["https://www.semanticscholar.org/paper/Inducing-Hierarchical-Multi-label-Classification-Cerri-Basgalupp/38509348279f3d67e624b1856336f95366bcf5e6", "https://www.semanticscholar.org/paper/Hierarchical-Multi-Label-Classification-Networks-Wehrmann-Cerri/8d6ed8b6d49097660408179eb20362c5f55d638c", "https://www.semanticscholar.org/paper/Hyperbolic-Interaction-Model-For-Hierarchical-Chen-Huang/e1efb8a836b40adc82d0b37eb9095b82a2a96019", "https://www.semanticscholar.org/paper/Combining-Instance-and-Feature-Neighbors-for-Feremans-Cule/9398923ef7fd7b9938149753caf745b5068eeabc", "https://www.semanticscholar.org/paper/Threshold-optimization-for-F-measure-of-precision-Berger-Guda/6ec80663d8bf7eee1d583c941ef551b7379a1fca", "https://www.semanticscholar.org/paper/Combining-instance-and-feature-neighbours-for-Feremans-Cule/e2c4dc680da76ef0014746504453dddb5937d60e", "https://www.semanticscholar.org/paper/Multi-label-classification-via-multi-target-on-data-Osojnik-Panov/e29d7acaba4b81b0022dff238f93006dca6d0224", "https://www.semanticscholar.org/paper/Multi-label-Classification-via-Multi-target-on-Data-Osojnik-Panov/05807a506b8bbbff822375fb096834c43028ea71", "https://www.semanticscholar.org/paper/An-Overview-of-Label-Space-Dimension-Reduction-for-Tang-Liu/7d95ca8f385a3e6977c408fd5b73f12e560a022d", "https://www.semanticscholar.org/paper/Incremental-Class-Learning-for-Hierarchical-Park-Kim/3eeac3710f9ad854072eb3bb1635470e22ca3fde", "https://www.semanticscholar.org/paper/Exploiting-Label-Dependency-for-Hierarchical-Alaydie-Reddy/c8cdb32ed6ec5dcb21e3839f9f4647fa2885ad5f", "https://www.semanticscholar.org/paper/Classifier-chains-for-multi-label-classification-Read-Pfahringer/e75a71274522980dce85b8ad00369d0a33667ea4", "https://www.semanticscholar.org/paper/Threshold-optimisation-for-multi-label-classifiers-Pillai-Fumera/06ac7bafac5ed14aca7dd79150b68d2a92fc0ffb", "https://www.semanticscholar.org/paper/The-importance-of-the-label-hierarchy-in-Levatic-Kocev/49e90f36c38174bce5654090cb476557f23c4b45", "https://www.semanticscholar.org/paper/Multi-label-classification-and-extracting-predicted-Brucker-Benites/a5af765f7f053418411c098fa406d7b7f420d2cf", "https://www.semanticscholar.org/paper/Hierarchical-multi-label-classification-using-local-Cerri-Barros/ff7eb443d708674a257ebf84f605daff578a9c99", "https://www.semanticscholar.org/paper/Decision-trees-for-hierarchical-multi-label-Vens-Struyf/0a665d99541ed9a5c509fa87db5581f30bb364f6", "https://www.semanticscholar.org/paper/An-Extensive-Evaluation-of-Decision-Tree-Based-and-Cerri-Pappa/649399575c8b5a38450d768b20b9d1a491e48bce", "https://www.semanticscholar.org/paper/Hierarchical-Multilabel-Classification-with-Minimum-Bi-Kwok/c1cb5746b3bd9d1c80c02a95a64dcb6ba86e30ff", "https://www.semanticscholar.org/paper/Hierarchical-classification%3A-combining-Bayes-with-Cesa-Bianchi-Gentile/ea0eb78042d22cb4c57c6d315e37e811c4e44e8f"]},
{"id": "Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "authors": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "date": "2014", "abstract": "The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5\u00d7 speedup with no loss in accuracy, and 4.5\u00d7 speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.", "references": ["https://www.semanticscholar.org/paper/Stability-Based-Filter-Pruning-for-Accelerating-Singh-Kadi/748b451f200aaf54935c14d2156cc78283a1f99a", "https://www.semanticscholar.org/paper/On-the-Reduction-of-Computational-Complexity-of-Maji-Mullins/0651ea8d9ca9890e8bcb32283c261483f89025ea", "https://www.semanticscholar.org/paper/Accelerating-Convolutional-Neural-Networks-for-Wang-Cheng/0440a984f4b843c8d4618ff02d1707de483f92b8", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/ESPACE%3A-Accelerating-Convolutional-Neural-Networks-Lin-Ji/46f83973aaab8574011a6630a84ce9c623c119ac", "https://www.semanticscholar.org/paper/Compression-of-descriptor-models-for-mobile-Miles-Mikolajczyk/49af49f79fd23b80b3e4eb2c960c79697f3b31c9", "https://www.semanticscholar.org/paper/1D-FALCON%3A-Accelerating-Deep-Convolutional-Neural-Maji-Mullins/7059f8b57e9d68260d351c0e5c55eb2a65ab806e", "https://www.semanticscholar.org/paper/Accelerating-Convolutional-Neural-Networks-via-Map-Georgiadis/b8495b6f77dac227fd23b608f1ad44c338eaa679", "https://www.semanticscholar.org/paper/Convolutional-neural-networks-at-constrained-time-He-Sun/8ad35df17ae4064dd174690efb04d347428f1117", "https://www.semanticscholar.org/paper/Neural-Network-Compression-and-Acceleration%3A-An-Yi/3619dab38b01dad00897eb4addfd9715b72b085c", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Simplifying-ConvNets-for-Fast-Learning-Mamalet-Garcia/4dbc68cf2e14155edb6da0def30661aca8c96c22", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Learning-and-Transferring-Mid-level-Image-Using-Oquab-Bottou/c08f5fa876181fc040d76c75fe2433eee3c9b001", "https://www.semanticscholar.org/paper/Learning-Convolutional-Feature-Hierarchies-for-Kavukcuoglu-Sermanet/8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/Multi-digit-Number-Recognition-from-Street-View-Goodfellow-Bulatov/b3d8dffb73bc93de239998548386c84177caa2ad"]},
{"id": "Spatial-Relation-Extraction-from-Radiology-Reports-Datta-Roberts/6e3dbfd2dc06e2bf94aab425fda1f4e8a7db67ac", "title": "Spatial Relation Extraction from Radiology Reports using Syntax-Aware Word Representations.", "authors": ["Surabhi Datta", "Kirk Roberts"], "date": "2020", "abstract": "In this paper, we investigate the task of spatial role labeling for extracting spatial relations from chest X-ray reports. Previous works have shown the usefulness of incorporating syntactic information in extracting spatial relations. We propose syntax-enhanced word representations in addition to word and character embeddings for extracting radiologyspecific spatial roles. We utilize a bidirectional long short-term memory (Bi-LSTM) conditional random field (CRF) as the baseline model to capture the word sequence and employ additional Bi-LSTMs to encode syntax based on dependency tree substructures. Our focus is on empirically evaluating the contribution of each syntax integration method in extracting the spatial roles with respect to a SPATIAL INDICATOR in a sentence. The incorporation of syntax embeddings to the baseline method achieves promising results, with improvements of 1.3, 0.8, 4.6, and 4.6 points in the average F1 measures for TRAJECTOR, LANDMARK, DIAGNOSIS, and HEDGE roles, respectively.", "references": ["https://www.semanticscholar.org/paper/Spatial-role-labeling-with-convolutional-neural-Mazalov-Martins/90086a2c847c2e2bc50354404adb8f0cd84712ca", "https://www.semanticscholar.org/paper/Modelling-Radiological-Language-with-Bidirectional-Cornegruta-Bakewell/b5602b44fd8d5e32eb2499f273effa9eb2f916fb", "https://www.semanticscholar.org/paper/Understanding-Spatial-Language-in-Radiology%3A-and-Datta-Si/0905d4a8f93f35f0c3b631388d3f5e9b3e993f1f", "https://www.semanticscholar.org/paper/Deep-Embedding-for-Spatial-Role-Labeling-Ludwig-Liu/f8ca1142602ce7b85b24f34c4de7bb2467d2c952", "https://www.semanticscholar.org/paper/Spatial-Role-Labeling%3A-Task-Definition-and-Scheme-Kordjamshidi-Otterlo/0ee8427e27e193c23dbfdc0aed8c5123528e6579", "https://www.semanticscholar.org/paper/Automatic-information-extraction-from-unstructured-Gupta-Banerjee/30daa95712ff74d44d15bd82b8bcf6b34db2b1c7", "https://www.semanticscholar.org/paper/Encoding-Sentences-with-Graph-Convolutional-for-Marcheggiani-Titov/c3a3c163f25b9181f1fb7e71a32482a7393d2088", "https://www.semanticscholar.org/paper/Automatic-Extraction-and-Post-coordination-of-in-Roberts-Rodriguez/2624cbc6f2a8f7a3c6a97dc195ba61f1e8fea89f", "https://www.semanticscholar.org/paper/Structured-learning-for-spatial-information-from-Kordjamshidi-Roth/e04c3d15bdd95fa1137115cba79791a3ba6cab1f", "https://www.semanticscholar.org/paper/Classifying-Relations-via-Long-Short-Term-Memory-Xu-Mou/032fbcb58e2282f02426a0f09c6d5b42787936ec"]},
{"id": "Discovering-Neural-Wirings-Wortsman-Farhadi/9c48f787f9590fcbad78707419ddfad269102cd3", "title": "Discovering Neural Wirings", "authors": ["Mitchell Wortsman", "Ali Farhadi", "Mohammad Rastegari"], "date": "2019", "abstract": "The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training -- as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1we boost the ImageNet accuracy by 10% at ~41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery with a combinatorial number of subnetworks. Code and pretrained models are available at this https URL while additional visualizations may be found at this https URL.", "references": ["https://www.semanticscholar.org/paper/What's-Hidden-in-a-Randomly-Weighted-Neural-Network-Ramanujan-Wortsman/4ceb09cba1edd9ff2c314363e775abbafcb8fa77", "https://www.semanticscholar.org/paper/Neural-Architecture-Generator-Optimization-Ru-Esperan%C3%A7a/757a33f5c440f681c76d02d221281e82da66a27b", "https://www.semanticscholar.org/paper/AutoShrink%3A-A-Topology-aware-NAS-for-Discovering-Zhang-Cheng/45df89d3c48eaa35e3382f0c2224bd85e33b7b91", "https://www.semanticscholar.org/paper/Ordering-Chaos%3A-Memory-Aware-Scheduling-of-Wired-Ahn-Lee/fd179b1e7fc4040ec73aa23ddfc9a61a43c3d600", "https://www.semanticscholar.org/paper/LogicNets%3A-Co-Designed-Neural-Networks-and-Circuits-Umuroglu-Akhauri/8f407875eb303bbb6037e57efa57c0fffd4b6e00", "https://www.semanticscholar.org/paper/Towards-Unifying-Neural-Architecture-Space-and-Bhardwaj-Marculescu/15d55a44afe3d890caac1394cd56a8f1729f178d", "https://www.semanticscholar.org/paper/Exposing-Hardware-Building-Blocks-to-Machine-Akhauri/b08740bf7a7a45591b89434c987f268f5ba06211", "https://www.semanticscholar.org/paper/Searching-for-Stage-wise-Neural-Graphs-In-the-Limit-Zhou-Dou/d22c238549db5ecfd293d684ca1d5281e39b1970", "https://www.semanticscholar.org/paper/Class-dependent-Compression-of-Deep-Neural-Networks-Entezari-Saukh/3d75a85ee6b2371d908649a55fb0438aead14c78", "https://www.semanticscholar.org/paper/Chameleon%3A-Adaptive-Code-Optimization-for-Expedited-Ahn-Pilligundla/e4c965324f9773ec3111686a4abe53ea61c89f7c", "https://www.semanticscholar.org/paper/Exploring-Randomly-Wired-Neural-Networks-for-Image-Xie-Kirillov/e1cb7ddae92273dfb71f194e01586eb2342d847d", "https://www.semanticscholar.org/paper/Deep-Expander-Networks%3A-Efficient-Deep-Networks-Prabhu-Varma/2f9a8dda218a23e8a9751180b7d4527ddf5a4f04", "https://www.semanticscholar.org/paper/Neural-Architecture-Search-with-Reinforcement-Zoph-Le/67d968c7450878190e45ac7886746de867bf673d", "https://www.semanticscholar.org/paper/Learning-Implicitly-Recurrent-CNNs-Through-Sharing-Savarese-Maire/359cdea86e4203f73de4c12356fd9139dba9a745", "https://www.semanticscholar.org/paper/Densely-Connected-Convolutional-Networks-Huang-Liu/5694e46284460a648fe29117cbc55f6c9be3fa3c", "https://www.semanticscholar.org/paper/Learning-Sparse-Networks-Using-Targeted-Dropout-Gomez-Zhang/13de3c06ef6dac1c296ada45df2be590f843edb7", "https://www.semanticscholar.org/paper/Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/308e0a4523537e9fdc25a6afff67f9d214738d76", "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086", "https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Finding-Sparse%2C-Frankle-Carbin/21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096"]},
{"id": "Energy-and-Policy-Considerations-for-Deep-Learning-Strubell-Ganesh/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP", "authors": ["Emma Strubell", "Ananya Ganesh", "Andrew McCallum"], "date": "2019", "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.", "references": ["https://www.semanticscholar.org/paper/Green-AI-Schwartz-Dodge/fb73b93de3734a996829caf31e4310e0054e9c6b", "https://www.semanticscholar.org/paper/HYPEREMBED%3A-TRADEOFFS-BETWEEN-RESOURCES/80b846fd65cf35647021a68d0619c6d340543752", "https://www.semanticscholar.org/paper/Ensemble-Learning-for-Sustainable-NLP-Berman-Hari/5bca81eacbf8e45580a11db455585991f1e5759d", "https://www.semanticscholar.org/paper/RNN-Architecture-Learning-with-Sparse-Dodge-Schwartz/f0471ad6daecabbd2521c6cede46007a02260771", "https://www.semanticscholar.org/paper/LEVERAGING-SIMPLE-MODEL-PREDICTIONS/74fb0fe2cbd64a1cb8ad74795bb305fbd2c44909", "https://www.semanticscholar.org/paper/Leveraging-Simple-Model-Predictions-for-Enhancing-Dhurandhar-Shanmugam/10e8ed40fdf9b4c91ca67ce1757778a90f6c7ebc", "https://www.semanticscholar.org/paper/Question-Classification-with-Untrained-Recurrent-Sarli-Gallicchio/02d190f8518ce7f857568f5a1c373e66fac2ba50", "https://www.semanticscholar.org/paper/Understanding-and-Optimizing-Packed-Neural-Network-Liu-Krishnan/af63f8d0593b514b54145c1f1a3a7a6fe5bd9625", "https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38", "https://www.semanticscholar.org/paper/Streamlining-Tensor-and-Network-Pruning-in-PyTorch-Paganini-Forde/10918361e4458b82c9f64700182774c995d4f8c3", "https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/9a786d1ecf77dfba3459a83cd3fa0f1781bbcba4", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Deep-Biaffine-Attention-for-Neural-Dependency-Dozat-Manning/8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2", "https://www.semanticscholar.org/paper/Evaluating-the-Energy-Efficiency-of-Deep-Neural-on-Li-Chen/6cd61b00eb6248ead7ea029d2daa9d79e68572cb", "https://www.semanticscholar.org/paper/Algorithms-for-Hyper-Parameter-Optimization-Bergstra-Bardenet/03911c85305d42aa2eeb02be82ef6fb7da644dd0", "https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992", "https://www.semanticscholar.org/paper/The-Evolved-Transformer-So-Liang/16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "https://www.semanticscholar.org/paper/Practical-Bayesian-Optimization-of-Machine-Learning-Snoek-Larochelle/2e2089ae76fe914706e6fa90081a79c8fe01611e", "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe", "https://www.semanticscholar.org/paper/Linguistically-Informed-Self-Attention-for-Semantic-Strubell-Verga/060ff1aad5619a7d6d6cdfaf8be5da29bff3808c"]},
{"id": "Improving-Landmark-Localization-with-Learning-Honari-Molchanov/dcd2ac544a8336d73e4d3d80b158477c783e1e50", "title": "Improving Landmark Localization with Semi-Supervised Learning", "authors": ["Sina Honari", "Pavlo Molchanov", "Stephen Tyree", "Pascal Vincent", "Christopher Joseph Pal", "Jan Kautz"], "date": "2018", "abstract": "We present two techniques to improve landmark localization in images from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification or regression tasks related to the landmarks are more abundantly available. First, we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that these techniques, improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results on two toy datasets and four real datasets, with hands and faces, and report new state-of-the-art on two datasets in the wild, e.g. with only 5% of labeled images we outperform previous state-of-the-art trained on the AFLW dataset.", "references": ["https://www.semanticscholar.org/paper/Laplace-Landmark-Localization-Robinson-Li/5d684c197c06139319bf575e2498fe040d9ea32f", "https://www.semanticscholar.org/paper/Adaloss%3A-Adaptive-Loss-Function-for-Landmark-Teixeira-Tamersoy/c7786806cdae80b767c2933532474ff57ff30932", "https://www.semanticscholar.org/paper/Pixel-In-Pixel-Net%3A-Towards-Efficient-Facial-in-the-Jin-Liao/6344fbc54002b42a2cc44f08c669ce26b518528c", "https://www.semanticscholar.org/paper/Facial-Landmark-Machines%3A-A-Backbone-Branches-With-Liu-Li/d7c4e1a31f7997ee3bf4560d764523b7e999ae02", "https://www.semanticscholar.org/paper/Knee-Anatomical-Landmark-Localization-Using-Tiulpin/46b8e518823b770c81557926f1cccd124e5eea3d", "https://www.semanticscholar.org/paper/Structured-Landmark-Detection-via-Topology-Adapting-Li-Lu/d2d42ea605adb86be2a26699414545d70ea821df", "https://www.semanticscholar.org/paper/Eye-landmarks-detection-via-weakly-supervised-Huang-Chen/e91ad80bbbac5fd7875e6cc4d41cdee43f2b3b00", "https://www.semanticscholar.org/paper/Robust-Facial-Landmark-Detection-via-Heatmap-Offset-Zhang-Hu/209b86fffcf2efb0b1e33b35e843b1d48f0acd68", "https://www.semanticscholar.org/paper/Feature-extraction-on-faces-%3A-from-landmark-to-Honari/a18c1f139f1aeb42c9139affedd1961ca4bad82d", "https://www.semanticscholar.org/paper/Self-Supervised-Viewpoint-Learning-From-Image-Mustikovela-Jampani/9cb90d2d2b717410df276cbd4e49774ba9c123e7", "https://www.semanticscholar.org/paper/Robust-Facial-Landmark-Detection-via-Recurrent-Xiao-Feng/0ea96fe9ce2912e96a1d431962b3796576f9fad7", "https://www.semanticscholar.org/paper/Face-detection%2C-pose-estimation%2C-and-landmark-in-Zhu-Ramanan/bb12b81196df90cad4a964bb14edfdb113aeb4ce", "https://www.semanticscholar.org/paper/Deep-Deformation-Network-for-Object-Landmark-Yu-Zhou/a002a51773d3745b576d2284a8c3029d95ba38aa", "https://www.semanticscholar.org/paper/Learning-Deep-Representation-for-Face-Alignment-Zhang-Luo/ac039ca73684241f747645dd72be8f8b03edd191", "https://www.semanticscholar.org/paper/A-Deep-Regression-Architecture-with-Two-Stage-for-Lv-Shao/0e7a792ef33af26c26970ffc275d0ae82ee8f5d1", "https://www.semanticscholar.org/paper/Pose-Free-Facial-Landmark-Fitting-via-Optimized-and-Yu-Huang/7c5f674ff89df148b9bd39737628f16845da2822", "https://www.semanticscholar.org/paper/Efficient-object-localization-using-Convolutional-Tompson-Goroshin/ebcea2d842d3d4e320500086aff0deb4cb4412ff", "https://www.semanticscholar.org/paper/HyperFace%3A-A-Deep-Multi-Task-Learning-Framework-for-Ranjan-Patel/04eed24e26d9e6aaf2ca434cad20facd5feb83d0", "https://www.semanticscholar.org/paper/Interactive-Facial-Feature-Localization-Le-Brandt/95f12d27c3b4914e0668a268360948bce92f7db3", "https://www.semanticscholar.org/paper/Latent-Regression-Forest%3A-Structured-Estimation-of-Tang-Chang/db0750837ca4e905ffb419f98f893e8ddfaa2da2"]},
{"id": "Efficient-Bidirectional-Neural-Machine-Translation-Tan-Xia/7cb1715b4e20c304ddf64964425c0bbf964f73b4", "title": "Efficient Bidirectional Neural Machine Translation", "authors": ["Xu Tan", "Yingce Xia", "Lijun Wu", "Tao Qin"], "date": "2019", "abstract": "The encoder-decoder based neural machine translation usually generates a target sequence token by token from left to right. Due to error propagation, the tokens in the right side of the generated sequence are usually of poorer quality than those in the left side. In this paper, we propose an efficient method to generate a sequence in both left-to-right and right-to-left manners using a single encoder and decoder, combining the advantages of both generation directions. Experiments on three translation tasks show that our method achieves significant improvements over conventional unidirectional approach. Compared with ensemble methods that train and combine two models with different generation directions, our method saves 50% model parameters and about 40% training time, and also improve inference speed.", "references": ["https://www.semanticscholar.org/paper/Neural-Machine-Translation-from-Jordanian-Dialect-Al-Ibrahim-Duwairi/9e6cd6d97e2b19fb26d27354a9009511a7b7df93", "https://www.semanticscholar.org/paper/Agreement-on-Target-bidirectional-Neural-Machine-Liu-Utiyama/b8e7aa5ee3d280a9032fa9b15a4b79cc021f33ed", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System%3A-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Towards-Neural-Phrase-based-Machine-Translation-Huang-Wang/5616064812996ab1fae525f9679f300c7c307895", "https://www.semanticscholar.org/paper/Layer-Wise-Coordination-between-Encoder-and-Decoder-He-Tan/b12ccd118974839db290f15c989649b2b5188636", "https://www.semanticscholar.org/paper/Edinburgh-Neural-Machine-Translation-Systems-for-16-Sennrich-Haddow/1a5ea605111eb3403868d4b679315e944beee8c6", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-in-Linear-Time-Kalchbrenner-Espeholt/98445f4172659ec5e891e031d8202c102135c644", "https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e", "https://www.semanticscholar.org/paper/Dense-Information-Flow-for-Neural-Machine-Shen-Tan/636280d54ea08cbe37cb4aba3efcec59349eaefa"]},
{"id": "ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "date": "2012", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "references": ["https://www.semanticscholar.org/paper/Understanding-of-a-convolutional-neural-network-Albawi-Mohammed/3a84214cb69ea0b34352285029f368b75718c32b", "https://www.semanticscholar.org/paper/Understanding-of-a-convolutional-neural-network-Albawi-Mohammed/3a84214cb69ea0b34352285029f368b75718c32b", "https://www.semanticscholar.org/paper/Study-of-Residual-Networks-for-Image-Recognition-Ebrahimi-Abadi/639f1998e947536d5860414c54cc229797cb19d1", "https://www.semanticscholar.org/paper/Study-of-Residual-Networks-for-Image-Recognition-Ebrahimi-Abadi/639f1998e947536d5860414c54cc229797cb19d1", "https://www.semanticscholar.org/paper/Image-Classification-Based-on-the-Boost-Neural-Lee-Chen/5c3630a6e745dc518f3d589e54e9433ae3dc8e9f", "https://www.semanticscholar.org/paper/Image-Classification-Based-on-the-Boost-Neural-Lee-Chen/5c3630a6e745dc518f3d589e54e9433ae3dc8e9f", "https://www.semanticscholar.org/paper/Tiny-ImageNet-Classification-with-Convolutional-Yao-Miller/cc2d885d915accad10613fdd103ad47b783a28ea", "https://www.semanticscholar.org/paper/Tiny-ImageNet-Classification-with-Convolutional-Yao-Miller/cc2d885d915accad10613fdd103ad47b783a28ea", "https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-Zhang/27b124d8fce9c01f3a930a4fe6779e73e1183e52", "https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-Zhang/27b124d8fce9c01f3a930a4fe6779e73e1183e52", "https://www.semanticscholar.org/paper/Convolutional-Deep-Belief-Networks-on-CIFAR-10-Krizhevsky/bea5780d621e669e8069f05d0f2fc0db9df4b50f", "https://www.semanticscholar.org/paper/High-Performance-Neural-Networks-for-Visual-Object-Ciresan-Meier/82b9099ddf092463f497bd48bb112c46ca52c4d1", "https://www.semanticscholar.org/paper/Multi-column-deep-neural-networks-for-image-Ciresan-Meier/398c296d0cc7f9d180f84969f8937e6d3a413796", "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086", "https://www.semanticscholar.org/paper/Convolutional-networks-and-applications-in-vision-LeCun-Kavukcuoglu/c43025c429b1fbf6f1379f61801a1b40834d62e7", "https://www.semanticscholar.org/paper/Metric-Learning-for-Large-Scale-Image-Generalizing-Mensink-Verbeek/3a4a53fe47036ac89dad070ab87a9d8795b139b1", "https://www.semanticscholar.org/paper/Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus/5562a56da3a96dae82add7de705e2bd841eb00fc", "https://www.semanticscholar.org/paper/Learning-methods-for-generic-object-recognition-to-LeCun-Huang/f354310098e09c1e1dc88758fca36767fd9d084d", "https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52"]},
{"id": "Analyzing-Word-Translation-of-Transformer-Layers-Xu-Genabith/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9", "title": "Analyzing Word Translation of Transformer Layers", "authors": ["Hongfei Xu", "Josef van Genabith", "Deyi Xiong", "Qiuhui Liu"], "date": "2020", "abstract": "The Transformer translation model is popular for its effective parallelization and performance. Though a wide range of analysis about the Transformer has been conducted recently, the role of each Transformer layer in translation has not been studied to our knowledge. In this paper, we propose approaches to analyze the translation performed in encoder / decoder layers of the Transformer. Our approaches in general project the representations of an analyzed layer to the pre-trained classifier and measure the word translation accuracy. For the analysis of encoder layers, our approach additionally learns a weight vector to merge multiple attention matrices into one and transform the source encoding to the target side with the merged alignment matrix to align source tokens with target translations while bridging different input - output lengths. While analyzing decoder layers, we additionally study the effects of the source context and the decoding history in word prediction through bypassing the corresponding self-attention or cross-attention sub-layers. Our analysis reveals that the translation starts at the very beginning of the \"encoding\" (specifically at the source word embedding layer), and shows how translation evolves during the forward computation of layers. Based on observations gained in our analysis, we propose that increasing encoder depth while removing the same number of decoder layers can simply but significantly boost the decoding speed. Furthermore, simply inserting a linear projection layer before the decoder classifier which shares the weight matrix with the embedding layer can effectively provide small but consistent and significant improvements in our experiments on the WMT 14 English-German, English-French and WMT 15 Czech-English translation tasks (+0.42, +0.37 and +0.47 respectively).", "references": ["https://www.semanticscholar.org/paper/The-Bottom-up-Evolution-of-Representations-in-the-A-Voita-Sennrich/112fd54ee193237b24f2ce7fce79e399609a29c5", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/Accelerating-Neural-Transformer-via-an-Average-Zhang-Xiong/6e45251b16cd423f3c025f004959c6d2b26efab0", "https://www.semanticscholar.org/paper/Transformer-Dissection%3A-An-Unified-Understanding-of-Tsai-Bai/f9080cc8474881e602c28dec8c2d910db83851aa", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Towards-Understanding-Neural-Machine-Translation-He-Tu/fe090ea947cf728cdef6cb5ef00e1e4fbf94e181", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Fine-grained-Analysis-of-Sentence-Embeddings-Using-Adi-Kermany/e44da7d8c71edcc6e575fa7faadd5e75785a7901", "https://www.semanticscholar.org/paper/The-Lazy-Encoder%3A-A-Fine-Grained-Analysis-of-the-of-Bisazza-Tump/af96e98684c278b66769b4656af069585ac6df85", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},
{"id": "Syntactically-Supervised-Transformers-for-Faster-Akoury-Krishna/d6e21619df572d04b2b2d97b4c5d1fd604f185fb", "title": "Syntactically Supervised Transformers for Faster Neural Machine Translation", "authors": ["Nader Akoury", "Kalpesh Krishna", "Mohit Iyyer"], "date": "2019", "abstract": "Standard decoders for neural machine translation autoregressively generate a single target token per time step, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi- autoregressive decoding produce multiple tokens per time step independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~ 5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.", "references": ["https://www.semanticscholar.org/paper/Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "https://www.semanticscholar.org/paper/Minimizing-the-Bag-of-Ngrams-Difference-for-Neural-Shao-Zhang/775f6da764a134e7bd0e361c27f7d7411afef4d3", "https://www.semanticscholar.org/paper/Syntax-driven-Iterative-Expansion-Language-Models-Casas-Fonollosa/27e441cfd261eb73074fd204261510577f2d434d", "https://www.semanticscholar.org/paper/Neural-Machine-Translation%3A-A-Review-Stahlberg/4d08dcd2cc1e9691defe664a10f021424a896a1e", "https://www.semanticscholar.org/paper/The-roles-of-language-models-and-hierarchical-in-Stahlberg/38b40ae531ddca434de07015637b78413370d15a", "https://www.semanticscholar.org/paper/Hard-Coded-Gaussian-Attention-for-Neural-Machine-You-Sun/07a9f47885cae97efb7b4aa109392128532433da", "https://www.semanticscholar.org/paper/NON-AUTOREGRESSIVE-MACHINE-TRANSLATION-Zhou-Gu/dec6bb3c7bb671c86296a2a089e0e38aa3f69279", "https://www.semanticscholar.org/paper/Understanding-Knowledge-Distillation-in-Machine-Zhou-Neubig/1e5b826ddf0754f6e93234ba1260bd939c255e7f", "https://www.semanticscholar.org/paper/HAT%3A-Hardware-Aware-Transformers-for-Efficient-Wang-Wu/ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Semi-Autoregressive-Neural-Machine-Translation-Wang-Zhang/6a1c61a0da5f56a3fdfca5515767cfd74529524a", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/Syntactically-Guided-Neural-Machine-Translation-Stahlberg-Hasler/c767849ab921c9983b0c00712902954b833df524", "https://www.semanticscholar.org/paper/End-to-End-Non-Autoregressive-Neural-Machine-with-Libovick%C3%BD-Helcl/5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Syntax-Augmented-Machine-Translation-via-Chart-Zollmann-Venugopal/56bc078f0b7b4c6112001af12527b3f7fcf4f021", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "https://www.semanticscholar.org/paper/Towards-String-To-Tree-Neural-Machine-Translation-Aharoni-Goldberg/fc8eb74acd25169672b3b85517da3d66eb0c47cf", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow/1af68821518f03568f913ab03fc02080247a27ff"]},
{"id": "Multilingual-Neural-Machine-Translation-with-Tan-Ren/1b24b7b4ac2427d20ab60c8451563eb8d99caf9c", "title": "Multilingual Neural Machine Translation with Knowledge Distillation", "authors": ["Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Zhou Zhao", "Tie-Yan Liu"], "date": "2019", "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.", "references": ["https://www.semanticscholar.org/paper/Knowledge-Distillation-for-Multilingual-Neural-Sun-Wang/1b856b7dd486d0db7565031720db4e051420ec3b", "https://www.semanticscholar.org/paper/Multilingual-Neural-Machine-Translation-with-Tan-Chen/3198d56ff8998070e9b2fd663d8257b5b697e0e9", "https://www.semanticscholar.org/paper/A-Study-of-Multilingual-Neural-Machine-Translation-Tan-Leng/c3b8ced521aa755c5602f3ac3f2e3144141e6cb8", "https://www.semanticscholar.org/paper/Massively-Multilingual-Neural-Machine-Translation-Arivazhagan-Bapna/c17985a669522e7e85ae3d34754c7df49c7187d1", "https://www.semanticscholar.org/paper/A-Survey-of-Multilingual-Neural-Machine-Translation-Dabre-Chu/b4693a93b033d6ec6c5c98a22797e43928ac7470", "https://www.semanticscholar.org/paper/A-Brief-Survey-of-Multilingual-Neural-Machine-Dabre-Chu/b081b540ed633f87e98bf94f0e768a4a48d9a66a", "https://www.semanticscholar.org/paper/A-Comprehensive-Survey-of-Multilingual-Neural-Dabre-Chu/01fd67db5f895a9f9da353e93df37d31b38b9552", "https://www.semanticscholar.org/paper/Language-Graph-Distillation-for-Low-Resource-He-Chen/684de356506d59bd32b4b9ee5c21678dd53cce86", "https://www.semanticscholar.org/paper/Structure-Level-Knowledge-Distillation-For-Sequence-Wang-Jiang/82eb7ed876778df5114e92ba8156444ec83461f5", "https://www.semanticscholar.org/paper/Building-a-Multi-domain-Neural-Machine-Translation-Mghabbar-Ratnamogan/9b4c9f7714fa60d54feb9527c86ba4dfd9adfa51", "https://www.semanticscholar.org/paper/Toward-Multilingual-Neural-Machine-Translation-with-Ha-Niehues/2d9604cdc3ef786b50b53aaf440d451ad16e7fb9", "https://www.semanticscholar.org/paper/Multi-Way%2C-Multilingual-Neural-Machine-Translation-Firat-Cho/9ed9bff37ec952134564b3b2a022b7aba9479ff2", "https://www.semanticscholar.org/paper/Rapid-Adaptation-of-Neural-Machine-Translation-to-Neubig-Hu/205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4", "https://www.semanticscholar.org/paper/Ensemble-Distillation-for-Neural-Machine-Freitag-Al-Onaizan/5a3a8e0a3ee8711ca9e7df7d1a22e532680c78ed", "https://www.semanticscholar.org/paper/Google%E2%80%99s-Multilingual-Neural-Machine-Translation-Johnson-Schuster/a486e2839291111bb44fa1f07731ada123539f75", "https://www.semanticscholar.org/paper/A-neural-interlingua-for-multilingual-machine-Lu-Keung/1252ffc3868958867f2394de7161ee857fbf6d3f", "https://www.semanticscholar.org/paper/Universal-Neural-Machine-Translation-for-Extremely-Gu-Hassan/2115a9127cb7ea4dfd5c3a08bfc41bab3b279266", "https://www.semanticscholar.org/paper/Meta-Learning-for-Low-Resource-Neural-Machine-Gu-Wang/a75869d69cc86f501939c237ae4711aa2885f6a6", "https://www.semanticscholar.org/paper/Tied-Transformers%3A-Neural-Machine-Translation-with-Xia-He/3e0b9b1e4d354bb7caef3fc90fa9828b483c6e81", "https://www.semanticscholar.org/paper/Sequence-Level-Knowledge-Distillation-Kim-Rush/57a10537978600fd33dcdd48922c791609a4851a"]},
{"id": "Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "title": "Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information", "authors": ["Qiu Ran", "Yankai Lin", "Peng Li", "Jie Zhou"], "date": "2019", "abstract": "Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the enormous decoding space. To address this problem, we propose a novel NAT framework named ReorderNAT which explicitly models the reordering information in the decoding procedure. We further introduce deterministic and non-deterministic decoding strategies that utilize reordering information to narrow the decoding search space in our proposed ReorderNAT. Experimental results on various widely-used datasets show that our proposed model achieves better performance compared to existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a significant speedup.", "references": ["https://www.semanticscholar.org/paper/Minimizing-the-Bag-of-Ngrams-Difference-for-Neural-Shao-Zhang/775f6da764a134e7bd0e361c27f7d7411afef4d3", "https://www.semanticscholar.org/paper/Deep-Encoder%2C-Shallow-Decoder%3A-Reevaluating-the-in-Kasai-Pappas/8c5a394654822a5de53ac2e4a355c1c6ead4750c", "https://www.semanticscholar.org/paper/Parallel-Machine-Translation-with-Disentangled-Kasai-Cross/8b7a8a08f4f339e25c447a144cb5de4ad2e84a7b", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Semi-Autoregressive-Neural-Machine-Translation-Wang-Zhang/6a1c61a0da5f56a3fdfca5515767cfd74529524a", "https://www.semanticscholar.org/paper/End-to-End-Non-Autoregressive-Neural-Machine-with-Libovick%C3%BD-Helcl/5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Reordering-Chen-Wang/957f53525b386d6f31077449c8ec23b83c19f506", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/Syntactically-Supervised-Transformers-for-Faster-Akoury-Krishna/d6e21619df572d04b2b2d97b4c5d1fd604f185fb", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Constant-Time-Machine-Translation-with-Conditional-Ghazvininejad-Levy/1a98ce71556e4602b313d424b3d689e026ca4706", "https://www.semanticscholar.org/paper/Exploiting-Pre-Ordering-for-Neural-Machine-Zhao-Zhang/ddc9c7bea4c4c55325ccfd2e084ee80ea0f386db"]},
{"id": "Efficient-Pairwise-Classification-and-Ranking-Park-F%C3%BCrnkranz/a19511548159c87c89881bb8c5b3a9de6de0650e", "title": "Efficient Pairwise Classification and Ranking", "authors": ["Sang-Hyeun Park", "Johannes F\u00fcrnkranz"], "date": "2007", "abstract": "Pairwise classification is a class binarization procedure that converts a multi-class problem into a series of two-class problems, one problem for each pair of classes. While it can be shown that for training, this procedure is more efficient than the more commonly used one-against-all approach, it still has to evaluate a quadratic number of classifiers when computing the predicted class for a given example. In this paper, we propose a method that allows a faster computation of the predicted class when weighted or unweighted voting are used for combining the predictions of the individual classifiers. While its worst-case complexity is still quadratic in the number of classes, we show that even in the case of completely random base classifiers, our method still outperforms the conventional pairwise classifier. For the more practical case of well-trained base classifiers, its asymptotic computational complexity seems to be almost linear. We also propose a method for approximating the full class ranking, based on the Swiss System, a common scheme for conducting multi-round chess tournaments. Our results indicate that this adaptive scheme offers a better trade-off between approximation quality and number of performed comparisons than alternative, fixed schemes for ordering the evaluation of the pairwise classifiers.", "references": ["https://www.semanticscholar.org/paper/Efficient-Pairwise-Classification-Park-F%C3%BCrnkranz/d3796b40df755ed87f5e915508a5b4259cf2cd9d", "https://www.semanticscholar.org/paper/Efficient-prediction-algorithms-for-binary-Park-F%C3%BCrnkranz/212aadd5ce16e0efb4151ecd33263949a6b2a290", "https://www.semanticscholar.org/paper/Advances-in-Efficient-Pairwise-Multilabel-Menc%C3%ADa-Park/f75be5708fd48ddeb939ecb373abb0ebd8f11234", "https://www.semanticscholar.org/paper/Round-Robin-Classification-F%C3%BCrnkranz/83d40f5e09b1143464d9c297bdb8c571e9a5ad4c", "https://www.semanticscholar.org/paper/Round-robin-ensembles-F%C3%BCrnkranz/693897783f227628de6380ff55b3bd1ccdcd0e6a", "https://www.semanticscholar.org/paper/How-to-Do-Multi-way-Classification-with-Two-Way-Cutzu/647559d2234aca1430dcd3036f39beb787e1e798", "https://www.semanticscholar.org/paper/Large-Margin-DAGs-for-Multiclass-Classification-Platt-Cristianini/32484f6d111bf21f1395a34a087991a9041dd0ae", "https://www.semanticscholar.org/paper/Learning-Label-Preferences%3A-Ranking-Error-Versus-H%C3%BCllermeier-F%C3%BCrnkranz/decc12528e188e24c615186d6b145c0cf9300261", "https://www.semanticscholar.org/paper/Ranking-by-pairwise-comparison-a-note-on-risk-H%C3%BCllermeier-F%C3%BCrnkranz/3d914f5b6c314659e13072eca27db38878c6768f", "https://www.semanticscholar.org/paper/Polychotomous-Classification-with-Pairwise-A-New-Cutzu/0cdf8b5d357efc3f4028588583d6685f47c62b1b", "https://www.semanticscholar.org/paper/Probability-Estimates-for-Multi-class-by-Pairwise-Wu-Lin/0d02083f5e9206ff88ed1e284da8665d4bee81ba", "https://www.semanticscholar.org/paper/Classification-by-Pairwise-Coupling-Hastie-Tibshirani/f642a692da944604a7df590e9f9fa06089b7991a", "https://www.semanticscholar.org/paper/A-Comparative-Study-on-Feature-Selection-in-Text-Yang-Pedersen/c3ebcef26c22a373b6f26a67934213eb0582804e"]},
{"id": "NON-AUTOREGRESSIVE-MACHINE-TRANSLATION-Zhou-Gu/dec6bb3c7bb671c86296a2a089e0e38aa3f69279", "title": "NON-AUTOREGRESSIVE MACHINE TRANSLATION", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "date": "2020", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial in NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the complexity of the distilled data that provides the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve state-of-theart performance for NAT-based models, and close the gap with the autoregressive baseline on the WMT14 En-De benchmark.1", "references": ["https://www.semanticscholar.org/paper/Semi-Autoregressive-Neural-Machine-Translation-Wang-Zhang/6a1c61a0da5f56a3fdfca5515767cfd74529524a", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "https://www.semanticscholar.org/paper/Retrieving-Sequential-Information-for-Neural-Shao-Feng/2b68036936596af716d529a8752a5b42edbf7251", "https://www.semanticscholar.org/paper/Blockwise-Parallel-Decoding-for-Deep-Autoregressive-Stern-Shazeer/5e04881e91bff952d102d967c4ffb498ec30d4af", "https://www.semanticscholar.org/paper/FlowSeq%3A-Non-Autoregressive-Conditional-Sequence-Ma-Zhou/75fe6c3ffdea2608794b4f21119c5a4dec07663a", "https://www.semanticscholar.org/paper/Sequence-Level-Knowledge-Distillation-Kim-Rush/57a10537978600fd33dcdd48922c791609a4851a", "https://www.semanticscholar.org/paper/Latent-Variable-Non-Autoregressive-Neural-Machine-a-Shu-Lee/d6cfb4e345b1031040ccd3683730854c560a2b0d", "https://www.semanticscholar.org/paper/Constant-Time-Machine-Translation-with-Conditional-Ghazvininejad-Levy/1a98ce71556e4602b313d424b3d689e026ca4706", "https://www.semanticscholar.org/paper/Levenshtein-Transformer-Gu-Wang/f87de21b46683b5743c4d82af3c9cb8bbcd26f21"]},
{"id": "Fast-Structured-Decoding-for-Sequence-Models-Sun-Li/182e0fe967972c19d99b83a5e8c317014fb3ae28", "title": "Fast Structured Decoding for Sequence Models", "authors": ["Zhiqing Sun", "Zhuohan Li", "Haoqing Wang", "Zi Lin", "Di He", "Zhi-Hong Deng"], "date": "2019", "abstract": "Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve then decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8~14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.", "references": ["https://www.semanticscholar.org/paper/LAVA-NAT%3A-A-Non-Autoregressive-Translation-Model-Li-Meng/2c3859564617e910651b56373e6c0941656f32ff", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Latent-Saharia-Chan/f6127bbe33d7e5776d3c313304b35d27e1051459", "https://www.semanticscholar.org/paper/Aligned-Cross-Entropy-for-Non-Autoregressive-Ghazvininejad-Karpukhin/3f11a2124af139af7c6f17eccab5149d759d7f52", "https://www.semanticscholar.org/paper/ENGINE%3A-Energy-Based-Inference-Networks-for-Machine-Tu-Pang/f2c0b478a30e653157dcdfe879b3082c6bbb0913", "https://www.semanticscholar.org/paper/Semi-Autoregressive-Training-Improves-Mask-Predict-Ghazvininejad-Levy/7b28577ed96bd1b76bbe79859b3222604b2dc369", "https://www.semanticscholar.org/paper/POINTER%3A-Constrained-Text-Generation-via-Generative-Zhang-Wang/3d58eb3c0a8ea3b81509d12ded42c5ad9e2076a7", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Dialogue-Generation-Han-Meng/d02d402cabe6d19649d823c4582028faf5667459", "https://www.semanticscholar.org/paper/Cascaded-Text-Generation-with-Markov-Transformers-Deng-Rush/195b06b4bfb0b478c7b6c3e3b5c7a7a0d7bdae18", "https://www.semanticscholar.org/paper/Deep-Encoder%2C-Shallow-Decoder%3A-Reevaluating-the-in-Kasai-Pappas/8c5a394654822a5de53ac2e4a355c1c6ead4750c", "https://www.semanticscholar.org/paper/Parallel-Machine-Translation-with-Disentangled-Kasai-Cross/8b7a8a08f4f339e25c447a144cb5de4ad2e84a7b", "https://www.semanticscholar.org/paper/Fast-Decoding-in-Sequence-Models-using-Discrete-Kaiser-Roy/2d08ed53491053d84b6de89aedbf2178b9c8cf84", "https://www.semanticscholar.org/paper/Hint-Based-Training-for-Non-Autoregressive-Machine-Li-Lin/5b446648504afeecf7c73028aa02c2da16db6224", "https://www.semanticscholar.org/paper/Non-Autoregressive-Machine-Translation-with-Wang-Tian/d9291740b644fc5feb4999c76ec2f50457ef3a77", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-with-Guo-Tan/9d9c4a3b27dab2293ed915ad69662ea09901ccab", "https://www.semanticscholar.org/paper/End-to-End-Non-Autoregressive-Neural-Machine-with-Libovick%C3%BD-Helcl/5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Sequence-Level-Knowledge-Distillation-Kim-Rush/57a10537978600fd33dcdd48922c791609a4851a", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Theory-and-Experiments-on-Vector-Quantized-Roy-Vaswani/6f6127faf516fb4dc47a24aaab9d5e96c4fcc751", "https://www.semanticscholar.org/paper/Deterministic-Non-Autoregressive-Neural-Sequence-by-Lee-Mansimov/9c5c89199114858eafbe50b46d77d38ffd03b28a"]},
{"id": "An-Overview-of-Label-Space-Dimension-Reduction-for-Tang-Liu/7d95ca8f385a3e6977c408fd5b73f12e560a022d", "title": "An Overview of Label Space Dimension Reduction for Multi-Label Classification", "authors": ["Lin Tang", "Lin Liu", "JianHou Gan"], "date": "2017", "abstract": "Multi-label classification with many labels are common in real-world application. However, traditional multi-label classifiers often become computationally inefficient for hundreds or even thousands of labels. Therefore, the label space dimension reduction is designed to address this problem. In this paper, the existing studies of label space dimension reduction are summarized; especially, these studies were classified into two categories: label space dimension reduction based on transformed labels and label subset; meanwhile, we analyze the studies belonging to each type and give the experimental comparison of two typical LSDR algorithms. To the best of our knowledge, this is the first effort to review the development of label space dimension reduction.", "references": ["https://www.semanticscholar.org/paper/Multi-label-classification-using-boolean-matrix-Wicker-Pfahringer/5d7b344dd9abadd798bfe42719e384e6ade46136", "https://www.semanticscholar.org/paper/Multi-Label-Prediction-via-Compressed-Sensing-Hsu-Kakade/cf40878c2de992b6be053f79d3e97d20307dba26", "https://www.semanticscholar.org/paper/Improved-algorithms-for-exact-and-approximate-Sun-Ye/2611084ebd0205179f723a2710dc3a84dcaf740e"]},
{"id": "Multi-label-Classification-via-Multi-target-on-Data-Osojnik-Panov/05807a506b8bbbff822375fb096834c43028ea71", "title": "Multi-label Classification via Multi-target Regression on Data Streams", "authors": ["Aljaz Osojnik", "Pance Panov", "Saso Dzeroski"], "date": "2015", "abstract": "Multi-label classification is becoming more and more critical in data mining applications. Many efficient methods exist in the classical batch setting, however, in the streaming setting, comparatively few methods exist. In this paper, we propose a new methodology for multi-label classification via multi-target regression in a streaming setting and develop a streaming multi-target regressor iSOUP-Tree, which uses this approach. We experimentally evaluated two variants of the iSOUP-Tree algorithm, and determined that the use of regression trees is advisable over the use model trees. Furthermore, we compared our results to the state-of-the-art and found that the iSOUP-Tree method is comparable to the other streaming multi-label learners. This is a motivation for the potential use of iSOUP-Tree in an ensemble setting as a base learner.", "references": ["https://www.semanticscholar.org/paper/Multi-label-classification-via-multi-target-on-data-Osojnik-Panov/e29d7acaba4b81b0022dff238f93006dca6d0224", "https://www.semanticscholar.org/paper/Multi-label-classification-via-multi-target-on-data-Osojnik-Panov/e29d7acaba4b81b0022dff238f93006dca6d0224", "https://www.semanticscholar.org/paper/Scalable-and-efficient-multi-label-classification-Read-Bifet/697349e6c83b1eb71ce41995e97345c0a41fb131", "https://www.semanticscholar.org/paper/Incremental-multi-target-model-trees-for-data-Ikonomovska-Gama/c8109a00abdb6cddf8f3fe58f1d8c5755d577310", "https://www.semanticscholar.org/paper/Dealing-with-Concept-Drift-and-Class-Imbalance-in-Xioufis-Spiliopoulou/d5bf225a082e87a1903bdae1535c53539cb9f274", "https://www.semanticscholar.org/paper/Efficient-class-incremental-learning-for-of-data-Shi-Xue/bc1897b655b9ceadf7532a3e9f832ab818d87fa4", "https://www.semanticscholar.org/paper/Multi-target-regression-via-input-space-expansion%3A-Xioufis-Tsoumakas/6d90968a8f2ab393f5ff1c1fe2fa4472d5fc21f3", "https://www.semanticscholar.org/paper/An-extensive-experimental-comparison-of-methods-for-Madjarov-Kocev/0344d69472e7c915ae97077e2626c522ec890a26", "https://www.semanticscholar.org/paper/Classifier-chains-for-multi-label-classification-Read-Pfahringer/e75a71274522980dce85b8ad00369d0a33667ea4", "https://www.semanticscholar.org/paper/Mining-Multi-label-Concept-Drifting-Data-Streams-Qu-Zhang/a724c5a4255579d6e32920633e54b6b46c95f795", "https://www.semanticscholar.org/paper/Drift-Detection-for-Multi-label-Data-Streams-Based-Shi-Wen/d3331264ed94acb3f2b982fe4cae5b51e5c76636"]},
{"id": "Class-dependent-Compression-of-Deep-Neural-Networks-Entezari-Saukh/3d75a85ee6b2371d908649a55fb0438aead14c78", "title": "Class-dependent Compression of Deep Neural Networks", "authors": ["Rahim Entezari", "Olga Saukh"], "date": "2019", "abstract": "Today's deep neural networks require substantial computation resources for their training, storage and inference, which limits their effective use on resource-constrained devices. On the one hand, many recent research activities explore different options of compressing and optimizing deep models. On the other hand, in many real-world applications we face the class imbalance problem, e.g. higher number of false positives produced by a compressed network may be tolerable, yet the number of false negatives must stay low. The problem originates from either an intrinsic nature of the imbalanced samples within the training data set, or from the fact that some classes are more important for the application domain of the model, e.g. in medical imaging. In this paper, we propose a class-dependent network compression method based on a newly introduced network pruning technique used to search for lottery tickets in an original deep network. We introduce a novel combined loss function to find efficient compressed sub-networks with the same or even lower number of false negatives compared to the original network. Our experimental evaluation using three benchmark data sets shows that the resulting compressed sub-networks achieve up to 50% lower number of false negatives and an overall higher AUC-ROC measure, yet use up to 99% fewer parameters compared to the original network.", "references": ["https://www.semanticscholar.org/paper/PhD-Forum-Abstract%3A-Understanding-Deep-Model-for-Entezari/fe5b75e4b5ac8a471e4cdcf2edc077937fee0f19", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/Adversarial-Network-Compression-Belagiannis-Farshad/b82accfcc4572d1cb33c2a41aecfb6e7e606def9", "https://www.semanticscholar.org/paper/DARC%3A-Differentiable-ARchitecture-Compression-Singh-Khetan/ee79ea251b165f976f0b2ac404a36d076eca8eb8", "https://www.semanticscholar.org/paper/Compressing-GANs-using-Knowledge-Distillation-Aguinaldo-Chiang/e7f2dc1c52dfce9c29f879808c7a6a601845d7bc", "https://www.semanticscholar.org/paper/Network-Trimming%3A-A-Data-Driven-Neuron-Pruning-Deep-Hu-Peng/60ae4f18cb53efff0174e3fea7064049737e1e67", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Learning-Deep-Representation-for-Imbalanced-Huang-Li/c88dbaa5d8f4c915e286be5e38b5599038220493", "https://www.semanticscholar.org/paper/Multi-Task-Zipping-via-Layer-wise-Neuron-Sharing-He-Zhou/e91d28c21b074bec7ea947c917cb3f9f71cf8916", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Learning-to-Prune-Deep-Neural-Networks-via-Optimal-Dong-Chen/773d5ddc414424a8948446ddaa5275b944f50891"]},
{"id": "Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02", "title": "Auto-Encoding Variational Bayes", "authors": ["Diederik P. Kingma", "Max Welling"], "date": "2014", "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.", "references": ["https://www.semanticscholar.org/paper/Likelihood-Almost-Free-Inference-Networks-Zheng-Yang/62bdbf02d6b2103615dbb73d0021eecb88af2dd3", "https://www.semanticscholar.org/paper/Asymmetric-Variational-Autoencoders.-Zheng-Yang/1fab230f9fc5b5445796bd5ce3dcdef867020962", "https://www.semanticscholar.org/paper/Trust-Region-Sequential-Variational-Inference-Kim-Jang/7f6618e7610f33a0f6446fe25fe2cc9794aa9b47", "https://www.semanticscholar.org/paper/Tutorial%3A-Deriving-the-Standard-Variational-(VAE)-Odaibo/3c64fa3cc7b1188da82e89125af0c59503467ea8", "https://www.semanticscholar.org/paper/Variational-Gaussian-Process-Tran-Ranganath/ed032736652ac7e1f36ea17bd253cd1bfdcc3864", "https://www.semanticscholar.org/paper/Fixing-a-Broken-ELBO-Alemi-Poole/8976e91ccae57a20c29f3c9d88bf45b19973c952", "https://www.semanticscholar.org/paper/Variational-Bayes-on-Monte-Carlo-Steroids-Grover-Ermon/1fd2a94b4e42c00b40770cb16518ad6b2756f431", "https://www.semanticscholar.org/paper/Variational-Measure-Preserving-Flows-Zhang-Hern%C3%A1ndez-Lobato/692546fb94ad97bcf9d4bef1393afc4f4309ecbd", "https://www.semanticscholar.org/paper/Sampling-Free-Variational-Inference-of-Bayesian-by-Kandemir-Haussmann/89b991bf028b1e5edd27116aa2c5baa63a1eb435", "https://www.semanticscholar.org/paper/Faithful-Model-Inversion-Substantially-Improves-Webb-Golinski/28177298a99dba915a6e63383da93d3b189c451e", "https://www.semanticscholar.org/paper/Black-Box-Variational-Inference-Ranganath-Gerrish/6a667700100e228cb30a5d884258a0db921603fe", "https://www.semanticscholar.org/paper/Stochastic-Back-propagation-and-Variational-in-Deep-Rezende-Mohamed/f87247fb37f6b48da0757d7a1acf38da44510cdb", "https://www.semanticscholar.org/paper/Variational-Bayesian-Inference-with-Stochastic-Paisley-Blei/9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "https://www.semanticscholar.org/paper/Stochastic-variational-inference-Hoffman-Blei/bccb2f99a9d1c105699f5d88c479569085e2c7ba", "https://www.semanticscholar.org/paper/Deep-Generative-Stochastic-Networks-Trainable-by-Bengio-Thibodeau-Laufer/5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d", "https://www.semanticscholar.org/paper/Fixed-Form-Variational-Posterior-Approximation-Salimans-Knowles/de58806bca096e0be83d74c353233898e37b69f3", "https://www.semanticscholar.org/paper/Efficient-Learning-of-Deep-Boltzmann-Machines-Salakhutdinov-Larochelle/00cd1dab559a9671b692f39f14c1573ab2d1416b", "https://www.semanticscholar.org/paper/Deep-AutoRegressive-Networks-Gregor-Danihelka/695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "https://www.semanticscholar.org/paper/Representation-Learning%3A-A-Review-and-New-Bengio-Courville/184ac0766262312ba76bbdece4e7ffad0aa8180b", "https://www.semanticscholar.org/paper/Stacked-Denoising-Autoencoders%3A-Learning-Useful-in-Vincent-Larochelle/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd"]},
{"id": "Chameleon%3A-Adaptive-Code-Optimization-for-Expedited-Ahn-Pilligundla/e4c965324f9773ec3111686a4abe53ea61c89f7c", "title": "Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation", "authors": ["Byung Hoon Ahn", "Prannoy Pilligundla", "Amir Yazdanbakhsh", "Hadi Esmaeilzadeh"], "date": "2020", "abstract": "Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed CHAMELEON leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that CHAMELEON provides 4.45\u00d7speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%.", "references": ["https://www.semanticscholar.org/paper/Ordering-Chaos%3A-Memory-Aware-Scheduling-of-Wired-Ahn-Lee/fd179b1e7fc4040ec73aa23ddfc9a61a43c3d600", "https://www.semanticscholar.org/paper/The-Deep-Learning-Compiler%3A-A-Comprehensive-Survey-Li-Liu/69046519775ca6ac40c7d577887149525df2ee5d", "https://www.semanticscholar.org/paper/TVM%3A-An-Automated-End-to-End-Optimizing-Compiler-Chen-Moreau/df013a17ab84d5403361da4538a04d574f58be83", "https://www.semanticscholar.org/paper/HAQ%3A-Hardware-Aware-Automated-Quantization-With-Wang-Liu/54c4642d017830e1faddbb49f0377228d2b01493", "https://www.semanticscholar.org/paper/Reinforced-Genetic-Algorithm-Learning-for-Graphs-Paliwal-Gimeno/93c14eb4f3d588ee6ca776df952b8271a2c192b3", "https://www.semanticscholar.org/paper/Post%3A-Device-Placement-with-Cross-Entropy-and-Gao-Chen/74b5515ada5667f02f3b198eeb726b5cd347708f", "https://www.semanticscholar.org/paper/Ordering-Chaos%3A-Memory-Aware-Scheduling-of-Wired-Ahn-Lee/fd179b1e7fc4040ec73aa23ddfc9a61a43c3d600", "https://www.semanticscholar.org/paper/DeepArchitect%3A-Automatically-Designing-and-Training-Negrinho-Gordon/71a80e7342e56f33fd120246e907151a0cf1b4d0", "https://www.semanticscholar.org/paper/Device-Placement-Optimization-with-Reinforcement-Mirhoseini-Pham/bfbd10ebffc9494423770a5bd30ebd0f9cbce66d", "https://www.semanticscholar.org/paper/PyTorch%3A-An-Imperative-Style%2C-High-Performance-Deep-Paszke-Gross/3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "https://www.semanticscholar.org/paper/ReLeQ%3A-A-Reinforcement-Learning-Approach-for-Deep-Elthakeb-Pilligundla/3f311aee9d25b0284d21274cfc8706d6f0277f87", "https://www.semanticscholar.org/paper/Glow%3A-Graph-Lowering-Compiler-Techniques-for-Neural-Rotem-Fix/0261195ae7e1545caefb0ea7afb92bd66bfbb790"]},
{"id": "Self-Supervised-Viewpoint-Learning-From-Image-Mustikovela-Jampani/9cb90d2d2b717410df276cbd4e49774ba9c123e7", "title": "Self-Supervised Viewpoint Learning From Image Collections", "authors": ["Siva Karthik Mustikovela", "Varun Jampani", "Shalini De Mello", "Sifei Liu", "Umar Iqbal", "Carsten Rother", "Jan Kautz"], "date": "2020", "abstract": "Training deep neural networks to estimate the viewpoint of objects requires large labeled training datasets. However, manually labeling viewpoints is notoriously hard, error-prone, and time-consuming. On the other hand, it is relatively easy to mine many unlabelled images of an object category from the internet, e.g., of cars or faces. We seek to answer the research question of whether such unlabeled collections of in-the-wild images can be successfully utilized to train viewpoint estimation networks for general object categories purely via self-supervision. Self-supervision here refers to the fact that the only true supervisory signal that the network has is the input image itself. We propose a novel learning framework which incorporates an analysis-by-synthesis paradigm to reconstruct images in a viewpoint aware manner with a generative network, along with symmetry and adversarial constraints to successfully supervise our viewpoint estimation network. We show that our approach performs competitively to fully-supervised approaches for several object categories like human faces, cars, buses, and trains. Our work opens up further research in self-supervised viewpoint learning and serves as a robust baseline for it. We open-source our code at this https URL.", "references": ["https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Object-Landmarks-through-Jakab-Gupta/073e98d1a443e7b5f8b65903f18cd05a2b884400", "https://www.semanticscholar.org/paper/Unsupervised-Geometry-Aware-Representation-for-3D-Rhodin-Salzmann/eacc28e0f02d8790705a4f5d04a60dfbf36a5c14", "https://www.semanticscholar.org/paper/Crafting-a-multi-task-CNN-for-viewpoint-estimation-Massa-Marlet/192fda5143ea389102c2d3520dddb4356f177f28", "https://www.semanticscholar.org/paper/SilNet-%3A-Single-and-Multi-View-Reconstruction-by-Wiles-Zisserman/abc80f313a5fd80e5f4735af38a0d6cf9a0ce703", "https://www.semanticscholar.org/paper/Self-Supervised-Multi-level-Face-Model-Learning-for-Tewari-Zollh%C3%B6fer/3f7a18a33eecf82630fdebe4c62899eccb359f42", "https://www.semanticscholar.org/paper/Learning-3D-Object-Categories-by-Looking-Around-Novotn%C3%BD-Larlus/ad70861f36428df7fb55135654df8dd98435119d", "https://www.semanticscholar.org/paper/HoloGAN%3A-Unsupervised-Learning-of-3D-From-Natural-Nguyen-Phuoc-Li/8ccb88958358ea59bdc9b76f660b01c8f631b2c0", "https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Object-Landmarks-by-Thewlis-Bilen/73c9cbbf3f9cea1bc7dce98fce429bf0616a1a8c", "https://www.semanticscholar.org/paper/Render-for-CNN%3A-Viewpoint-Estimation-in-Images-CNNs-Su-Qi/6f115fffa3a2af837cf869996e76b805e8f8cea4", "https://www.semanticscholar.org/paper/Canonical-Surface-Mapping-via-Geometric-Cycle-Kulkarni-Gupta/16fe5186db05f8d4a2ddab382f9342cfe92b1dbd"]},
{"id": "Robust-Facial-Landmark-Detection-via-Heatmap-Offset-Zhang-Hu/209b86fffcf2efb0b1e33b35e843b1d48f0acd68", "title": "Robust Facial Landmark Detection via Heatmap-Offset Regression", "authors": ["Junfeng Zhang", "Haifeng Hu", "Shenming Feng"], "date": "2020", "abstract": "Facial landmark detection aims at localizing multiple keypoints for a given facial image, which usually suffers from variations caused by arbitrary pose, diverse facial expression and partial occlusion. In this paper, we develop a two-stage regression network for facial landmark detection on unconstrained conditions. Our model consists of a Structural Hourglass Network (SHN) for detecting the initial locations of all facial landmarks based on heatmap generation, and a Global Constraint Network (GCN) for further refining the detected locations based on offset estimation. Specifically, SHN introduces an improved Inception-ResNet unit as basic building block, which can effectively improve the receptive field and learn contextual feature representations. In the meanwhile, a novel loss function with adaptive weight is proposed to make the whole model focus on the hard landmarks precisely. GCN attempts to explore the spatial contextual relationship between facial landmarks and refine the initial locations of facial landmarks by optimizing the global constraint. Moreover, we develop a pre-processing network to generate features with different scales, which will be transmitted to SHN and GCN for effective feature representations. Different from existing models, the proposed method realizes the heatmap-offset framework, which combines the outputs of heatmaps generated by SHN and coordinates estimated by GCN, to obtain an accurate prediction. The extensive experimental results on several challenging datasets, including 300W, COFW, AFLW, and 300-VW confirm that our method achieve competitive performance compared with the state-of-the-art algorithms.", "references": ["https://www.semanticscholar.org/paper/Robust-Facial-Landmark-Detection-via-Recurrent-Xiao-Feng/0ea96fe9ce2912e96a1d431962b3796576f9fad7", "https://www.semanticscholar.org/paper/A-Deep-Regression-Architecture-with-Two-Stage-for-Lv-Shao/0e7a792ef33af26c26970ffc275d0ae82ee8f5d1", "https://www.semanticscholar.org/paper/Convolutional-Experts-Constrained-Local-Model-for-Zadeh-Lim/88e2efab01e883e037a416c63a03075d66625c26", "https://www.semanticscholar.org/paper/Convolutional-aggregation-of-local-evidence-for-Bulat-Tzimiropoulos/721e5ba3383b05a78ef1dfe85bf38efa7e2d611d", "https://www.semanticscholar.org/paper/Joint-Head-Attribute-Classifier-and-Domain-Specific-Zhang-Hu/8c7ee7acf850562e7515a3e01800c98cc393e62b", "https://www.semanticscholar.org/paper/Deep-Convolutional-Network-Cascade-for-Facial-Point-Sun-Wang/57ebeff9273dea933e2a75c306849baf43081a8c", "https://www.semanticscholar.org/paper/Constrained-Joint-Cascade-Regression-Framework-for-Wu-Ji/ee80b3a7467c422336a72103522d87da591481ac", "https://www.semanticscholar.org/paper/Deep-Structured-Prediction-for-Facial-Landmark-Chen-Su/e6aed11e17a16d1b021d99d60a5868620b41ae55", "https://www.semanticscholar.org/paper/Robust-Facial-Landmark-Detection-via-a-Local-Global-Merget-Rock/489c5a50b58af004b0050bfedf10877199ed7ad5", "https://www.semanticscholar.org/paper/Stacked-Hourglass-Network-for-Robust-Facial-Yang-Liu/6932baa348943507d992aba75402cfe8545a1a9b"]},
{"id": "Incremental-Class-Learning-for-Hierarchical-Park-Kim/3eeac3710f9ad854072eb3bb1635470e22ca3fde", "title": "Incremental Class Learning for Hierarchical Classification", "authors": ["Ju-Youn Park", "Jong-Hwan Kim"], "date": "2020", "abstract": "Objects can be described in hierarchical semantics, and people also perceive them this way. It leads to the need for hierarchical classification in machine learning. On the other hand, when a new data that belongs to a new class is given, the existing classification methods should be retrained for all data including the new data. To deal with these issues, we propose an adaptive resonance theory-supervised predictive mapping for hierarchical classification (ARTMAP-HC) network that allows incremental class learning for raw data without normalization in advance. Our proposed ARTMAP-HC is composed of hierarchically stacked modules, and each module incorporates two fuzzy ARTMAP networks. Regardless of the level of the class hierarchy and the number of classes for each level, ARTMAP-HC is able to incrementally learn sequentially added input data belonging to new classes. By using a novel online normalization process, ARTMAP-HC can classify the new data without prior knowledge of the maximum value of the dataset. By adopting the prior labels appending process, the class dependency between class hierarchy levels is reflected in ARTMAP-HC. The effectiveness of the proposed ARTMAP-HC is validated through experiments on hierarchical classification datasets. To demonstrate the applicability, ARTMAP-HC is applied to a multimedia recommendation system for digital storytelling.", "references": ["https://www.semanticscholar.org/paper/A-Class-Incremental-Classification-Method-Based-on-Sherki-Vala/53943342005e5c33b28dae5476ac7a85b884c487", "https://www.semanticscholar.org/paper/Individualized-AI-Tutor-Based-on-Developmental-Kim-Kim/7db014e7ade9f45f2142b8936a8ee82f1a734109", "https://www.semanticscholar.org/paper/Conceptual-Cognitive-Modeling-for-Fine-Grained-of-Guo-Xu/c9fd562cf0e7be1899451fce1e8183c975006381", "https://www.semanticscholar.org/paper/Hierarchical-multi-label-classification-using-local-Cerri-Barros/ff7eb443d708674a257ebf84f605daff578a9c99", "https://www.semanticscholar.org/paper/Incremental-Algorithms-for-Hierarchical-Cesa-Bianchi-Gentile/4bcf06dc6a371fea69d846c7ccfde4e7ab216408", "https://www.semanticscholar.org/paper/Kernel-Based-Learning-of-Hierarchical-Multilabel-Rousu-Saunders/5cd1c62dc99b6c3ecb2e678aa6fb2bffe3853c28", "https://www.semanticscholar.org/paper/An-evaluation-of-global-model-hierarchical-for-with-Borges-Silla/7730b3292af55d5e624895f6532ac5f60dfdeb94", "https://www.semanticscholar.org/paper/Labelling-strategies-for-hierarchical-multi-label-Triguero-Vens/e1ab9b55e8b85f08913bb01af912d076095fce9a", "https://www.semanticscholar.org/paper/Reduction-strategies-for-hierarchical-multi-label-Cerri-Barros/3f087e754d5b16fb464ff6f792b759f8413a391f", "https://www.semanticscholar.org/paper/A-Novel-Incremental-Class-Learning-Technique-for-Er-Yalavarthi/f15fb69050e022059c74fa3257416bb43888510d", "https://www.semanticscholar.org/paper/Incorporating-label-dependency-into-the-binary-for-Cherman-Metz/d908a75200f6a74ef4c397bf64c6e7d22a6024b9", "https://www.semanticscholar.org/paper/Exploiting-Known-Taxonomies-in-Learning-Overlapping-Cai-Hofmann/2c42e0490354a8f655c03d3eea8127bc7bd1de9a", "https://www.semanticscholar.org/paper/A-Class-Incremental-Learning-Method-for-Multi-Class-Zhang-Su/74bef6ee05315fce32f7964ee7f9cf6e1379ddd7"]},
{"id": "Convolutional-Neural-Networks-Zhang/27b124d8fce9c01f3a930a4fe6779e73e1183e52", "title": "Convolutional Neural Networks", "authors": ["Quan Zhang"], "date": "2018", "abstract": "The earliest Convolution Neural Network (CNN) model is leNet-5 model proposed by LeCun in 1998. However, in the next few years, the development of CNN had been almost stopped until the article \u2018Reducing the dimensionality of data with neural networks\u2019 presented by Hinton in 2006. CNN started entering a period of rapid development. AlexNet won the championship in the image classification contest of ImageNet with the huge superiority of 11% beyond the second place in 2012, and the proposal of DeepFace and DeepID, as two relatively successful models for high-performance face recognition and authentication in 2014, marking the important position of CNN. Convolution Neural Network (CNN) is an efficient recognition algorithm widely used in image recognition and other fields in recent years. That the core features of CNN include local field, shared weights and pooling greatly reducing the parameters, as well as simple structure, make CNN become an academic focus. In this paper, the Convolution Neural Network\u2019s history and structure are summarized. And then several areas of Convolutional Neural Network applications are enumerated. At last, some new insights for the future research of CNN are presented.", "references": ["https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Face-recognition%3A-a-convolutional-neural-network-Lawrence-Giles/86890c82b589e24007c56e1f40c5f928a0e04183", "https://www.semanticscholar.org/paper/Deep-Feature-Learning-for-Knee-Cartilage-Using-a-Prasoon-Petersen/c538d1b01505c8780de2c40a5e0118c88e3b166a", "https://www.semanticscholar.org/paper/Neocognitron%3A-A-hierarchical-neural-network-capable-Fukushima/33c3e56439b11e2d77d99da667ae86afbf6e1ec3", "https://www.semanticscholar.org/paper/AdaBoost-with-SVM-based-component-classifiers-Li-Wang/5e9e028c20a3f4c9383e02ddecf85e0d03b2f174", "https://www.semanticscholar.org/paper/An-introduction-to-the-work-of-David-Hubel-and-Kandel/f7405ae3020f20ebdaef6b263cea1bd88eb4356c", "https://www.semanticscholar.org/paper/Survey-of-convolutional-neural-network-Yan-dong-Zong-bo/de810357ba9bcc7f4b1dac8bbc05e4d9d42a6db9"]},
{"id": "The-Lazy-Encoder%3A-A-Fine-Grained-Analysis-of-the-of-Bisazza-Tump/af96e98684c278b66769b4656af069585ac6df85", "title": "The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation", "authors": ["Arianna Bisazza", "Clara Tump"], "date": "2018", "abstract": "Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.", "references": ["https://www.semanticscholar.org/paper/On-the-Linguistic-Representational-Power-of-Neural-Belinkov-Durrani/2d94a995267903535f78c661a341ee34eabc1ec0", "https://www.semanticscholar.org/paper/On-the-Linguistic-Representational-Power-of-Neural-Belinkov-Durrani/904d305e232f3590189f6108441bdce4584027de", "https://www.semanticscholar.org/paper/The-Bottom-up-Evolution-of-Representations-in-the-A-Voita-Sennrich/112fd54ee193237b24f2ce7fce79e399609a29c5", "https://www.semanticscholar.org/paper/Better-Character-Language-Modeling-Through-Blevins-Zettlemoyer/1d51b59fcf0297e8df931c8b614bd55165b24172", "https://www.semanticscholar.org/paper/Analyzing-Word-Translation-of-Transformer-Layers-Xu-Genabith/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9", "https://www.semanticscholar.org/paper/Analyzing-Multi-Head-Self-Attention%3A-Specialized-Do-Voita-Talbot/07a64686ce8e43ac475a8d820a8a9f1d87989583", "https://www.semanticscholar.org/paper/LINSPECTOR%3A-Multilingual-Probing-Tasks-for-Word-Sahin-Vania/2d35cca5f4db8e32eaeea170f107115f4f416d8a", "https://www.semanticscholar.org/paper/Multilingual-Probing-Tasks-for-Word-Representations-Sahin-Vania/4f6554b71ae29678b010a0e2ae7748818ba68a8f", "https://www.semanticscholar.org/paper/Understanding-Cross-Lingual-Syntactic-Transfer-in-Dhar-Bisazza/70b0dd1c5265ded758854068834a84b4378b5b1a", "https://www.semanticscholar.org/paper/Analysis-Methods-in-Neural-Language-Processing%3A-A-Belinkov-Glass/668f42a4d4094f0a66d402a16087e14269b31a1f", "https://www.semanticscholar.org/paper/What-do-Neural-Machine-Translation-Models-Learn-Belinkov-Durrani/82364428995c29b3dcb60c1835548eeff4adcd20", "https://www.semanticscholar.org/paper/Understanding-and-Improving-Morphological-Learning-Dalvi-Durrani/5da4567918c9d47a85575008edecb78fdf9dd391", "https://www.semanticscholar.org/paper/Linguistic-Input-Features-Improve-Neural-Machine-Sennrich-Haddow/46d0aa6b357c5427f46c7f8ff7053617c4309649", "https://www.semanticscholar.org/paper/Evaluating-Layers-of-Representation-in-Neural-on-Belinkov-Villodre/dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Does-String-Based-Neural-MT-Learn-Source-Syntax-Shi-Padhi/d821ce08da6c0084d5eacbdf65e25556bc1b9bc3", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Exploring-the-Syntactic-Abilities-of-RNNs-with-Enguehard-Goldberg/b64a0511b5e802b4dafef63b81800cf64f359eb1", "https://www.semanticscholar.org/paper/Google%E2%80%99s-Multilingual-Neural-Machine-Translation-Johnson-Schuster/a486e2839291111bb44fa1f07731ada123539f75", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d"]},
{"id": "Feature-extraction-on-faces-%3A-from-landmark-to-Honari/a18c1f139f1aeb42c9139affedd1961ca4bad82d", "title": "Feature extraction on faces : from landmark localization to depth estimation", "authors": ["Sina Honari"], "date": "2019", "abstract": "This thesis focuses on learning algorithms that extract important features from faces. The features of main interest are landmarks; the two dimensional (2D) or three dimensional (3D) locations of important facial features such as eye centers, nose tip, and mouth corners. Landmarks are used to solve complex tasks that cannot be solved directly or require guidance for enhanced performance, such as pose or gesture recognition, tracking, or face verification. The application of the models presented in this thesis is on facial images; however, the algorithms proposed are more general and can be applied to the landmarks of other forms of objects, such as hands, full body or man-made objects. This thesis is written by article and explores different techniques to solve various aspects of landmark localization. In the first article, we disentangle identity and expression of a given face to learn a prior distribution over the joint set of landmarks. This prior is then merged with a discriminative classifier that learns an independent probability distribution per landmark. The merged model is capable of explaining differences in expressions for the same identity representation. In the second article, we propose an architecture that aims at uncovering image features to do tasks that require high pixel-level accuracy, such as landmark localization or image segmentation. The proposed architecture gradually extracts coarser features in its encoding steps to get more global information over the image and then it expands the coarse features back to the image resolution by recombining the features of the encoding path. The model, termed Recombinator Networks, obtained state-of-the-art on several datasets, while also speeding up training. In the third article, we aim at improving landmark localization when only a few images with labelled landmarks are available. In particular, we leverage a weaker form of data labels that are easier to acquire or more abundantly available such as emotion or head pose. To do so, we propose an architecture to backpropagate gradients of the weaker labels through landmarks, effectively training the landmark localization network. We also propose an unsupervised loss component", "references": ["https://www.semanticscholar.org/paper/Pose-Free-Facial-Landmark-Fitting-via-Optimized-and-Yu-Huang/7c5f674ff89df148b9bd39737628f16845da2822", "https://www.semanticscholar.org/paper/300-Faces-in-the-Wild-Challenge%3A-The-First-Facial-Sagonas-Tzimiropoulos/08d4b0960e90b390db7e51b7e6ba4bc40680dfd4", "https://www.semanticscholar.org/paper/Annotated-Facial-Landmarks-in-the-Wild%3A-A-database-K%C3%B6stinger-Wohlhart/b4d2151e29fb12dbe5d164b430273de65103d39b", "https://www.semanticscholar.org/paper/Multi-view-Face-Detection-Using-Deep-Convolutional-Farfade-Saberian/174930cac7174257515a189cd3ecfdd80ee7dd54", "https://www.semanticscholar.org/paper/Robust-Face-Landmark-Estimation-under-Occlusion-Burgos-Artizzu-Perona/2724ba85ec4a66de18da33925e537f3902f21249", "https://www.semanticscholar.org/paper/Nonparametric-Context-Modeling-of-Local-Appearance-Smith-Brandt/6ebd47a22df9631dc63dd3412ed1d0f07b93b3ce", "https://www.semanticscholar.org/paper/Face-detection%2C-pose-estimation%2C-and-landmark-in-Zhu-Ramanan/bb12b81196df90cad4a964bb14edfdb113aeb4ce", "https://www.semanticscholar.org/paper/Deep-Convolutional-Network-Cascade-for-Facial-Point-Sun-Wang/57ebeff9273dea933e2a75c306849baf43081a8c", "https://www.semanticscholar.org/paper/Learning-Deep-Representation-for-Face-Alignment-Zhang-Luo/ac039ca73684241f747645dd72be8f8b03edd191", "https://www.semanticscholar.org/paper/Sieving-Regression-Forest-Votes-for-Facial-Feature-Yang-Patras/26ac607a101492bc86fd81a141311066cfe9e2b5"]},
{"id": "Improved-algorithms-for-exact-and-approximate-Sun-Ye/2611084ebd0205179f723a2710dc3a84dcaf740e", "title": "Improved algorithms for exact and approximate boolean matrix decomposition", "authors": ["Yuan Sun", "Shiwei Ye", "Yi Sun", "Tsunehiko Kameda"], "date": "2015", "abstract": "An arbitrary m\u00d7n Boolean matrix M can be decomposed exactly as M = U\u03bfV, where U (resp. V) is an m\u00d7k (resp. k \u00d7n) Boolean matrix and \u03bf denotes the Boolean matrix multiplication operator. We first prove an exact formula for the Boolean matrix J such that M = M\u03bfJT holds, where J is maximal in the sense that if any 0 element in J is changed to a 1 then this equality no longer holds. Since minimizing k is NP-hard, we propose two heuristic algorithms for finding suboptimal but good decomposition. We measure the performance (in minimizing k) of our algorithms on several real datasets in comparison with other representative heuristic algorithms for Boolean matrix decomposition (BMD). The results on some popular benchmark datasets demonstrate that one of our proposed algorithms performs as well or better on most of them. Our algorithms have a number of other advantages: They are based on exact mathematical formula, which can be interpreted intuitively. They can be used for approximation as well with competitive \u201ccoverage.\u201d Last but not least, they also run very fast. Due to interpretability issues in data mining, we impose the condition, called the \u201ccolumn use condition,\u201d that the columns of the factor matrix U must form a subset of the columns of M. In educational databases, the \u201cideal item response matrix\u201d R, the \u201cknowledge state matrix\u201d A and the \u201cQ-matrix\u201d Q play important roles. We show that they are related exactly by R\u0305 = A \u0305\u03bf QT. Thus, given R, we can find A and Q with a small number (k) of \u201cknowledge states,\u201d using our exact BMD heuristics.", "references": ["https://www.semanticscholar.org/paper/Q-matrix-learning-and-DINA-model-parameter-Sun-Ye/29149677b52a489ca7f60c0abf4d09ec1c9d1d2e", "https://www.semanticscholar.org/paper/A-Multi-Label-Supervised-Topic-Model-Conditioned-on-Liu-Tang/3f97e1f45f80ceeb2c525a99fb4c2cb99b6aa50e", "https://www.semanticscholar.org/paper/A-partially-function-to-topic-model-for-protein-Liu-Tang/356815537ee2bf110f479942f23db1b3f14afeec", "https://www.semanticscholar.org/paper/An-Overview-of-Label-Space-Dimension-Reduction-for-Tang-Liu/7d95ca8f385a3e6977c408fd5b73f12e560a022d", "https://www.semanticscholar.org/paper/Boolean-Matrix-Decomposition-for-Label-Space-and-Liu-Tang/3f215e421b6e8353e086bde98414cf9566cd421e", "https://www.semanticscholar.org/paper/Discovery-of-optimal-factors-in-binary-data-via-a-Belohl%C3%A1vek-Vychodil/3d37e7fb35c78e1c41694106b7d5dd07a6331dd3", "https://www.semanticscholar.org/paper/The-Boolean-column-and-column-row-matrix-Miettinen/0aea23ed519406a55e758bedee0d7d773df261fc", "https://www.semanticscholar.org/paper/Matrix-Decomposition-Methods-for-Data-Mining-%3A-and-Miettinen/0c3344ad4f7a6032a8fdce8ac0bd47694cc2cb97", "https://www.semanticscholar.org/paper/The-Discrete-Basis-Problem-Miettinen-Mielik%C3%A4inen/e720b8b8494b2ea1bc8ecf5841050e0f1fdd84db", "https://www.semanticscholar.org/paper/Approximating-Clique-and-Biclique-Problems-Hochbaum/dc5add7da9a5672b9d72580cb0f3784d4873a8be", "https://www.semanticscholar.org/paper/Relative-Error-CUR-Matrix-Decompositions-Drineas-Mahoney/7ab959ec1915f63bb91dd706f6b177cef0abde84", "https://www.semanticscholar.org/paper/Fast-Monte-Carlo-Algorithms-for-Matrices-III%3A-a-Drineas-Kannan/857e0bdfc1f1115e56a736819ae7bd6a0bb5b1f0", "https://www.semanticscholar.org/paper/Summarizing-transactional-databases-with-overlapped-Xiang-Jin/d90573543136e0596076526c07c6d5d408236b83", "https://www.semanticscholar.org/paper/Interpretable-nonnegative-matrix-decompositions-Hyv%C3%B6nen-Miettinen/a06e1b85b364c28c0b8939cc892ebabc1248e72d", "https://www.semanticscholar.org/paper/Alternating-Recursive-Method-for-Q-matrix-Learning-Sun-Ye/e2b8d77a96e64890690c17202476a873a8f63e0d"]},
{"id": "Parallel-Machine-Translation-with-Disentangled-Kasai-Cross/8b7a8a08f4f339e25c447a144cb5de4ad2e84a7b", "title": "Parallel Machine Translation with Disentangled Context Transformer", "authors": ["Jungo Kasai", "James C Cross", "Marjan Ghazvininejad", "Jiatao Gu"], "date": "2020", "abstract": "State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average.", "references": ["https://www.semanticscholar.org/paper/Deep-Encoder%2C-Shallow-Decoder%3A-Reevaluating-the-in-Kasai-Pappas/8c5a394654822a5de53ac2e4a355c1c6ead4750c", "https://www.semanticscholar.org/paper/Aligned-Cross-Entropy-for-Non-Autoregressive-Ghazvininejad-Karpukhin/3f11a2124af139af7c6f17eccab5149d759d7f52", "https://www.semanticscholar.org/paper/ENGINE%3A-Energy-Based-Inference-Networks-for-Machine-Tu-Pang/f2c0b478a30e653157dcdfe879b3082c6bbb0913", "https://www.semanticscholar.org/paper/Memory-Transformer-Burtsev-Sapunov/f80a7d865227cb9e4c0b0489d97ed4657b6055f2", "https://www.semanticscholar.org/paper/POINTER%3A-Constrained-Text-Generation-via-Generative-Zhang-Wang/3d58eb3c0a8ea3b81509d12ded42c5ad9e2076a7", "https://www.semanticscholar.org/paper/A-Call-for-Clarity-in-Reporting-BLEU-Scores-Post/b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992", "https://www.semanticscholar.org/paper/Mask-Predict%3A-Parallel-Decoding-of-Conditional-Ghazvininejad-Levy/5efadc9019ce3378a0eb6c8f939cdde6c8918b1e", "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Non-Autoregressive-Neural-Machine-Translation-Gu-Bradbury/15e81c8d1c21f9e928c72721ac46d458f3341454", "https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8", "https://www.semanticscholar.org/paper/Convolutional-Sequence-to-Sequence-Learning-Gehring-Auli/43428880d75b3a14257c3ee9bda054e61eb869c0", "https://www.semanticscholar.org/paper/Deterministic-Non-Autoregressive-Neural-Sequence-by-Lee-Mansimov/9c5c89199114858eafbe50b46d77d38ffd03b28a", "https://www.semanticscholar.org/paper/Hint-Based-Training-for-Non-Autoregressive-Machine-Li-Lin/5b446648504afeecf7c73028aa02c2da16db6224", "https://www.semanticscholar.org/paper/Scaling-Neural-Machine-Translation-Ott-Edunov/bf8fe437f779f2098f9af82b534aa51dc9edb06f"]},
{"id": "Neural-Machine-Translation-with-Reordering-Chen-Wang/957f53525b386d6f31077449c8ec23b83c19f506", "title": "Neural Machine Translation with Reordering Embeddings", "authors": ["Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita"], "date": "2019", "abstract": "The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT\u201914 English-toGerman, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system.", "references": ["https://www.semanticscholar.org/paper/Explicit-Reordering-for-Neural-Machine-Translation-Chen-Wang/951747a8d4f516cb011b4f8d19c764f14f155723", "https://www.semanticscholar.org/paper/Recurrent-Positional-Embedding-for-Neural-Machine-Chen-Wang/114745b95c7029c5163b745a37829d1e85fc3083", "https://www.semanticscholar.org/paper/Guiding-Non-Autoregressive-Neural-Machine-Decoding-Ran-Lin/0174a1619b23fd74e6295be4d6231a45c0858f08", "https://www.semanticscholar.org/paper/Self-Attention-with-Cross-Lingual-Position-Ding-Wang/39283c3d6262b24bd61c88038353f3ed0145b6e4", "https://www.semanticscholar.org/paper/Enhancing-Phrase-Based-Statistical-Machine-by-Using-Ahmadnia-Dorr/2c166cb575859879038bd5fc5846fe7ca432e61d", "https://www.semanticscholar.org/paper/Effective-Representation-for-Easy-First-Dependency-Li-Cai/dcedc927e6ff4d88cebc83197b74aff23f2419d6", "https://www.semanticscholar.org/paper/A-Hierarchical-Clustering-Approach-to-Fuzzy-of-Rare-Yang-Liu/66c81a4cd0ba6f2cfce8e1e76ec1d2dd0e389add", "https://www.semanticscholar.org/paper/Improving-Self-Attention-Networks-With-Sequential-Zheng-Huang/86d70b862b37402b42848c544813c7b920fc2e84", "https://www.semanticscholar.org/paper/Incorporating-Word-Reordering-Knowledge-into-Neural-Zhang-Wang/c0451aee5609ace685d72aa3a78e8208f56169cd", "https://www.semanticscholar.org/paper/A-Neural-Reordering-Model-for-Phrase-based-Li-Liu/66ed8e132eb9ef868d4d1efbd8c5d4f1354873c7", "https://www.semanticscholar.org/paper/Pre-Reordering-for-Neural-Machine-Translation%3A-or-Du-Way/be0c2ccbc4b97b10f2123dd5fb9f2992418fae31", "https://www.semanticscholar.org/paper/Coverage-Embedding-Models-for-Neural-Machine-Mi-Sankaran/f997c2f1f668b942c4cccd425bc192df651ed516", "https://www.semanticscholar.org/paper/Maximum-Entropy-Based-Phrase-Reordering-Model-for-Xiong-Liu/7c51326b7efed781adc4c01f1f457a714af3cb79", "https://www.semanticscholar.org/paper/Recursive-Neural-Network-Based-Preordering-for-Kawara-Chu/cc667b01b13cd98825f087289f3502c74961c62f", "https://www.semanticscholar.org/paper/A-Clustered-Global-Phrase-Reordering-Model-for-Nagata-Saito/750f03c4cb56190b66401809999d6fb47c7c44c7", "https://www.semanticscholar.org/paper/A-Comparison-between-Count-and-Neural-Network-Based-Guta-Alkhouli/37f71a27ff3b080a7954249f7fddc60c70e2acb9", "https://www.semanticscholar.org/paper/Sentence-Embedding-for-Neural-Machine-Translation-Wang-Finch/78e592524e336afdf586c51ac228488a71e31340", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Source-Dependency-Chen-Wang/d145755c038599f340f8958e4a27e93ce522b5f9"]},
{"id": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "date": "2015", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "references": ["https://www.semanticscholar.org/paper/Improved-Neural-Machine-Translation-with-SMT-He-He/7768f88efe03b9735f79462c0f89aa04a074f107", "https://www.semanticscholar.org/paper/An-Efficient-Character-Level-Neural-Machine-Zhao-Zhang/6a31ff302d15f5159dc566603f15ac1d774c290b", "https://www.semanticscholar.org/paper/Variational-Neural-Machine-Translation-Zhang-Xiong/ceb154e8f8ac411914f3327d67257776db3aa413", "https://www.semanticscholar.org/paper/Pre-Translation-for-Neural-Machine-Translation-Niehues-Cho/d2f9cab490a506e2574e9d8a15bc20a181ff999b", "https://www.semanticscholar.org/paper/Re-encoding-in-Neural-Machine-Translation-Baptist/6a577fec7944becdd1d38b686048bb8975ad6bde", "https://www.semanticscholar.org/paper/Incorporating-Source-Side-Phrase-Structures-into-Eriguchi-Hashimoto/485e81d23640c814de2cbc81d720efc119c868c0", "https://www.semanticscholar.org/paper/Incorporating-Source-Side-Phrase-Structures-into-Eriguchi-Hashimoto/ff4b449f9ae124958b272cfb29bd47325ec02e5f", "https://www.semanticscholar.org/paper/Language-Independent-Representor-for-Neural-Machine-Zhou-Liu/4ab6ffdf7ee2136a1750e2624fe3e1189705912d", "https://www.semanticscholar.org/paper/Learning-to-Refine-Source-Representations-for-Geng-Wang/9cdd6ecaf25535ebde784cda3447939b6d8a6314", "https://www.semanticscholar.org/paper/Machine-Translation-%3A-From-Statistical-to-modern-Srivastava-Shukla/b2eaeca23e450eb418d555c642f872d7da51bafd", "https://www.semanticscholar.org/paper/On-the-Properties-of-Neural-Machine-Translation%3A-Cho-Merrienboer/1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "https://www.semanticscholar.org/paper/Continuous-Space-Translation-Models-for-Statistical-Schwenk/5f08df805f14baa826dbddcb002277b15d3f1556", "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a", "https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e", "https://www.semanticscholar.org/paper/Overcoming-the-Curse-of-Sentence-Length-for-Neural-Pouget-Abadie-Bahdanau/6122c95ac6475e965bf4e120f7a588d29bb00ecc", "https://www.semanticscholar.org/paper/Continuous-Space-Language-Models-for-Statistical-Schwenk/d4a258df43cc14e46988de9a4a7b2f0ea817529b", "https://www.semanticscholar.org/paper/Sequence-Transduction-with-Recurrent-Neural-Graves/7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Domain-Adaptation-via-Pseudo-In-Domain-Data-Axelrod-He/396aabd694da04cdb846cb724ca9f866f345cbd5"]},
{"id": "Multi-label-classification-using-boolean-matrix-Wicker-Pfahringer/5d7b344dd9abadd798bfe42719e384e6ade46136", "title": "Multi-label classification using boolean matrix decomposition", "authors": ["J\u00f6rg Wicker", "Bernhard Pfahringer", "Stefan Kramer"], "date": "2012", "abstract": "This paper introduces a new multi-label classifier based on Boolean matrix decomposition. Boolean matrix decomposition is used to extract, from the full label matrix, latent labels representing useful Boolean combinations of the original labels. Base level models predict latent labels, which are subsequently transformed into the actual labels by Boolean matrix multiplication with the second matrix from the decomposition. The new method is tested on six publicly available datasets with varying numbers of labels. The experimental evaluation shows that the new method works particularly well on datasets with a large number of labels and strong dependencies among them.", "references": ["https://www.semanticscholar.org/paper/NMF-Based-Label-Space-Factorization-for-Multi-label-Firouzi-Karimian/82b36a0c7e660497ee5c3776a8f4c1b6a449fbc4", "https://www.semanticscholar.org/paper/An-Overview-of-Label-Space-Dimension-Reduction-for-Tang-Liu/7d95ca8f385a3e6977c408fd5b73f12e560a022d", "https://www.semanticscholar.org/paper/Online-Multi-Label-Classification%3A-A-Label-Method-Ahmadi-Kramer/6c1b859cd8e6c018216cd789784935008b490e27", "https://www.semanticscholar.org/paper/Binary-Linear-Compression-for-Multi-label-Zhou-Yu/d8a9f04fe88d11fc15cd12e6cb1a81a281019995", "https://www.semanticscholar.org/paper/A-Label-Embedding-Method-for-Multi-label-via-Local-Wang-Li/bfd05d5b0d07f87407e79774e40534bd3e6ab514", "https://www.semanticscholar.org/paper/Multi-label-extreme-learning-machine-based-on-label-Si-hao-Fu-cai/71f12d1d050ca4621f76882e079398595d4cfe67", "https://www.semanticscholar.org/paper/Online-multi-label-dependency-topic-models-for-text-Burkhardt-Kramer/d617d448521a1f01ca2df9d5b15258b65765adc0", "https://www.semanticscholar.org/paper/Collaborative-Filtering-and-Multi-Label-with-Matrix-Kumar/3d99a3162a467d2ee61d7dd5279268d867ca3374", "https://www.semanticscholar.org/paper/A-Two-Stage-Dual-Space-Reduction-Framework-for-Pacharawongsakda-Theeramunkong/3a4fff4655e981137e96d0c215b34eb7cfe511e3", "https://www.semanticscholar.org/paper/Exploring-Multi-Objective-Optimization-for-Saha-Sarkar/639462ef3a62f8410c6e36b1f25761f3d4961b6c", "https://www.semanticscholar.org/paper/Correlation-Based-Pruning-of-Stacked-Binary-Models-Tsoumakas-Dimou/5e7dc496af5a0210e2d5f99af83351a882408c51", "https://www.semanticscholar.org/paper/Multilabel-Classification-with-Principal-Label-Tai-Lin/8d77fb7b84a2993434c8bf3267046cecbe6bb92c", "https://www.semanticscholar.org/paper/The-Boolean-Column-and-Column-Row-Matrix-Miettinen/23f36ab77a504640a60e75ad77d7664d684ace7a"]},
{"id": "Faithful-Model-Inversion-Substantially-Improves-Webb-Golinski/28177298a99dba915a6e63383da93d3b189c451e", "title": "Faithful Model Inversion Substantially Improves Auto-encoding Variational Inference", "authors": ["Stefan Webb", "Adam Golinski", "Robert Zinkov", "N. Siddharth", "Yee Whye Teh", "Frank D. Wood"], "date": "2017", "abstract": "In learning deep generative models, the encoder for variational inference is typically formed in an ad hoc manner with a structure and parametrization analogous to the forward model. Our chief insight is that this results in coarse approximations to the posterior, and that the d-separation properties of the BN structure of the forward model should be used, in a principled way, to produce ones that are faithful to the posterior, for which we introduce the novel Compact Minimal I-map (CoMI) algorithm. Applying our method to common models reveals that standard encoder design choices lack many important edges, and through experiments we demonstrate that modelling these edges is important for optimal learning. We show how using a faithful encoder is crucial when modelling with continuous relaxations of categorical distributions.", "references": ["https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02", "https://www.semanticscholar.org/paper/Learning-Disentangled-Representations-with-Deep-Narayanaswamy-Paige/097889e0b93591d75de08f9da661ff882a1532f6", "https://www.semanticscholar.org/paper/MADE%3A-Masked-Autoencoder-for-Distribution-Germain-Gregor/90f72fbbe5f0a29e627db28999e01a30a9655bc6", "https://www.semanticscholar.org/paper/Structured-Inference-Networks-for-Nonlinear-State-Krishnan-Shalit/2af17f153e3fd71e15db9216b972aef222f46617", "https://www.semanticscholar.org/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214", "https://www.semanticscholar.org/paper/Inference-Networks-for-Sequential-Monte-Carlo-in-Paige-Wood/b64185358bbe743635129b3462da481606bdbae8", "https://www.semanticscholar.org/paper/Conditional-Image-Generation-with-PixelCNN-Decoders-Oord-Kalchbrenner/0936352b78a52bc5d2b5e3f04233efc56664af51", "https://www.semanticscholar.org/paper/Deep-Temporal-Sigmoid-Belief-Networks-for-Sequence-Gan-Li/0ac2b379fe9b2bf9d5b8129d6117adbb0ab595fb", "https://www.semanticscholar.org/paper/Learning-Stochastic-Inverses-Stuhlm%C3%BCller-Taylor/f887b89684157c2c842010ed63f12bea7787745b", "https://www.semanticscholar.org/paper/Deep-Exponential-Families-Ranganath-Tang/d6559f35be0679c6b3371a2e44e3be293704b600"]},
{"id": "Sampling-Free-Variational-Inference-of-Bayesian-by-Kandemir-Haussmann/89b991bf028b1e5edd27116aa2c5baa63a1eb435", "title": "Sampling-Free Variational Inference of Bayesian Neural Networks by Variance Backpropagation", "authors": ["Melih Kandemir", "Manuel Haussmann", "Fred A. Hamprecht"], "date": "2019", "abstract": "We propose a new Bayesian Neural Net formulation that affords variational inference for which the evidence lower bound is analytically tractable subject to a tight approximation. We achieve this tractability by (i) decomposing ReLU nonlinearities into the product of an identity and a Heaviside step function, (ii) introducing a separate path that decomposes the neural net expectation from its variance. We demonstrate formally that introducing separate latent binary variables to the activations allows representing the neural network likelihood as a chain of linear operations. Performing variational inference on this construction enables a sampling-free computation of the evidence lower bound which is a more effective approximation than the widely applied Monte Carlo sampling and CLT related techniques. We evaluate the model on a range of regression and classification tasks against BNN inference alternatives, showing competitive or improved performance over the current state-of-the-art.", "references": ["https://www.semanticscholar.org/paper/Deep-Active-Learning-with-Adaptive-Acquisition-Hau%C3%9Fmann-Hamprecht/4d62a6e16e6530b6998e7d2702a5b2bebd72907e", "https://www.semanticscholar.org/paper/BYNQNet%3A-Bayesian-Neural-Network-with-Quadratic-for-Awano-Hashimoto/34607b4b40c69ceaf9969d3366ac5cc4d61abfeb", "https://www.semanticscholar.org/paper/Variational-closed-Form-deep-neural-net-inference-Kandemir/d0f8aa044e266d07af4ad0b65eeb6d4983bb2a48", "https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02", "https://www.semanticscholar.org/paper/R%C3%A9nyi-Divergence-Variational-Inference-Li-Turner/803f5fc50765fa2ac1c72659dca02f285fa48d4b", "https://www.semanticscholar.org/paper/Deterministic-Variational-Inference-for-Robust-Wu-Nowozin/b2f9c202a25ad6dc3472a2b2e23b1181225fef3d", "https://www.semanticscholar.org/paper/Variational-Gaussian-Dropout-is-not-Bayesian-Hron-Matthews/65437a516dbcf375b0a2b63ef75e8de8cef4d19b", "https://www.semanticscholar.org/paper/Multiplicative-Normalizing-Flows-for-Variational-Louizos-Welling/b5fa038000a81e55f1160136f401a9cde3be2f71", "https://www.semanticscholar.org/paper/Structured-and-Efficient-Variational-Deep-Learning-Louizos-Welling/63d56435544a48ce508da40e31581920e8608b24", "https://www.semanticscholar.org/paper/Variational-Inference-with-Normalizing-Flows-Rezende-Mohamed/0f899b92b7fb03b609fee887e4b6f3b633eaf30d", "https://www.semanticscholar.org/paper/Stochastic-Backpropagation-and-Approximate-in-Deep-Rezende-Mohamed/484ad17c926292fbe0d5211540832a8c8a8e958b", "https://www.semanticscholar.org/paper/Practical-Variational-Inference-for-Neural-Networks-Graves/5a9ef216bf11f222438fff130c778267d39a9564"]},
{"id": "Multi-Label-Prediction-via-Compressed-Sensing-Hsu-Kakade/cf40878c2de992b6be053f79d3e97d20307dba26", "title": "Multi-Label Prediction via Compressed Sensing", "authors": ["Daniel J. Hsu", "Sham M. Kakade", "John Langford", "Tong Zhang"], "date": "2009", "abstract": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity - that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.", "references": ["https://www.semanticscholar.org/paper/On-the-benefits-of-output-sparsity-for-multi-label-Chzhen-Denis/f9c21c9a4017d5527b959709499ab5bd72ba3cac", "https://www.semanticscholar.org/paper/Large-scale-Multi-label-Learning-with-Missing-Yu-Jain/a9c6fe887d12dab7610836de128f6961bae8f86f", "https://www.semanticscholar.org/paper/Compressed-Sensing-based-Multi-label-Classification-Kai-Jing-zhi/8fe3e6db1c4e12d461c173f8c419c9cfe3d736d6", "https://www.semanticscholar.org/paper/Robust-Extreme-Multi-label-Learning-Xu-Tao/0a66f6e8d3d167e5feca3df990eba1fc7c2ea11d", "https://www.semanticscholar.org/paper/Multi-Label-learning-in-the-independent-label-Barezi-Kwok/835bc343fe790e162f8c49c2c378f1e464799234", "https://www.semanticscholar.org/paper/Multi-label-Subspace-Ensemble-Zhou-Tao/935f0e1572c971b0ff24e7d5af0179804b7643cb", "https://www.semanticscholar.org/paper/Multi-label-Learning-via-Structured-Decomposition-Zhou-Tao/d1aac57446a73a407d38e70c548f230886a1a35f", "https://www.semanticscholar.org/paper/Maximum-Margin-Multi-Label-Structured-Prediction-Lampert/2f95e189c30e4dc0c203229af93ddfce67f8c0ed", "https://www.semanticscholar.org/paper/Learning-label-structure-for-compressed-sensing-Som/e199090f49f9aa27dbe32517a514be4136a6d05b", "https://www.semanticscholar.org/paper/Sparse-Extreme-Multi-label-Learning-with-Oracle-Liu-Shen/3e075ac2ea30fad1588ed76006d737ab4bd6be10", "https://www.semanticscholar.org/paper/Adaptive-Forward-Backward-Greedy-Algorithm-for-with-Zhang/6fc3e647624014ce8340cf28f51ed82bbaa21b6e", "https://www.semanticscholar.org/paper/Max-Margin-Markov-Networks-Taskar-Guestrin/0c450531e1121cfb657be5195e310217a4675397", "https://www.semanticscholar.org/paper/On-the-Complexity-of-Linear-Prediction%3A-Risk-Margin-Kakade-Sridharan/ae6e206c8c2994e04c3fdc5bd97d81fdd0f27493", "https://www.semanticscholar.org/paper/Deterministic-constructions-of-compressed-sensing-DeVore/d0682e1dbdec523567c1167036aa8d46c5a8ab3e", "https://www.semanticscholar.org/paper/Sensitive-Error-Correcting-Output-Codes-Langford-Beygelzimer/e53254f9d08defd6767cd1da7d1472b6edd37910", "https://www.semanticscholar.org/paper/Stable-recovery-of-sparse-overcomplete-in-the-of-Donoho-Elad/7236864d8c2f62defea559465462c43a4b4b6b47", "https://www.semanticscholar.org/paper/Sparse-reconstruction-by-convex-relaxation%3A-Fourier-Rudelson-Vershynin/b70151017955f66aee8cef6880f5f246e4ec39fb", "https://www.semanticscholar.org/paper/Compressed-sensing-Donoho/9fb8c76e6b17f3fdbd0c8f293ce8da4b79f4ffeb", "https://www.semanticscholar.org/paper/Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann/93aa298b40bb3ec23c25239089284fdf61ded917", "https://www.semanticscholar.org/paper/Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri/d221bbcbd20c7157e4500f942de8ceec490f8936"]},
{"id": "Variational-Measure-Preserving-Flows-Zhang-Hern%C3%A1ndez-Lobato/692546fb94ad97bcf9d4bef1393afc4f4309ecbd", "title": "Variational Measure Preserving Flows", "authors": ["Yichuan Zhang", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Zoubin Ghahramani"], "date": "2018", "abstract": "Probabilistic modelling is a general and elegant framework to capture the uncertainty, ambiguity and diversity of hidden structures in data. Probabilistic inference is the key operation on probabilistic models to obtain the distribution over the latent representations given data. Unfortunately, the computation of inference on complex models is extremely challenging. In spite of the success of existing inference methods, like Markov chain Monte Carlo(MCMC) and variational inference(VI), many powerful models are not available for large scale problems because inference is simply computationally intractable. The recent advances in using neural networks for probabilistic inference have shown promising results on this challenge. In this work, we propose a novel general inference framework that has the strength from both MCMC and VI. The proposed method is not only computationally scalable and efficient, but also has its root from the ergodicity theorem, that provides the guarantee of better performance with more computational power. Our experiment results suggest that our method can outperform state-of-the-art methods on generative models and Bayesian neural networks on some popular benchmark problems.", "references": ["https://www.semanticscholar.org/paper/A-Contrastive-Divergence-for-Combining-Variational-Ruiz-Titsias/a15c87c6e6397c4f77350fe3e75ca4ca6fb445e3", "https://www.semanticscholar.org/paper/DEEP-MODELS-CALIBRATION-WITH-BAYESIAN-NEURAL/03f3bde03f83c3ff4f346d761fde4ce031dd4c69", "https://www.semanticscholar.org/paper/NeuTra-lizing-Bad-Geometry-in-Hamiltonian-Monte-Hoffman-Sountsov/2884e6017cb1605d08aca74a33477a5505f9a087", "https://www.semanticscholar.org/paper/Calibration-of-Deep-Probabilistic-Models-with-Maro%C3%B1as-Paredes/10197e1afe07254233e3f4dee4a8706d46562e92", "https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02", "https://www.semanticscholar.org/paper/Variational-Gaussian-Process-Tran-Ranganath/ed032736652ac7e1f36ea17bd253cd1bfdcc3864", "https://www.semanticscholar.org/paper/Variational-Inference-with-Normalizing-Flows-Rezende-Mohamed/0f899b92b7fb03b609fee887e4b6f3b633eaf30d", "https://www.semanticscholar.org/paper/Hamiltonian-Variational-Auto-Encoder-Caterini-Doucet/b6a0aa4ed34a034fd5bfb08c2c9b209cf69cf354", "https://www.semanticscholar.org/paper/Learning-Deep-Latent-Gaussian-Models-with-Markov-Hoffman/780991e9d94dc7b032d1cee66e84edca02259d0f", "https://www.semanticscholar.org/paper/Markov-Chain-Monte-Carlo-and-Variational-Inference%3A-Salimans-Kingma/7198597c62f6b6433d9ed6d5b44f887bd05d3c56", "https://www.semanticscholar.org/paper/Stochastic-Backpropagation-and-Approximate-in-Deep-Rezende-Mohamed/484ad17c926292fbe0d5211540832a8c8a8e958b", "https://www.semanticscholar.org/paper/Ladder-Variational-Autoencoders-S%C3%B8nderby-Raiko/64d698ecd01eab99e81e586400e86d3d70b9cba7", "https://www.semanticscholar.org/paper/Importance-Weighted-Autoencoders-Burda-Grosse/3e47c4c2dd98c49b7771c7228812d5fd9eee56a3", "https://www.semanticscholar.org/paper/Tighter-Variational-Bounds-are-Not-Necessarily-Rainforth-Kosiorek/720ef8fabb9b7cf099a20dda7a22e5cca752b14f"]},
{"id": "A-Novel-Incremental-Class-Learning-Technique-for-Er-Yalavarthi/f15fb69050e022059c74fa3257416bb43888510d", "title": "A Novel Incremental Class Learning Technique for Multi-class Classification", "authors": ["Meng Joo Er", "Vijaya Krishna Yalavarthi", "Ning Wang", "Rajasekar Venkatesan"], "date": "2016", "abstract": "In this paper, a novel technique for multi-class classification, which is independent of the number of class constraints and can learn the new classes it encounters, is developed. The developed technique enables remodelling of the network to adapt to the dynamic nature of non-stationary input samples. It not only can learn the new classes, but also the new patterns created in the input. The proposed algorithm is evaluated using various benchmark datasets and a comparative study of classification performance shows that the proposed algorithm is superior.", "references": ["https://www.semanticscholar.org/paper/Incremental-Class-Learning-for-Hierarchical-Park-Kim/3eeac3710f9ad854072eb3bb1635470e22ca3fde", "https://www.semanticscholar.org/paper/Online-Incremental-Classification-Resonance-Network-Park-Kim/69a1f46a12d1acfd1f05236b378390f3df5bbd4f"]},
{"id": "Stacked-Hourglass-Network-for-Robust-Facial-Yang-Liu/6932baa348943507d992aba75402cfe8545a1a9b", "title": "Stacked Hourglass Network for Robust Facial Landmark Localisation", "authors": ["Jing Yang", "Qingshan Liu", "Kaihua Zhang"], "date": "2017", "abstract": "With the increasing number of public available training data for face alignment, the regression-based methods attracted much attention and have become the dominant methods to solve this problem. There are two main factors, the variance of the regression target and the capacity of the regression model, affecting the performance of the regression task. In this paper, we present a Stacked Hourglass Network for robust facial landmark localisation. We first adopt a supervised face transformation to remove the translation, scale and rotation variation of each face, in order to reduce the variance of the regression target. Then we employ a deep convolutional neural network named Stacked Hourglass Network to increase the capacity of the regression model. To better evaluate the proposed method, we reimplement two popular cascade shape regression models, SDM and LBF, for comparison. Extensive experiments on four challenging datasets prove the effectiveness of the proposed method.", "references": ["https://www.semanticscholar.org/paper/Stacked-Hourglass-Network-Joint-with-Salient-Region-Zhang-Hu/5420ba17afbf550a9ee7eec933321ad7817cb70d", "https://www.semanticscholar.org/paper/Improved-Stacked-Hourglass-Network-with-Offset-for-Shi-Wang/ece14cb628125f0bc0970a32e1737fcf9535d2a6", "https://www.semanticscholar.org/paper/Joint-Stacked-Hourglass-Network-and-Salient-Region-Zhang-Hu/a49e8da790b1c5f393fb7e41f4fc5d488cf1c9ca", "https://www.semanticscholar.org/paper/Facial-Landmark-Localization-Based-on-Auto-Stacked-Hong-Guo/9ec380808062f93233dad06e9b6ea3cdfa498a13", "https://www.semanticscholar.org/paper/Face-Alignment-by-Combining-Residual-Features-in-Chen-Zhou/70c3f7ade95e5f79adc02ca6171b4194ec347427", "https://www.semanticscholar.org/paper/2D-Wasserstein-Loss-for-Robust-Facial-Landmark-Yan-Duffner/063b268b3dd6aebcadbf736a326adb782e11feb7", "https://www.semanticscholar.org/paper/Dynamic-Cascaded-Regression-Network-with-Learning-Zhang-Zhuang/a863a8c5654570e60f1667797373810ecebcb7cf", "https://www.semanticscholar.org/paper/Deformable-hourglass-network-with-face-region-for-Cheng-Wang/7001632e71159e51bc84e83768348f1125e31547", "https://www.semanticscholar.org/paper/A-Deeply-Initialized-Coarse-to-fine-Ensemble-of-for-Valle-Buenaposada/0eb45876359473156c0d4309f548da63470d30ee", "https://www.semanticscholar.org/paper/Wing-Loss-for-Robust-Facial-Landmark-Localisation-Feng-Kittler/822346af4fafcc54d7d95da2cbbb0bbe189405b4", "https://www.semanticscholar.org/paper/Dual-Sparse-Constrained-Cascade-Regression-for-Face-Liu-Deng/ebc2a3e8a510c625353637e8e8f07bd34410228f", "https://www.semanticscholar.org/paper/Supervised-Transformer-Network-for-Efficient-Face-Chen-Hua/1eff0e56f8acace1b28a532730119f7e900f2ec4", "https://www.semanticscholar.org/paper/UMDFaces%3A-An-annotated-face-dataset-for-training-Bansal-Nanduri/ca45746d158e9d58bdb8a62b6d10163a23cf5b6f", "https://www.semanticscholar.org/paper/Facial-Landmark-Detection-by-Deep-Multi-task-Zhang-Luo/f500b1a7df00f67c417673e0538d86abb8a333fa", "https://www.semanticscholar.org/paper/Convolutional-aggregation-of-local-evidence-for-Bulat-Tzimiropoulos/721e5ba3383b05a78ef1dfe85bf38efa7e2d611d", "https://www.semanticscholar.org/paper/The-Menpo-Facial-Landmark-Localisation-Challenge%3A-A-Zafeiriou-Trigeorgis/59d8fa6fd91cdb72cd0fa74c04016d79ef5a752b", "https://www.semanticscholar.org/paper/Deep-Convolutional-Network-Cascade-for-Facial-Point-Sun-Wang/57ebeff9273dea933e2a75c306849baf43081a8c", "https://www.semanticscholar.org/paper/M3-CSR%3A-Multi-view%2C-multi-scale-and-multi-component-Deng-Liu/482a8c826b8521642d4009cd515c600953c677ee", "https://www.semanticscholar.org/paper/Face-Alignment-Assisted-by-Head-Pose-Estimation-Yang-Mou/d6bea9c5001383b52e9971446c6f4838d69669c1", "https://www.semanticscholar.org/paper/Mnemonic-Descent-Method%3A-A-Recurrent-Process-for-Trigeorgis-Snape/aa99593486a1649f520f7c3a78d424de2c4d04e6"]},
{"id": "Sieving-Regression-Forest-Votes-for-Facial-Feature-Yang-Patras/26ac607a101492bc86fd81a141311066cfe9e2b5", "title": "Sieving Regression Forest Votes for Facial Feature Detection in the Wild", "authors": ["Heng Yang", "Ioannis Patras"], "date": "2013", "abstract": "In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts on-the-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on 'difficult' face images.", "references": ["https://www.semanticscholar.org/paper/Fine-Tuning-Regression-Forests-Votes-for-Object-in-Yang-Patras/2b832186a9d204b65d0c1fadcffc9c157d4a2434", "https://www.semanticscholar.org/paper/Privileged-Information-Based-Conditional-Structured-Yang-Patras/73a70595af6500505b46de91afbe108ce2fc2aee", "https://www.semanticscholar.org/paper/Structured-Semi-supervised-Forest-for-Facial-with-Jia-Yang/6f0c5572cad918d72228e90b7720cade3ec0baa0", "https://www.semanticscholar.org/paper/Robust-Face-Alignment-Under-Occlusion-via-Regional-Yang-He/6c52a454eeee58125c12e7d7f1fbde9b44dd1d2c", "https://www.semanticscholar.org/paper/Consensus-of-Regression-for-Occlusion-Robust-Facial-Yu-Lin/3b470b76045745c0ef5321e0f1e0e6a4b1821339", "https://www.semanticscholar.org/paper/Feature-extraction-on-faces-%3A-from-landmark-to-Honari/a18c1f139f1aeb42c9139affedd1961ca4bad82d", "https://www.semanticscholar.org/paper/Human-and-sheep-facial-landmarks-localisation-by-Yang-Zhang/c0f6a104afe19123e657f3227cd8bae6f9a2396d", "https://www.semanticscholar.org/paper/Cascade-of-forests-for-face-alignment-Yang-Zou/86c053c162c08bc3fe093cc10398b9e64367a100", "https://www.semanticscholar.org/paper/Face-Sketch-Landmarks-Localization-in-the-Wild-Yang-Zou/4b6387e608afa83ac8d855de2c9b0ae3b86f31cc", "https://www.semanticscholar.org/paper/Facial-Features-Detection-and-Localization-Hassaballah-Bekhet/36abb583bc108d3a4a3fb3b46ee2992ff788adef", "https://www.semanticscholar.org/paper/Privileged-information-based-conditional-regression-Yang-Patras/d511e903a882658c9f6f930d6dd183007f508eda", "https://www.semanticscholar.org/paper/Robust-and-Accurate-Shape-Model-Matching-Using-Lindner-Bromiley/f7646fa0061e4418a51d9c608280c6d752f36c4a", "https://www.semanticscholar.org/paper/Real-time-facial-feature-detection-using-regression-Dantone-Gall/5dc056fe911a3e34a932513abe637076250d96da", "https://www.semanticscholar.org/paper/Facial-point-detection-using-boosted-regression-and-Valstar-Mart%C3%ADnez/7e6d4876d4b25b0aa31af2c3437fb3f640e16694", "https://www.semanticscholar.org/paper/Facial-landmark-detection-in-uncontrolled-Efraty-Huang/48853c25dc75481b0c77f408a8a76383287ebe2a", "https://www.semanticscholar.org/paper/Annotated-Facial-Landmarks-in-the-Wild%3A-A-database-K%C3%B6stinger-Wohlhart/b4d2151e29fb12dbe5d164b430273de65103d39b", "https://www.semanticscholar.org/paper/Face-detection%2C-pose-estimation%2C-and-landmark-in-Zhu-Ramanan/bb12b81196df90cad4a964bb14edfdb113aeb4ce", "https://www.semanticscholar.org/paper/Face-Parts-Localization-Using-Structured-Output-Yang-Patras/1a140d9265df8cf50a3cd69074db7e20dc060d14", "https://www.semanticscholar.org/paper/Face-Alignment-by-Explicit-Shape-Regression-Cao-Wei/489fc2a88f0257b577c3fecb5e4a59451c97dbfe", "https://www.semanticscholar.org/paper/Coupled-Gaussian-processes-for-pose-invariant-Rudovic-Pantic/281d1a5ad7a1e7f6168f48678d6980a0349d6f74"]},
{"id": "Canonical-Surface-Mapping-via-Geometric-Cycle-Kulkarni-Gupta/16fe5186db05f8d4a2ddab382f9342cfe92b1dbd", "title": "Canonical Surface Mapping via Geometric Cycle Consistency", "authors": ["Nilesh V. Kulkarni", "Abhinav Gupta", "Shubham Tulsiani"], "date": "2019", "abstract": "We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.", "references": ["https://www.semanticscholar.org/paper/Articulation-aware-Canonical-Surface-Mapping-Kulkarni-Gupta/2b37e8fee6c827b066107bb4bba020b265c0e00f", "https://www.semanticscholar.org/paper/Autolabeling-3D-Objects-with-Differentiable-of-SDF-Zakharov-Kehl/21480a47587a478a1e60d4883125fb855df87213", "https://www.semanticscholar.org/paper/Self-supervised-Single-view-3D-Reconstruction-via-Li-Liu/6719ef619fd20787bb6993213bff15ef952ab96f", "https://www.semanticscholar.org/paper/Semantic-Correspondence-via-2D-3D-2D-Cycle-You-Li/70461096d9c7894d8cdd89afa553de129a6ae86c", "https://www.semanticscholar.org/paper/Self-Supervised-Viewpoint-Learning-From-Image-Mustikovela-Jampani/9cb90d2d2b717410df276cbd4e49774ba9c123e7", "https://www.semanticscholar.org/paper/Learning-to-Transfer-Texture-from-Clothing-Images-Mir-Alldieck/30ebd7d48a2039ddf867bf853b2780f843bbb84f", "https://www.semanticscholar.org/paper/Neural-Mesh-Refiner-for-6-DoF-Pose-Estimation-Wu-Chen/a03d0095fca7d87ba76354ef015aa8cddbc3d9ab", "https://www.semanticscholar.org/paper/Tackling-Two-Challenges-of-6D-Object-Pose-Lack-of-Sock-Castro/c2e9440d34ab2a0fe68cfce232a560782709796f", "https://www.semanticscholar.org/paper/Inter-and-Intra-Image-Relationships-in-3-D-Space-Kulkarni/f6b410aebf5f06ce3bcff245a9f2227fe675bc80", "https://www.semanticscholar.org/paper/Fine-grained-Object-Semantic-Understanding-from-You-Li/78aa7181803a222697637795839ae7419ce000f6", "https://www.semanticscholar.org/paper/Learning-Dense-Correspondence-via-3D-Guided-Cycle-Zhou-Kr%C3%A4henb%C3%BChl/442c36baa9c00c135d5d40aab967d84dbad9686a", "https://www.semanticscholar.org/paper/Learning-Category-Specific-Mesh-Reconstruction-from-Kanazawa-Tulsiani/1dbe6021ddb511b70cfc30373ed9cd699af06bdd", "https://www.semanticscholar.org/paper/3DMatch%3A-Learning-Local-Geometric-Descriptors-from-Zeng-Song/5b589d7564dbef07ec98b3248f58a481b4ca1395", "https://www.semanticscholar.org/paper/Universal-Correspondence-Network-Choy-Gwak/6b581c1ee47e215456a2908309606486d7a559cc", "https://www.semanticscholar.org/paper/WarpNet%3A-Weakly-Supervised-Matching-for-Single-View-Kanazawa-Jacobs/eff5860709f54b8842274962983a7f4fca00a63e", "https://www.semanticscholar.org/paper/Unsupervised-learning-of-object-frames-by-dense-Thewlis-Bilen/4e169c4de1d2c99297494d289decf475376b4457", "https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Architecture-for-Rocco-Arandjelovic/b6db6a0b1e7522fd9449bec480b4f918bea52e91", "https://www.semanticscholar.org/paper/End-to-End-Weakly-Supervised-Semantic-Alignment-Rocco-Arandjelovic/5bc2624a73da20133996c9d416080517367f645f", "https://www.semanticscholar.org/paper/Unsupervised-Geometry-Aware-Representation-for-3D-Rhodin-Salzmann/eacc28e0f02d8790705a4f5d04a60dfbf36a5c14", "https://www.semanticscholar.org/paper/Metric-Regression-Forests-for-Correspondence-Pons-Moll-Taylor/cdd43392404c4ba3e0ea26899f24ec993893b8df"]},
{"id": "Learning-Deep-Representation-for-Face-Alignment-Zhang-Luo/ac039ca73684241f747645dd72be8f8b03edd191", "title": "Learning Deep Representation for Face Alignment with Auxiliary Attributes", "authors": ["Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "date": "2016", "abstract": "In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.", "references": ["https://www.semanticscholar.org/paper/Fast-multi-view-face-alignment-via-multi-task-Li-Sun/055cd8173536031e189628c879a2acad6cf2a5d0", "https://www.semanticscholar.org/paper/Tree-gated-Deep-Regressor-Ensemble-For-Face-In-The-Arnaud-Dapogny/7f8282a3b5bb2dfd6057168ecab4e49d2de5e8cb", "https://www.semanticscholar.org/paper/Learning-Deep-Sharable-and-Structural-Detectors-for-Liu-Lu/df80fed59ffdf751a20af317f265848fe6bfb9c9", "https://www.semanticscholar.org/paper/Deep-Multi-Center-Learning-for-Face-Alignment-Shao-Zhu/2d294bde112b892068636f3a48300b3c033d98da", "https://www.semanticscholar.org/paper/General-to-specific-learning-for-facial-attribute-Sun-Yu/42a48ce71daa2d05c698d2e9e90335320ddd9841", "https://www.semanticscholar.org/paper/HyperFace%3A-A-Deep-Multi-Task-Learning-Framework-for-Ranjan-Patel/04eed24e26d9e6aaf2ca434cad20facd5feb83d0", "https://www.semanticscholar.org/paper/Multi-Task-Convolutional-Neural-Network-for-Face-Yin-Liu/79673a7e28237338792aff809d2ba04f92f9da1b", "https://www.semanticscholar.org/paper/Robust-Facial-Alignment-with-Internal-Denoising-Aspandi-Mart%C3%ADnez/5e857799d0359fefb0543e324cb246ccfa6a6cdb", "https://www.semanticscholar.org/paper/Joint-Multi-View-Face-Alignment-in-the-Wild-Deng-Trigeorgis/34fe617eb4881289eb1d33c4aedc905e9c3f22b9", "https://www.semanticscholar.org/paper/Facial-Landmark-Detection-via-Attention-Adaptive-Sadiq-Shi/487c53ef5ac78ccd41cf1bef190a3ef0201239ac", "https://www.semanticscholar.org/paper/Facial-Landmark-Detection-by-Deep-Multi-task-Zhang-Luo/f500b1a7df00f67c417673e0538d86abb8a333fa", "https://www.semanticscholar.org/paper/Face-detection%2C-pose-estimation%2C-and-landmark-in-Zhu-Ramanan/bb12b81196df90cad4a964bb14edfdb113aeb4ce", "https://www.semanticscholar.org/paper/Coupling-Alignments-with-Recognition-for-Face-Huang-Zhao/acba9d87aa561689f761dd4b47219418e51ac419", "https://www.semanticscholar.org/paper/Joint-Cascade-Face-Detection-and-Alignment-Chen-Ren/f68b3031e7092072bd7b38c05448031f17b087d1", "https://www.semanticscholar.org/paper/Surpassing-Human-Level-Face-Verification-on-LFW-Lu-Tang/c3a4630f7e8dd741f1b0c5a020d9eab9472eed79", "https://www.semanticscholar.org/paper/Coarse-to-Fine-Auto-Encoder-Networks-(CFAN)-for-Zhang-Shan/581f5c2e00aaa19942355ef99ecd46bcff55be08", "https://www.semanticscholar.org/paper/Using-a-Deformation-Field-Model-for-Localizing-and-Pedersoli-Timofte/dbd18bc955ed92ae6b68e691ab6b551cf842e078", "https://www.semanticscholar.org/paper/A-Deep-Sum-Product-Architecture-for-Robust-Facial-Luo-Wang/5c10a0d79a5e2dd2d7079dffe98450e6ff49cb3b", "https://www.semanticscholar.org/paper/Face-Alignment-by-Explicit-Shape-Regression-Cao-Wei/489fc2a88f0257b577c3fecb5e4a59451c97dbfe", "https://www.semanticscholar.org/paper/Pose-Free-Facial-Landmark-Fitting-via-Optimized-and-Yu-Huang/7c5f674ff89df148b9bd39737628f16845da2822"]},
{"id": "Approximating-Clique-and-Biclique-Problems-Hochbaum/dc5add7da9a5672b9d72580cb0f3784d4873a8be", "title": "Approximating Clique and Biclique Problems", "authors": ["Dorit S. Hochbaum"], "date": "1998", "abstract": "We present here 2-approximation algorithms for several node deletion and edge deletion biclique problems and for an edge deletion clique problem. The biclique problem is to find a node induced subgraph that is bipartite and complete. The objective is to minimize the total weight of nodes or edges deleted so that the remaining subgraph is bipartite complete. Several variants of the biclique problem are studied here, where the problem is defined on bipartite graph or on general graphs with or without the requirement that each side of the bipartition forms an independent set. The maximum clique problem is formulated as maximizing the number (or weight) of edges in the complete subgraph. A 2-approximation algorithm is given for the minimum edge deletion version of this problem. The approximation algorithms given here are derived as a special case of an approximation technique devised for a class of formulations introduced by Hochbaum. All approximation algorithms described (and the polynomial algorithms for two versions of the node biclique problem) involve calls to a minimum cut algorithm. One conclusion of our analysis of the NP-hard problems here is that all of these problems are MAX SNP-hard and at least as difficult to approximate as the vertex cover problem. Another conclusion is that the problem of finding the minimum node cut-set, the removal of which leaves two cliques in the graph, is NP-hard and 2-approximable.", "references": ["https://www.semanticscholar.org/paper/On-Bipartite-and-Multipartite-Clique-Problems-Dawande-Keskinocak/4392d950120075769d68bfedc6d5476425870c54", "https://www.semanticscholar.org/paper/Clustering-and-the-Biclique-Partition-Problem-Bein-Morales/824f452c6f5cecd0601e0819fa71911d8612d6b2", "https://www.semanticscholar.org/paper/Scale-Reduction-Techniques-for-Computing-Maximum-Shahinpour-Shirvani/218f6c2ff73ee5fb611db2908c3a8928801eaa62", "https://www.semanticscholar.org/paper/Exact-exponential-time-algorithms-for-finding-Binkele-Raible-Fernau/c93ff55adca5654980bf885f5a61c6440ffd90a7", "https://www.semanticscholar.org/paper/Maximum-Weighted-Edge-Biclique-Problem-on-Bipartite-Pandey-Sharma/043adf377a2b359bee1816f94cf902cdcab8863b", "https://www.semanticscholar.org/paper/Dense-subgraph-problems-with-output-density-Suzuki-Tokuyama/c6e663a61f45c8aa9fa819acfea4fbd61c46e7c9", "https://www.semanticscholar.org/paper/Structural-properties-of-biclique-graphs-and-the-Groshaus-Montero/9475cbce06f35052d6b607df7ad805805a3dd174", "https://www.semanticscholar.org/paper/On-the-generation-of-bicliques-of-a-graph-Dias-Figueiredo/fe394132f81b33274d69be9a464268b9d0cf3e57", "https://www.semanticscholar.org/paper/Submodular-problems-approximations-and-algorithms-Hochbaum/332577694063f587937c4a18c16101731b3e49b9", "https://www.semanticscholar.org/paper/An-Exact-Algorithm-for-3-Biclique-Vertex-Partition-Liu-Zhu/3fc88b219c0a079cd0474b292c0aad9859a34949", "https://www.semanticscholar.org/paper/Edge-Deletion-Problems-Yannakakis/b59a2ae98053a756b08b67815c3a5afa2d5ac7f3"]},
{"id": "A-Neural-Reordering-Model-for-Phrase-based-Li-Liu/66ed8e132eb9ef868d4d1efbd8c5d4f1354873c7", "title": "A Neural Reordering Model for Phrase-based Translation", "authors": ["Peng Li", "Yang Liu", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang"], "date": "2014", "abstract": "While lexicalized reordering models have been widely used in phrase-based translation systems, they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a neural reordering model that conditions reordering probabilities on the words of both the current and previous phrase pairs. Including the words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we build one classifier for all phrase pairs, which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models.", "references": ["https://www.semanticscholar.org/paper/Neural-Reordering-Model-Considering-Phrase-and-Word-Kanouchi-Sudoh/83f1b1f344737c4dbc3b0ec4f966d4b7d283a672", "https://www.semanticscholar.org/paper/Learning-Better-Classification-Based-Reordering-for-Fu-xue-Tong/34a5e5318a05a9e52ff49f9b564cf6e1f3e2c7f7", "https://www.semanticscholar.org/paper/Learning-Word-Reorderings-for-Hierarchical-Machine-Zhang-Utiyama/edee9d15b32bc7e01b6bf71951825c3849062519", "https://www.semanticscholar.org/paper/A-Dependency-Based-Neural-Reordering-Model-for-Hadiwinoto-Ng/2b611b9550a88059b903210f924d474947556210", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-with-Reordering-Chen-Wang/957f53525b386d6f31077449c8ec23b83c19f506", "https://www.semanticscholar.org/paper/Learning-local-word-reorderings-for-hierarchical-Zhang-Utiyama/d26254cf3ec537f37708afaaf7f5a76a7922d4a2", "https://www.semanticscholar.org/paper/Neural-Reordering-Feature-for-Statistical-Machine-Cui-Wang/41dbaba567f31111f24331d073f0cc3ef8818e96", "https://www.semanticscholar.org/paper/LSTM-Neural-Reordering-Feature-for-Statistical-Cui-Wang/7f0232b9ffbc637cdddaed2520098cddc066e5d6", "https://www.semanticscholar.org/paper/Continuous-Space-Reordering-Models-for-Phrase-based-Durrani-Dalvi/d5e8ca6d80978257cbd0effc5ec99d70542e5a49", "https://www.semanticscholar.org/paper/Which-Words-Matter-in-Defining-Phrase-Reorderings-Ghader-Monz/1f3ed055c2573dbf3a29ddc17f1ed0a6251c7769", "https://www.semanticscholar.org/paper/Maximum-Entropy-Based-Phrase-Reordering-Model-for-Xiong-Liu/7c51326b7efed781adc4c01f1f457a714af3cb79", "https://www.semanticscholar.org/paper/Modified-Distortion-Matrices-for-Phrase-Based-Bisazza-Federico/7116c2f6e42bd4c406bf68451a913b3bb02e90cc", "https://www.semanticscholar.org/paper/Improved-Reordering-for-Phrase-Based-Translation-Cherry/ab2fb799e0fea005f77c8e958b9b5d8b6a4282f7", "https://www.semanticscholar.org/paper/Can-Markov-Models-Over-Minimal-Translation-Units-Durrani-Fraser/76232f8fa3377c7382220a196470d11cb30fb45c", "https://www.semanticscholar.org/paper/A-Simple-and-Effective-Hierarchical-Phrase-Model-Galley-Manning/efe0cf9a64f0e580a530cc16b79cfac5f4e61aca", "https://www.semanticscholar.org/paper/A-Joint-Sequence-Translation-Model-with-Integrated-Durrani-Schmid/019e706ff091a69bdbe348e782fe5ea2998d93b9", "https://www.semanticscholar.org/paper/Recursive-Autoencoders-for-ITG-Based-Translation-Li-Liu/30cb96f8e6a2727ddfa8b7ee874672c092ee632e", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Source-reordering-using-MaxEnt-classifiers-and-Khalilov-Sima'an/a0a8aaed60bf2f64dfc604dc78dfee9608fcdfef", "https://www.semanticscholar.org/paper/Reordering-Constraints-for-Phrase-Based-Statistical-Zens-Ney/baa4de120a631c719928c2466681e322c9da7848"]},
{"id": "Robust-Facial-Landmark-Detection-via-a-Local-Global-Merget-Rock/489c5a50b58af004b0050bfedf10877199ed7ad5", "title": "Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network", "authors": ["Daniel Merget", "Matthias Rock", "Gerhard Rigoll"], "date": "2018", "abstract": "While fully-convolutional neural networks are very strong at modeling local features, they fail to aggregate global context due to their constrained receptive field. Modern methods typically address the lack of global context by introducing cascades, pooling, or by fitting a statistical model. In this work, we propose a new approach that introduces global context into a fully-convolutional neural network directly. The key concept is an implicit kernel convolution within the network. The kernel convolution blurs the output of a local-context subnet, which is then refined by a global-context subnet using dilated convolutions. The kernel convolution is crucial for the convergence of the network because it smoothens the gradients and reduces overfitting. In a postprocessing step, a simple PCA-based 2D shape model is fitted to the network output in order to filter outliers. Our experiments demonstrate the effectiveness of our approach, outperforming several state-of-the-art methods in facial landmark detection.", "references": ["https://www.semanticscholar.org/paper/Dilated-Skip-Convolution-for-Facial-Landmark-Chim-Lee/a5a921c77bae13f9c0b661fe3041fbb68e4b6ef8", "https://www.semanticscholar.org/paper/Robust-Facial-Landmark-Detection-via-Heatmap-Offset-Zhang-Hu/209b86fffcf2efb0b1e33b35e843b1d48f0acd68", "https://www.semanticscholar.org/paper/Region-Based-Context-Enhanced-Network-for-Robust-Lin-Liang/3a43a0f8e507ba7977073a75a954f0366d19331e", "https://www.semanticscholar.org/paper/Facial-Landmark-Machines%3A-A-Backbone-Branches-With-Liu-Li/d7c4e1a31f7997ee3bf4560d764523b7e999ae02", "https://www.semanticscholar.org/paper/Pixel-In-Pixel-Net%3A-Towards-Efficient-Facial-in-the-Jin-Liao/6344fbc54002b42a2cc44f08c669ce26b518528c", "https://www.semanticscholar.org/paper/2D-Wasserstein-Loss-for-Robust-Facial-Landmark-Yan-Duffner/063b268b3dd6aebcadbf736a326adb782e11feb7", "https://www.semanticscholar.org/paper/Stacked-Multi-Target-Network-for-Robust-Facial-Yang-Yu/918996bac4632a1673e69e2907b145dabc20dda4", "https://www.semanticscholar.org/paper/Outlier-Robust-Neural-Aggregation-Network-for-Video-H%C3%B6rmann-Knoche/8239255ebe5a346c8fc7732f32c1fa7e88b62050", "https://www.semanticscholar.org/paper/Learning-Robust-Facial-Landmark-Detection-via-Zou-Zhong/7ab47c9090b448e2ba75bb162fa1dd9ca778c0dd", "https://www.semanticscholar.org/paper/FAB%3A-A-Robust-Facial-Landmark-Detection-Framework-Sun-Wu/98adcf3649cc3a04ea8ec881b7a38f131bef6b36", "https://www.semanticscholar.org/paper/Improving-Facial-Landmark-Detection-via-a-Inception-Knoche-Merget/e6178de1ef15a6a973aad2791ce5fbabc2cb8ae5", "https://www.semanticscholar.org/paper/Deep-Convolutional-Network-Cascade-for-Facial-Point-Sun-Wang/57ebeff9273dea933e2a75c306849baf43081a8c", "https://www.semanticscholar.org/paper/Convolutional-Experts-Constrained-Local-Model-for-Zadeh-Lim/88e2efab01e883e037a416c63a03075d66625c26", "https://www.semanticscholar.org/paper/Coarse-to-Fine-Auto-Encoder-Networks-(CFAN)-for-Zhang-Shan/581f5c2e00aaa19942355ef99ecd46bcff55be08", "https://www.semanticscholar.org/paper/DenseReg%3A-Fully-Convolutional-Dense-Shape-G%C3%BCler-Trigeorgis/3d5bab25ff8e2c47a07851977635ae8c290d6e14", "https://www.semanticscholar.org/paper/Human-Pose-Estimation-via-Convolutional-Part-Bulat-Tzimiropoulos/985fc76c7fb578222169ce90bef6a829d8322b7b", "https://www.semanticscholar.org/paper/Robust-FEC-CNN%3A-A-High-Accuracy-Facial-Landmark-He-Zhang/7fcfd72ba6bc14bbb90b31fe14c2c77a8b220ab2", "https://www.semanticscholar.org/paper/DeepFace%3A-Closing-the-Gap-to-Human-Level-in-Face-Taigman-Yang/9f2efadf66817f1b38f58b3f50c7c8f34c69d89a", "https://www.semanticscholar.org/paper/A-Deep-Convolutional-Neural-Network-for-Background-Babaee-Dinh/fdbca6bc9c4a1c76b4442571b4c0a53b465d9f7c", "https://www.semanticscholar.org/paper/Facial-Landmark-Detection-by-Deep-Multi-task-Zhang-Luo/f500b1a7df00f67c417673e0538d86abb8a333fa"]}
]
