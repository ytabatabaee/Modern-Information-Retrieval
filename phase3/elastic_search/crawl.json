[
{"type": "paper", "id": "Coordinated-actor-model-of-self-adaptive-traffic-Bagheri-Sirjani/45ee43eb193409c96107c5aa76e8668a62312ee8", "title": "Coordinated actor model of self-adaptive track-based traffic control systems", "authors": ["Maryam Bagheri", "Marjan Sirjani", "Ehsan Khamespanah", "Narges Khakpour", "Ilge Akkaya", "Ali Movaghar-Rahimabadi", "Edward A. Lee"], "date": "2018", "abstract": "Abstract Self-adaptation is a well-known technique to handle growing complexities of software systems, where a system autonomously adapts itself in response to changes in a dynamic and unpredictable environment. With the increasing need for developing self-adaptive systems, providing a model and an implementation platform to facilitate integration of adaptation mechanisms into the systems and assuring their safety and quality is crucial. In this paper, we target Track-based Traffic Control Systems (TTCSs) in which the traffic flows through pre-specified sub-tracks and is coordinated by a traffic controller. We introduce a coordinated actor model to design self-adaptive TTCSs and provide a general mapping between various TTCSs and the coordinated actor model. The coordinated actor model is extended to build large-scale self-adaptive TTCSs in a decentralized setting. We also discuss the benefits of using Ptolemy II as a framework for model-based development of large-scale self-adaptive systems that supports designing multiple hierarchical MAPE-K feedback loops interacting with each other. We propose a template based on the coordinated actor model to design a self-adaptive TTCS in Ptolemy II that can be instantiated for various TTCSs. We enhance the proposed template with a predictive adaptation feature. We illustrate applicability of the coordinated actor model and consequently the proposed template by designing two real-life case studies in the domains of air traffic control systems and railway traffic control systems in Ptolemy II.", "references": []},
{"type": "paper", "id": "Domino-Temporal-Data-Prefetcher-Bakhshalipour-Lotfi-Kamran/665c0dde22c2f8598869d690d59c9b6d84b07c01", "title": "Domino Temporal Data Prefetcher", "authors": ["Mohammad Bakhshalipour", "Pejman Lotfi-Kamran", "Hamid Sarbazi-Azad"], "date": "2018", "abstract": "Big-data server applications frequently encounter data misses, and hence, lose significant performance potential. One way to reduce the number of data misses or their effect is data prefetching. As data accesses have high temporal correlations, temporal prefetching techniques are promising for them. While state-of-the-art temporal prefetching techniques are effective at reducing the number of data misses, we observe that there is a significant gap between what they offer and the opportunity. This work aims to improve the effectiveness of temporal prefetching techniques. We identify the lookup mechanism of existing temporal prefetchers responsible for the large gap between what they offer and the opportunity. Existing lookup mechanisms either not choose the right stream in the history, or unnecessarily delay the stream selection, and hence, miss the opportunity at the beginning of every stream. In this work, we introduce Domino prefetching to address the limitations of existing temporal prefetchers. Domino prefetcher is a temporal data prefetching technique that logically looks up the history with both one and two last miss addresses to find a match for prefetching. We propose a practical design for Domino prefetcher that employs an Enhanced Index Table that is indexed by just a single miss address. We show that Domino prefetcher captures more than 90% of the temporal opportunity. Through detailed evaluation targeting a quad-core processor and a set of server workloads, we show that Domino prefetcher improves system performance by 16% over the baseline with no data prefetcher and 6% over the state-of- the-art temporal data prefetcher.", "references": []},
{"type": "paper", "id": "Fair-Allocation-of-Indivisible-Goods%3A-Improvements-Ghodsi-Hajiaghayi/80b1abef1ab1317e7ca23d532cd8a165f7284916", "title": "Fair Allocation of Indivisible Goods: Improvements and Generalizations", "authors": ["Mohammad Ghodsi", "Mohammad Taghi Hajiaghayi", "Masoud Seddighin", "Saeed Seddighin", "Hadi Yami"], "date": "2018", "abstract": "We study the problem of fair allocation for indivisible goods. We use the maxmin share paradigm introduced by Budish~\\citeBudish:first as a measure for fairness. \\procacciafirst ~\\citeProcaccia:first were the first to investigate this fundamental problem in the additive setting. They show that a maxmin guarantee (1-$\\MMS$ allocation) is not always possible even when the number of agents is limited to 3. While the existence of an approximation solution (e.g. a $1/2$-$\\MMS$ allocation) is quite straightforward, improving the guarantee becomes subtler for larger constants. \\sprocacciafirst ~\\citeProcaccia:first provide a proof for the existence of a $2/3$-$\\MMS$ allocation and leave the question open for better guarantees. Our main contribution is an answer to the above question. We improve the result of \\sprocacciafirst~to a $3/4$ factor in the additive setting. The main idea for our $3/4$-$\\MMS$ allocation method is clustering the agents. To this end, we introduce three notions and techniques, namely reducibility, matching allocation, and cycle-envy-freeness, and prove the approximation guarantee of our algorithm via non-trivial applications of these techniques. Our analysis involves coloring and double counting arguments that might be of independent interest. One major shortcoming of the current studies on fair allocation is the additivity assumption on the valuations. We alleviate this by extending our results to the case of submodular, fractionally subadditive, and subadditive settings. More precisely, we give constant approximation guarantees for submodular and XOS agents, and a logarithmic approximation for the case of subadditive agents. Furthermore, we complement our results by providing close upper bounds for each class of valuation functions. Finally, we present algorithms to find such allocations for additive, submodular, and XOS settings in polynomial time. The reader can find a summary of our results in Table \\refresultstable.", "references": []},
{"type": "paper", "id": "Restoring-highly-corrupted-images-by-impulse-noise-Taherkhani-Jamzad/9e86f7c13fdc859fd7e7713dc9b72ec3a1bc9a73", "title": "Restoring highly corrupted images by impulse noise using radial basis functions interpolation", "authors": ["Fariborz Taherkhani", "Mansour Jamzad"], "date": "2018", "abstract": "Preserving details in restoring images highly corrupted by impulse noise remains a challenging problem. We proposed an algorithm based on radial basis functions (RBF) interpolation which estimates the intensities of corrupted pixels by their neighbors. In this algorithm, first intensity values of noisy pixels in the corrupted image are estimated using RBFs. Next, the image is smoothed. The proposed algorithm can effectively remove the highly dense impulse noise. Experimental results show the superiority of the proposed algorithm in comparison to the recent similar methods both in noise suppression and detail preservation. Extensive simulations show better results in measure of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), especially when the image is corrupted by very highly dense impulse noise.", "references": []},
{"type": "paper", "id": "Deep-Private-Feature-Extraction-Ossia-Taheri/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da", "title": "Deep Private-Feature Extraction", "authors": ["Seyed Ali Ossia", "Ali Taheri", "Ali Shahin Shamsabadi", "Kleomenis Katevas", "Hamed Haddadi", "Hamid R. Rabiee"], "date": "2020", "abstract": "We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.", "references": []}
]
