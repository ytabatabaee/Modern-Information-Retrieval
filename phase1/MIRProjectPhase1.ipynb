{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "def prepare_text(raw_text, stem_flag=True):\n",
    "    prepared_text = raw_text\n",
    "    #print(prepared_text)\n",
    "    # 1. Remove Punctuation marks & non-persian characters\n",
    "    prepared_text = re.sub('[^۰-۹ آ-ی \\u200c]', ' ', prepared_text)\n",
    "    #print(prepared_text)\n",
    "    # 2. Normalization\n",
    "    normalizer = Normalizer()\n",
    "    prepared_text = normalizer.normalize(prepared_text) \n",
    "    #print(prepared_text)\n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(prepared_text) \n",
    "    #print(tokens)\n",
    "    # 4. Stemming \n",
    "    if stem_flag:\n",
    "        stemmer = Stemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " <div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "doc_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_next_doc(corpus):\n",
    "    content = ''\n",
    "    for line in corpus:\n",
    "        content += line\n",
    "        if '</page>' in line:\n",
    "            break\n",
    "    try:\n",
    "        id = re.search('<id>(.*?)</id>', content, re.DOTALL).group(1)\n",
    "        title = re.search('<title>(.*?)</title>', content, re.DOTALL).group(1) \n",
    "        text = re.search('<text(.*?)</text>', content, re.DOTALL).group(1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    doc_dict = {'id': id, 'title': title, 'text': text}\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zone_to_index(doc_dict, zone):\n",
    "    doc_id = doc_dict.get('id')\n",
    "    prepared_zone = prepare_text(doc_dict.get(zone), True)\n",
    "    for pos, term in enumerate(prepared_zone):\n",
    "        if term in index:\n",
    "            try:\n",
    "                index[term][doc_id][zone].append(pos)\n",
    "            except:\n",
    "                try:\n",
    "                    index[term][doc_id][zone] = [pos]\n",
    "                except:\n",
    "                    index[term][doc_id] = {}\n",
    "                    index[term][doc_id][zone] = [pos]    \n",
    "                 \n",
    "        else:\n",
    "            index[term] = {}\n",
    "            index[term][doc_id] = {}\n",
    "            index[term][doc_id][zone] = [pos]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is build completely!\n"
     ]
    }
   ],
   "source": [
    "def construct_positional_indexes(docs_path):\n",
    "    global index\n",
    "    corpus = open(docs_path, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(corpus)\n",
    "    while doc_dict:\n",
    "        doc_ids.append(doc_dict.get('id'))\n",
    "        add_zone_to_index(doc_dict, 'title')                \n",
    "        add_zone_to_index(doc_dict, 'text')\n",
    "        doc_dict = parse_next_doc(corpus)\n",
    "    print(\"Index is build completely!\")    \n",
    "        \n",
    "construct_positional_indexes('data/Persian.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [],
   "source": [
    "def get_posting_list(word):\n",
    "    try:\n",
    "        posting_list = index[word]\n",
    "    except:\n",
    "        posting_list = []\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_index = {}\n",
    "bigram_dict = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Index is build completely!\n"
     ]
    }
   ],
   "source": [
    "def construct_bigram_index(docs_path):\n",
    "    global bigram_index\n",
    "    corpus = open(docs_path, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(corpus)\n",
    "    while doc_dict:\n",
    "        doc_id = doc_dict['id']\n",
    "        words = prepare_text(doc_dict['title'], stem_flag=False) + prepare_text(doc_dict['text'], stem_flag=False)\n",
    "        for word in words:\n",
    "            bigram_dict.add(word)\n",
    "            marked_word = '$' + word + '$'\n",
    "            bigrams = [marked_word[i:i + 2] for i in range(0, len(marked_word) - 1)]\n",
    "            for bigram in bigrams:\n",
    "                try:\n",
    "                    bigram_index[bigram].add(word)\n",
    "                except:\n",
    "                    bigram_index[bigram] = set()\n",
    "                    bigram_index[bigram].add(word)\n",
    "        doc_dict = parse_next_doc(corpus)\n",
    "    print(\"Bigram Index is build completely!\") \n",
    "\n",
    "construct_bigram_index('data/Persian.xml')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['میثرا',\n",
       " 'متکثر',\n",
       " 'موثرند',\n",
       " 'دراکثرشهرهای',\n",
       " 'موثرتر',\n",
       " 'ثروت',\n",
       " 'براثر',\n",
       " 'اکثر',\n",
       " 'منثره',\n",
       " 'ثروتمندترین',\n",
       " 'کوثری',\n",
       " 'مؤثرند',\n",
       " 'اثراتی',\n",
       " 'حداکثر',\n",
       " 'متاثر',\n",
       " 'کوثر',\n",
       " 'پرثروت',\n",
       " 'ثروتمندان',\n",
       " 'حدکثر',\n",
       " 'نثر',\n",
       " 'مؤثر',\n",
       " 'اکثرا',\n",
       " 'ناموثر',\n",
       " 'تأثر',\n",
       " 'هیثرو',\n",
       " 'ثروتمند',\n",
       " 'موثری',\n",
       " 'خشثریه',\n",
       " 'گوثری',\n",
       " 'اثرگذارترین',\n",
       " 'ثریایی',\n",
       " 'ثریا',\n",
       " 'اکثریت',\n",
       " 'مؤثرترین',\n",
       " 'ثروت\\u200cها',\n",
       " 'حداکثری',\n",
       " 'اکثرنفاط',\n",
       " 'اثرگذار',\n",
       " 'مؤثر\\u200cتر',\n",
       " 'ثروتهای',\n",
       " 'أکثرهم',\n",
       " 'دراکثر',\n",
       " 'متأثر',\n",
       " 'اثرهای',\n",
       " 'ثریتونه',\n",
       " 'ثروت\\u200cهای',\n",
       " 'ورثرغنه',\n",
       " 'اثر',\n",
       " 'کثرة',\n",
       " 'ثروتمندترین\\u200cها',\n",
       " 'ثروتی',\n",
       " 'اثرش',\n",
       " 'اثر\\u200cها',\n",
       " 'میثره',\n",
       " 'مفقودالاثرهای',\n",
       " 'تکثر',\n",
       " 'نثرنویسی',\n",
       " 'مؤثری',\n",
       " 'اثرها',\n",
       " 'یثرب',\n",
       " 'اثربخشی',\n",
       " 'اکثرأ',\n",
       " 'موثرتری',\n",
       " 'مؤثرتر',\n",
       " 'نثری',\n",
       " 'میثر',\n",
       " 'تأثری',\n",
       " 'اثری',\n",
       " 'موثرترین',\n",
       " 'اکثریتی',\n",
       " 'دراثر',\n",
       " 'اثربخش',\n",
       " 'کثرت',\n",
       " 'تأثرش',\n",
       " 'اثرهایی',\n",
       " 'اثرگذاری',\n",
       " 'اثرات',\n",
       " 'تکثرگرایی',\n",
       " 'اثره']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    try:\n",
    "        return list(bigram_index[bigram])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "get_words_with_bigram('ثر')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49432\n",
      "{'3415': {'text': [1363, 1373]}, '4058': {'text': [869]}, '4659': {'text': [783]}, '5237': {'text': [2381]}, '6054': {'text': [1455]}, '7013': {'text': [397, 1548]}}\n"
     ]
    }
   ],
   "source": [
    "print(len(index))\n",
    "print(index['نیوتون'])\n",
    "#print(len(index['نیوتون']))\n",
    "#print(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    file_name = docs_path + '/' + str(doc_num) + '.xml'\n",
    "    global index\n",
    "    doc = open(file_name, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(doc)\n",
    "        \n",
    "    if doc_dict:\n",
    "        if doc_dict.get('id') in doc_ids:\n",
    "            return\n",
    "        add_zone_to_index(doc_dict, 'title')                \n",
    "        add_zone_to_index(doc_dict, 'text')\n",
    "        doc_ids.append(doc_dict.get('id'))\n",
    "\n",
    "add_document_to_indexes('data/wiki', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zone_from_index(doc_dict, zone):\n",
    "    doc_id = doc_dict.get('id')\n",
    "    prepared_zone = prepare_text(doc_dict.get(zone), True)\n",
    "    for term in prepared_zone:\n",
    "            try:\n",
    "                del index[term][doc_id]\n",
    "                if not bool(index[term]):\n",
    "                    del index[term]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    file_name = docs_path + '/' + str(doc_num) + '.xml'\n",
    "    global index\n",
    "    doc = open(file_name, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(doc)\n",
    "    \n",
    "    if doc_dict:\n",
    "        if doc_dict.get('id') not in doc_ids:\n",
    "            return\n",
    "        remove_zone_from_index(doc_dict, 'title')\n",
    "        remove_zone_from_index(doc_dict, 'text')\n",
    "        doc_ids.remove(doc_dict.get('id'))\n",
    "\n",
    "delete_document_from_indexes('data/wiki', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    file_name = destination + '/index.json'\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(index, json_file)\n",
    "\n",
    "save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    if not os.path.exists(source):\n",
    "        return \"No such directory\"\n",
    "    file_name = source + '/index.json'\n",
    "    global index\n",
    "    with open(file_name) as json_file:\n",
    "        index = json.load(json_file)\n",
    "\n",
    "load_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(token_set1, token_set2):\n",
    "    intersection = [b for b in token_set1 if b in token_set2]\n",
    "    union = [b for b in token_set1 or b in token_set2]\n",
    "    return  len(intersection) / len(union) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['شلام', 'حالا', 'برسهان', 'درسک', 'شد']\n",
      "['غلام', 'حالا', 'برهان', 'دراک', 'شد']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'سلام حالا پرسمان درست شد.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_query(query):\n",
    "    query_terms = prepare_text(query, stem_flag = False)\n",
    "    print(query_terms)\n",
    "    for term in query_terms:\n",
    "        if term not in bigram_dict:\n",
    "            marked_term = '$' + term + '$'\n",
    "            term_bigrams = [marked_term[i:i + 2] for i in range(0, len(marked_term) - 1)]\n",
    "            # print(term_bigrams)\n",
    "            candidate_set = []\n",
    "\n",
    "            for b in term_bigrams:\n",
    "                candidate_set.extend(list(bigram_index[b]))\n",
    "            candidate_set = list(set(candidate_set))\n",
    "            \n",
    "            # filtering based on jaccard coefficient\n",
    "            jaccard_candidates = []\n",
    "            threshold = 0.5\n",
    "            for j in range(len(candidate_set)):\n",
    "                marked_c = '$' + candidate_set[j] + '$'\n",
    "                candidate_bigrams = [marked_c[i:i + 2] for i in range(0, len(marked_c) - 1)]\n",
    "                if jaccard_coeff(candidate_bigrams, term_bigrams) > threshold:\n",
    "                      jaccard_candidates.append(candidate_set[j]) \n",
    "                        \n",
    "            # filtering based on edit distance  \n",
    "            edit_distances = [editdistance.eval(jaccard_candidates[j], term) for j in range(len(jaccard_candidates))]\n",
    "            final_candidates = [jaccard_candidates[j] for j in range(len(jaccard_candidates)) if edit_distances[j] == min(edit_distances)]\n",
    "            idx = query_terms.index(term)\n",
    "            query_terms[idx] = random.choice(final_candidates)\n",
    "            \n",
    "            \n",
    "    print(query_terms)\n",
    "    correct_query = \"سلام حالا پرسمان درست شد.\"\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6824']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrasal_search(phrase, zones = ['title', 'text']):\n",
    "    phrase_terms = prepare_text(phrase)\n",
    "    if len(phrase_terms) == 1:\n",
    "        posting_list = get_posting_list(phrase)\n",
    "        if len(posting_list) > 0:\n",
    "            return list(posting_list.keys())\n",
    "        return []\n",
    "        \n",
    "    common_docs = get_posting_list(phrase_terms[0]).keys()\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for term in phrase_terms:\n",
    "        posting_list = index[term]\n",
    "        common_docs = list(set(common_docs) & set(index[term].keys()))\n",
    "    \n",
    "    for zone in zones:  \n",
    "        for doc in common_docs:\n",
    "            try:\n",
    "                for pos in index[phrase_terms[0]][doc][zone]:\n",
    "                    flag = True\n",
    "                    for i in range(1, len(phrase_terms)):\n",
    "                        if not (pos +  i) in index[phrase_terms[i]][doc][zone]:\n",
    "                            flag = False\n",
    "                            break\n",
    "                    if flag and doc not in relevant_docs:\n",
    "                        relevant_docs.append(doc)\n",
    "                        break           \n",
    "            except:\n",
    "                pass\n",
    "    return relevant_docs        \n",
    "            \n",
    "phrasal_search('نظرخواهی انجام شده توسط دانشگاه') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building static tf-idf matrixes\n",
    "\n",
    "def build_df_tf():\n",
    "    tf = {}\n",
    "    df = {}\n",
    "    for term, posting in index.items():\n",
    "        tf[term] = {}\n",
    "        df[term] = {}\n",
    "        df[term]['title'] = 0\n",
    "        df[term]['text'] = 0\n",
    "        \n",
    "        for doc in posting:\n",
    "            tf[term][doc] = {}\n",
    "            try:\n",
    "                tf[term][doc]['title'] = len(index[term][doc]['title'])\n",
    "                df[term]['title'] += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:    \n",
    "                tf[term][doc]['text'] = len(index[term][doc]['text'])\n",
    "                df[term]['text'] += 1\n",
    "            except:\n",
    "                pass\n",
    "    return tf, df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "global tf, df\n",
    "tf, df = build_df_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3415': {'text': [1363, 1373]}, '4058': {'text': [869]}, '4659': {'text': [783]}, '5237': {'text': [2381]}, '6054': {'text': [1455]}, '7013': {'text': [397, 1548]}}\n",
      "6\n",
      "{'text': 2}\n",
      "{'3415': {'text': 2}, '4058': {'text': 1}, '4659': {'text': 1}, '5237': {'text': 1}, '6054': {'text': 1}, '7013': {'text': 2}}\n",
      "{'title': 0, 'text': 6}\n"
     ]
    }
   ],
   "source": [
    "word = 'نیوتون'\n",
    "print(index[word])\n",
    "print(len(index[word]))\n",
    "print(tf[word]['3415'])\n",
    "print(tf[word])\n",
    "print(df[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3197,\n",
       " 3039,\n",
       " 5509,\n",
       " 3099,\n",
       " 3747,\n",
       " 6694,\n",
       " 6749,\n",
       " 4321,\n",
       " 5293,\n",
       " 6915,\n",
       " 6824,\n",
       " 5508,\n",
       " 4838,\n",
       " 3905,\n",
       " 6772]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query, method=\"ltn-lnn\", weight=2, max_retrieved=15):\n",
    "    relevant_docs = [] \n",
    "    scores = []\n",
    "    phrases = re.findall(r'\"(.*?)\"', query)\n",
    "    non_phrase = re.sub(r'\"(.*?)\"', ' ', query)\n",
    "    query_terms = prepare_text(query)\n",
    "    non_phrase_terms = prepare_text(non_phrase)\n",
    "    \n",
    "    for phrase in phrases + non_phrase_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase))\n",
    "            \n",
    "    relevant_docs = list(set(relevant_docs))\n",
    "    scores = [0] * len(relevant_docs)\n",
    "        \n",
    "    w_q = [0] * len(query_terms)\n",
    "    \n",
    "    # compute query weight vector\n",
    "    for i in range(len(query_terms)):\n",
    "        term = query_terms[i]\n",
    "        w_q[i] = math.log10(query_terms.count(term)) + 1\n",
    "\n",
    "    # cosine normalization\n",
    "    if method[2] == 'c': \n",
    "        w_q = w_q / np.linalg.norm(w_q)\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        w_d = [0] * len(query_terms)\n",
    "        for i in range(len(query_terms)):\n",
    "            term = query_terms[i]\n",
    "            w_td_title, w_td_text = 0, 0\n",
    "            n = len(doc_ids)\n",
    "            try:\n",
    "                w_td_title = (math.log10(tf[term][doc]['title']) + 1) * math.log10(n / df[term]['title'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                w_td_text = (math.log10(tf[term][doc]['text']) + 1) * math.log10(n / df[term]['text'])\n",
    "            except:\n",
    "                pass\n",
    "            w_d[i] = w_td_title * 2 + w_td_text\n",
    "            \n",
    "        doc_idx = relevant_docs.index(doc)\n",
    "        # cosine normalization\n",
    "        if method[2] == 'c':\n",
    "            w_d = w_d / np.linalg.norm(w_d)\n",
    "        scores[doc_idx] = np.dot(w_d, w_q)\n",
    "    \n",
    "    # sort docs according to score\n",
    "    relevant_docs = [int(x) for _,x in sorted(zip(scores, relevant_docs), reverse=True)]\n",
    "    return relevant_docs[:min(len(relevant_docs), max_retrieved)]\n",
    "\n",
    "search('\"نظرخواهی انجام شده توسط دانشگاه\" شهر نیویورک', \"ltc-lnc\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['عجایب', 'هف', 'گانه', 'چشمگیر', 'بنا', 'تاریخ', 'جه']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5967,\n",
       " 3260,\n",
       " 6949,\n",
       " 5192,\n",
       " 4530,\n",
       " 6969,\n",
       " 5508,\n",
       " 6752,\n",
       " 3654,\n",
       " 3666,\n",
       " 5293,\n",
       " 7143,\n",
       " 5236,\n",
       " 3938,\n",
       " 3874]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\", max_retrieved=15):\n",
    "    relevant_docs = []\n",
    "    scores = []\n",
    "    title_phrases = re.findall(r'\"(.*?)\"', title_query)\n",
    "    text_phrases = re.findall(r'\"(.*?)\"', text_query)\n",
    "    title_non_phrase = re.sub(r'\"(.*?)\"', ' ', title_query)\n",
    "    text_non_phrase = re.sub(r'\"(.*?)\"', ' ', text_query)\n",
    "    non_phrase_title_terms = prepare_text(title_non_phrase)\n",
    "    non_phrase_text_terms = prepare_text(text_non_phrase)\n",
    "    \n",
    "    for phrase in title_phrases + non_phrase_title_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase, zones=['title']))\n",
    "        \n",
    "    for phrase in text_phrases + non_phrase_text_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase, zones=['text']))\n",
    "        \n",
    "    query_terms = prepare_text(title_query) + prepare_text(text_query)\n",
    "    print(query_terms)\n",
    "            \n",
    "    relevant_docs = list(set(relevant_docs))\n",
    "    scores = [0] * len(relevant_docs)\n",
    "    w_q = [0] * len(query_terms)\n",
    "    \n",
    "    # compute query weight vector\n",
    "    for i in range(len(query_terms)):\n",
    "        term = query_terms[i]\n",
    "        w_q[i] = math.log10(query_terms.count(term)) + 1\n",
    "\n",
    "    # cosine normalization\n",
    "    if method[2] == 'c': \n",
    "        w_q = w_q / np.linalg.norm(w_q)\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        w_d = [0] * len(query_terms)\n",
    "        for i in range(len(query_terms)):\n",
    "            term = query_terms[i]\n",
    "            w_td_title, w_td_text = 0, 0\n",
    "            n = len(doc_ids)\n",
    "            try:\n",
    "                w_td_title = (math.log10(tf[term][doc]['title']) + 1) * math.log10(n / df[term]['title'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                w_td_text = (math.log10(tf[term][doc]['text']) + 1) * math.log10(n / df[term]['text'])\n",
    "            except:\n",
    "                pass\n",
    "            w_d[i] = w_td_title * 2 + w_td_text\n",
    "            \n",
    "        doc_idx = relevant_docs.index(doc)\n",
    "        # cosine normalization\n",
    "        if method[2] == 'c':\n",
    "            w_d = w_d / np.linalg.norm(w_d)\n",
    "        scores[doc_idx] = np.dot(w_d, w_q)\n",
    "                  \n",
    "  \n",
    "    # sort docs according to score\n",
    "    relevant_docs = [int(x) for _,x in sorted(zip(scores, relevant_docs), reverse=True)]\n",
    "    return relevant_docs[:min(len(relevant_docs), max_retrieved)]\n",
    "    \n",
    "    relevant_docs = []\n",
    "    \n",
    "    print(title_query)\n",
    "    print(text_query)\n",
    "    return relevant_docs\n",
    "\n",
    "detailed_search('عجایب \"هفت‌گانه\"', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevance = [], []\n",
    "        for file in glob.glob('data/queries/*.txt'):\n",
    "            query = []\n",
    "            with open(file, encoding = 'utf8') as query_file:\n",
    "                for line in query_file:\n",
    "                    query.append(line)   \n",
    "            queries.append(query)  \n",
    "        for file in glob.glob('data/relevance/*.txt'):\n",
    "            with open(file, encoding = 'utf8') as relevance_file:\n",
    "                relevance.append([int(x) for x in relevance_file.read().split(',')]) \n",
    "        return queries, relevance   \n",
    "    else:\n",
    "        query = []\n",
    "        with open('data/queries/%s.txt'%(query_id,), encoding = 'utf8') as query_file:\n",
    "            for line in query_file:\n",
    "                query.append(line)\n",
    "        with open('data/relevance/%s.txt'%(query_id,)) as relevance_file:   \n",
    "            relevance = [int(x) for x in relevance_file.read().split(',')]\n",
    "        return query, relevance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_docs(query, max_retrieved=15):\n",
    "    if len(query) == 1:\n",
    "        return search(query[0], \"ltc-lnc\", max_retrieved=max_retrieved)\n",
    "    elif len(query) == 2:\n",
    "        return detailed_search(query[0], query[1], \"ltn-lnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f_measure(retrieved, relevant):\n",
    "    alpha = 0.5\n",
    "    p = len(list(set(retrieved) & set(relevant))) / len(retrieved)\n",
    "    r = len(list(set(retrieved) & set(relevant))) / len(relevant)\n",
    "    beta2 = (1 - alpha) / alpha\n",
    "    f_measure = (beta2 + 1) * p * r / (beta2 * p + r)\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_precision(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1   \n",
    "    p = np.divide(np.cumsum(p) * p, np.arange(1, len(retrieved) + 1))\n",
    "    p = p[p > 0]\n",
    "    return np.mean(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1 \n",
    "    ideal = np.array([1] * int(np.sum(p)) + [0] * (len(p) - int(np.sum(p))))\n",
    "    dcg_rf = p[0] + sum(np.divide(p[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    dcg_gt = ideal[0] + sum(np.divide(ideal[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    ndcg = dcg_rf / dcg_gt\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61710172153679\n",
      "0.5591997699531904\n",
      "0.739371233574426\n",
      "0.8374986182024546\n"
     ]
    }
   ],
   "source": [
    "def R_Precision(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        r_precision = []\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        for i in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[i], max_retrieved=len(relevants[i]))\n",
    "            r_precision.append(len(list(set(retrieved) & set(relevants[i]))) / len(relevants[i]))\n",
    "        r_precision = np.mean(r_precision)\n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query, max_retrieved=len(relevant))\n",
    "        r_precision = len(list(set(retrieved) & set(relevant))) / len(relevant)\n",
    "\n",
    "    return r_precision\n",
    "\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        f_measures = []\n",
    "        for i in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[i])\n",
    "            f_measures.append(compute_f_measure(retrieved, relevants[i]))\n",
    "        f_measure = np.mean(f_measures)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        f_measure = compute_f_measure(retrieved, relevant)\n",
    "    \n",
    "    return f_measure\n",
    "  \n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        average_p = []\n",
    "        for j in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[j])\n",
    "            average_p.append(compute_avg_precision(retrieved, relevants[j]))            \n",
    "        map_value = np.mean(average_p)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        map_value = compute_avg_precision(retrieved, relevant)\n",
    "        \n",
    "    return map_value\n",
    "\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        ndcg_values = []\n",
    "        for j in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[j])\n",
    "            ndcg_values.append(compute_ndcg(retrieved, relevants[j]))\n",
    "        ndcg_value = np.mean(ndcg_values)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        ndcg_value = compute_ndcg(retrieved, relevant)\n",
    "        \n",
    "    return ndcg_value\n",
    "\n",
    "\n",
    "print(R_Precision())\n",
    "print(F_measure())\n",
    "print(MAP())\n",
    "print(NDCG())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
