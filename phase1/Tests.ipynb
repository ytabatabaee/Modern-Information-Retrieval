{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1, 2- Tests for Data preparation and index construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdO8v3QCFDf_"
   },
   "outputs": [],
   "source": [
    "word1 = 'فکری'\n",
    "doc_id = 3014\n",
    "\n",
    "\n",
    "word2 = 'هیلاندراس'\n",
    "doc_id2 = 6752\n",
    "bigram = 'لا'\n",
    "\n",
    "def get_count (l):\n",
    "    i = [1 for _,t in l.items() for q in t['text']]\n",
    "    j = [1 for _,t in l.items() if 'title' in t.keys() for q in t['title']]\n",
    "    return len (i) + len(j)\n",
    "\n",
    "\n",
    "def test_prepare_text():\n",
    "    print (\"\\n============ testing 'prepare_text' =============================================\")\n",
    "    raw_text = \"کتابهای مناسبی نوشته شوند ! در راستای ارتقای . سطح آموزش کشور ؟ تلاش‌های زیادی صورت می‌گیرد\"\n",
    "    prepared_text = prepare_text(raw_text)\n",
    "    \n",
    "    print(\"prepared text is :\", prepared_text , \"with length:\" , len (prepared_text))\n",
    "    \n",
    "test_prepare_text()\n",
    "\n",
    "def test_get_posting_list():\n",
    "    \n",
    "    print (\"\\n============ testing 'get_posting_list' =========================================\")\n",
    "    \n",
    "    prepared_text = prepare_text(word1)[0]\n",
    "    posting_list = get_posting_list(prepared_text)\n",
    "    # posting_list = {3014:{'title':[...] , 'text':[...]}}\n",
    "    \n",
    "    \n",
    "#     print (\"posting list for input\" , prepared_text, \"is :\", posting_list , \"with length:\" , len (posting_list))\n",
    "    print (\"number of ocurrences of the word\", word1 , \" in documents = \", get_count (posting_list))\n",
    "    print ('docs with the word:' , sorted (list (posting_list.keys())))\n",
    "    \n",
    "test_get_posting_list()\n",
    "\n",
    "\n",
    "def test_bigram():\n",
    "    print (\"\\n============ testing 'get_words_with_bigram' ====================================\")\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "check_bigram = True\n",
    "if check_bigram:\n",
    "    test_bigram()\n",
    "\n",
    "def test_doc_remove():\n",
    "    \n",
    "    print (\"\\n============ testing 'doc_remove' ================================================\")\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    \n",
    "    # *********** Note ************\n",
    "    # This part has changed, I've removed the /Persian.xml and added a seperate file \n",
    "    # for 3014.xml and 6752.xml in data directory\n",
    "    delete_document_from_indexes('data', doc_id)\n",
    "    \n",
    "    # *********** Note ************\n",
    "    # This part is added in order to call bigram removal function\n",
    "    remove_bigram_index('data/' + str(doc_id) + '.xml')\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"before removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    delete_document_from_indexes('data', doc_id2)\n",
    "    \n",
    "     # *********** Note ************\n",
    "    # This part is added in order to call bigram removal function\n",
    "    remove_bigram_index('data/' + str(doc_id2) + '.xml')\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"after removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "test_doc_remove()\n",
    "\n",
    "def test_doc_remove_bigram():\n",
    "    print (\"\\n============ testing correct bigram removal ========================================\")\n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_remove_bigram()\n",
    "\n",
    "def test_doc_add():\n",
    "    print (\"\\n============ testing 'doc_add' ================================================\")\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    add_document_to_indexes('data', doc_id)\n",
    "    \n",
    "     # *********** Note ************\n",
    "    # This part is added in order to call bigram adding function\n",
    "    construct_bigram_index('data/' + str(doc_id) + '.xml')\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"before adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    add_document_to_indexes('data', doc_id2)\n",
    "    \n",
    "    # *********** Note ************\n",
    "    # This part is added in order to call bigram adding function\n",
    "    construct_bigram_index('data/' + str(doc_id2) + '.xml')\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"after adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    \n",
    "test_doc_add()\n",
    "\n",
    "\n",
    "def test_doc_add_bigram():\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_remove_bigram()\n",
    "    \n",
    "def test_save_and_load ():\n",
    "    print (\"\\n============ testing save and load methods ========================================\")\n",
    "    \n",
    "    destination = \"storage/backup/\"\n",
    "\n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before saving:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    save_index(destination)\n",
    "    load_index(destination)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after loading:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "test_save_and_load ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Test for Document retreival, Scoring and VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGxtv7qZM90t"
   },
   "outputs": [],
   "source": [
    "def test_correct_query():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'ابذار های فظایی و پیشرفته ناصا'\n",
    "    ##################################\n",
    "    \n",
    "    result = correct_query(query)\n",
    "    print(result)\n",
    "\n",
    "test_correct_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9n-4JRDNDcS"
   },
   "outputs": [],
   "source": [
    "def test_search():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
    "    method = \"ltc-lnc\"\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = search(query, method)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi_cziuINH8x"
   },
   "outputs": [],
   "source": [
    "def test_detailed_search():\n",
    "    \n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    title_query = 'فهرست شهرهای ایران'\n",
    "    text_query = 'استان گیلان شهرستان لنگرود'\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = detailed_search(title_query, text_query)\n",
    "    print(relevant_docs)\n",
    "\n",
    "test_detailed_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Tests for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPyzE_xlFDgG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    python_open\n",
    "    print(\"Already done!\")\n",
    "except NameError:\n",
    "    python_open = open\n",
    "    def open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n",
    "        encoding=\"utf-8\"\n",
    "        return python_open(file, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd, opener=opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOLBO7uMFDgK"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = ['all', 1, 2, 3]\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "\n",
    "for doc in test_docs:\n",
    "    print(\"{}\\ndoc:\\t{}\".format('-'*30, doc))\n",
    "    for f in functions.keys():\n",
    "        out = functions[f](doc)\n",
    "        print(\"{:11}:\\t{:.2f}\".format(f, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39oS5a9AFDgN"
   },
   "source": [
    "------------------------------\n",
    "    doc:\tall\n",
    "    R_Precision:\t0.73\n",
    "    F_measure  :\t0.65\n",
    "    MAP        :\t0.63\n",
    "    NDCG       :\t0.75\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t1\n",
    "    R_Precision:\t1.00\n",
    "    F_measure  :\t0.97\n",
    "    MAP        :\t0.94\n",
    "    NDCG       :\t0.96\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t2\n",
    "    R_Precision:\t1.00\n",
    "    F_measure  :\t0.91\n",
    "    MAP        :\t0.83\n",
    "    NDCG       :\t0.90\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t3\n",
    "    R_Precision:\t0.73\n",
    "    F_measure  :\t0.49\n",
    "    MAP        :\t0.29\n",
    "    NDCG       :\t0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edited functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the average precision for relevant and retrieved docs\n",
    "\n",
    "def compute_avg_precision(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1   \n",
    "    p = np.divide(np.cumsum(p) * p, np.arange(1, len(retrieved) + 1))\n",
    "    p = p[p > 0]  \n",
    "    # return np.nan_to_num(np.mean(p))\n",
    "    return np.nan_to_num(np.sum(p)) / len(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the NDCG for relevant and retrieved docs\n",
    "\n",
    "def compute_ndcg(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1 \n",
    "    # ideal = np.array([1] * int(np.sum(p)) + [0] * (len(p) - int(np.sum(p))))\n",
    "    ideal = np.array([1] * len(relevant))\n",
    "    dcg_rf = p[0] + sum(np.divide(p[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    # dcg_gt = ideal[0] + sum(np.divide(ideal[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    dcg_gt = ideal[0] + sum(np.divide(ideal[1:], np.log2(np.arange(2, len(relevant) + 1))))\n",
    "    ndcg = dcg_rf / dcg_gt\n",
    "    return np.nan_to_num(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the f-measure for relevant and retrieved docs\n",
    "\n",
    "def compute_f_measure(retrieved, relevant):\n",
    "    alpha = 0.5\n",
    "    p = len(list(set(retrieved) & set(relevant))) / len(retrieved)\n",
    "    r = len(list(set(retrieved) & set(relevant))) / len(relevant)\n",
    "    beta2 = (1 - alpha) / alpha\n",
    "    if beta2 * p + r == 0: # this is added\n",
    "        return 0\n",
    "    f_measure = (beta2 + 1) * p * r / (beta2 * p + r)\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKPkT5jEFDgN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [1, 2, 3]\n",
    "rels = [\n",
    "    [6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666, 5967],\n",
    "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
    "    list(range(20))\n",
    "]\n",
    "outputs = [{'R_Precision': 1.0, 'F_measure': 0.967741935483871, 'MAP': 0.9375, 'NDCG': 0.9635640110263509},\n",
    "           {'R_Precision': 0.4444444444444444, 'F_measure': 0.6153846153846153, 'MAP': 0.4444444444444444, 'NDCG': 0.6313802022799658},\n",
    "           {'R_Precision': 0.0, 'F_measure': 0.0, 'MAP': 0.0, 'NDCG': 0.0}]\n",
    "\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "idx = 0 \n",
    "\n",
    "ds = detailed_search\n",
    "s = search\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    return rels[idx][:max_retrieved]\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2, max_retrieved=15):\n",
    "    return rels[idx][:max_retrieved]\n",
    "\n",
    "for f in functions.keys():\n",
    "    print(\"{}\\n{}:\".format('-'*30, f))\n",
    "    idx = 0\n",
    "    for doc in test_docs: \n",
    "        out = functions[f](doc)\n",
    "        expected = outputs[idx][f]\n",
    "        print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out-expected)<1e-3))\n",
    "        idx += 1\n",
    "\n",
    "detailed_search = ds\n",
    "search = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8vPRt7GFDgS"
   },
   "source": [
    "------------------------------\n",
    "    R_Precision: \n",
    "    1:\t1.00\tTrue \n",
    "    2:\t0.44\tTrue \n",
    "    3:\t0.00\tTrue \n",
    "    \n",
    "------------------------------\n",
    "    F_measure: \n",
    "    1:\t0.97\tTrue\n",
    "    2:\t0.62\tTrue\n",
    "    3:\t0.00\tTrue\n",
    "\n",
    "------------------------------\n",
    "    MAP: \n",
    "    1:\t0.94\tTrue\n",
    "    2:\t0.44\tTrue\n",
    "    3:\t0.00\tTrue\n",
    "\n",
    "------------------------------\n",
    "    NDCG:\n",
    "    1:\t0.96\tTrue\n",
    "    2:\t0.63\tTrue\n",
    "    3:\t0.00\tTrue\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
