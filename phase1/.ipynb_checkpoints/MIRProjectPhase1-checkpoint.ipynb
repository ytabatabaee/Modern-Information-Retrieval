{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text:\n",
      "سلام\n",
      "['سلا']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "def prepare_text(raw_text, stem=True):\n",
    "    punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n»«\\'،.)(!؟؛:×٪ٓ<>}{[]==\\'\\'\\'\\'\\'\"\"\"\n",
    "    \n",
    "    # یکسان‌سازی متن\n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "\n",
    "    # جداکردن کلمات یک جمله\n",
    "    tokenizer = WordTokenizer(separate_emoji=True)\n",
    "    tokenized_text = tokenizer.tokenize(normalized_text)\n",
    "\n",
    "    # حذف علائم نگارشی\n",
    "    tokens = []\n",
    "    for token in tokenized_text:\n",
    "        if token in punctuation:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # بازگرداندن کلمات به ریشه\n",
    "    #if stem:\n",
    "    stemmer = Stemmer()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = stemmer.stem(tokens[i])\n",
    "\n",
    "    # حذف رشته‌های خالی\n",
    "    tokens = [tokens[i] for i in range(len(tokens)) if tokens[i] is not '']\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print('Enter text:')\n",
    "raw_text = input()\n",
    "print(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " <div (30 نمره)   iv style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"> <font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# read data\n",
    "def parse_xml(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    docs = []\n",
    "    for page in root.findall('{http://www.mediawiki.org/xml/export-0.10/}page'):\n",
    "        doc = {}\n",
    "        title = page.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "        id = int(page.find('{http://www.mediawiki.org/xml/export-0.10/}id').text)\n",
    "        text = page.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find(\n",
    "            '{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "\n",
    "        doc['id'] = id\n",
    "        doc['title'] = title\n",
    "        doc['text'] = text\n",
    "\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "    \n",
    "# construct positional index\n",
    "pos_index = defaultdict(list)\n",
    "bigram_index = defaultdict(set)\n",
    "doc_ids = set()\n",
    "\n",
    "\n",
    "def add_term_to_bigram_index(term):\n",
    "    global bigram_index\n",
    "    chars = '$' + term + '$'\n",
    "    for i in range(len(chars) - 1):\n",
    "        if chars[i:i + 2] not in bigram_index or term not in bigram_index[chars[i:i + 2]]:\n",
    "            bigram_index[chars[i:i + 2]].add(term)\n",
    "\n",
    "\n",
    "def add_doc_to_pos_index(doc):\n",
    "    \n",
    "\n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    global pos_index\n",
    "    global doc_ids\n",
    "    docs = parse_xml(docs_path)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_id = doc['id']\n",
    "        prepared_title = prepare_text(doc['title'])\n",
    "        prepared_text = prepare_text(doc['text'])\n",
    "\n",
    "        #bigram_prepare = prepare_text(doc['title'], False) + prepare_text(doc['text'], False)\n",
    "\n",
    "        #for term in bigram_prepare:\n",
    "        #add_term_to_bigram_index(term)\n",
    "\n",
    "        doc_ids.add(doc_id)\n",
    "\n",
    "        for pos, term in enumerate(prepared_title):\n",
    "            if term in pos_index:\n",
    "                # increase frequency\n",
    "                pos_index[term][0] += 1\n",
    "                if doc_id not in pos_index[term][1]:\n",
    "                    pos_index[term][1][doc_id] = {'title': []}\n",
    "            else:\n",
    "                # create positional posting list\n",
    "                pos_index[term] = [1]\n",
    "                pos_index[term].append({})\n",
    "                pos_index[term][1][doc_id] = {'title': []}\n",
    "\n",
    "            if 'title' not in pos_index[term][1][doc_id]:\n",
    "                pos_index[term][1][doc_id]['title'] = []\n",
    "\n",
    "            # add position to posting list\n",
    "            pos_index[term][1][doc_id]['title'].append(pos)      \n",
    "\n",
    "        for pos, term in enumerate(prepared_text):\n",
    "            if term in pos_index:\n",
    "                # increase frequency\n",
    "                pos_index[term][0] += 1\n",
    "                if doc_id not in pos_index[term][1]:\n",
    "                    pos_index[term][1][doc_id] = {'text': []}\n",
    "            else:\n",
    "                # create positional posting list\n",
    "                pos_index[term] = [1]\n",
    "                pos_index[term].append({})\n",
    "                pos_index[term][1][doc_id] = {'text': []}\n",
    "\n",
    "            if 'text' not in pos_index[term][1][doc_id]:\n",
    "                pos_index[term][1][doc_id]['text'] = []\n",
    "\n",
    "            # add position to posting list\n",
    "            pos_index[term][1][doc_id]['text'].append(pos)\n",
    "\n",
    "\n",
    "data_path = 'data/Persian.xml'\n",
    "construct_positional_indexes(data_path)\n",
    "print(pos_index['بازی'])\n",
    "print(bigram_index['یا'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [],
   "source": [
    "#posting_list = {10: {'title': [2], 'text': [4, 8]}, 29: {'text': [19]}}\n",
    "\n",
    "def get_posting_list(word):\n",
    "    global pos_index\n",
    "    posting_list = pos_index[word][1]\n",
    "    return posting_list\n",
    "\n",
    "get_posting_list('سلام')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    global bigram_index\n",
    "    words = bigram_index[bigram]\n",
    "    return words\n",
    "\n",
    "get_words_with_bigram('لا')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    global pos_index, doc_ids\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    doc = {}\n",
    "    for page in root.findall('{http://www.mediawiki.org/xml/export-0.10/}page'):\n",
    "        id = int(page.find('{http://www.mediawiki.org/xml/export-0.10/}id').text)\n",
    "        if id == doc_num:\n",
    "            title = page.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "            text = page.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find(\n",
    "                '{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "            doc['id'] = id\n",
    "            doc['title'] = title\n",
    "            doc['text'] = text\n",
    "            break\n",
    "\n",
    "    if len(doc) == 0:\n",
    "        return -1\n",
    "\n",
    "    doc_id = doc['id']\n",
    "\n",
    "    if doc_id in doc_ids:\n",
    "        return 0\n",
    "    \n",
    "    add_doc_to_pos_index(doc)\n",
    "    return 1\n",
    "\n",
    "\n",
    "add_document_to_indexes(data_path, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_term_from_bigram_index(term):\n",
    "    global bigram_index\n",
    "    chars = '$' + term + '$'\n",
    "    for i in range(len(chars) - 1):\n",
    "        if chars[i:i + 2] in bigram_index and term in bigram_index[chars[i:i + 2]]:\n",
    "            bigram_index[chars[i:i + 2]].remove(term)\n",
    "    \n",
    "\n",
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    global pos_index, doc_ids, bigram_index\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    doc = {}\n",
    "    for page in root.findall('{http://www.mediawiki.org/xml/export-0.10/}page'):\n",
    "        id = int(page.find('{http://www.mediawiki.org/xml/export-0.10/}id').text)\n",
    "        if id == doc_num:\n",
    "            title = page.find('{http://www.mediawiki.org/xml/export-0.10/}title').text\n",
    "            text = page.find('{http://www.mediawiki.org/xml/export-0.10/}revision').find(\n",
    "                '{http://www.mediawiki.org/xml/export-0.10/}text').text\n",
    "            doc['id'] = id\n",
    "            doc['title'] = title\n",
    "            doc['text'] = text\n",
    "\n",
    "    if len(doc) == 0:\n",
    "        return -1\n",
    "\n",
    "    doc_id = doc['id']\n",
    "    prepared_title = prepare_text(doc['title'])\n",
    "    prepared_text = prepare_text(doc['text'])\n",
    "    \n",
    "    doc_ids.remove(doc_id)\n",
    "\n",
    "    terms = prepared_title + prepared_text\n",
    "\n",
    "    for term in terms:\n",
    "        if term not in pos_index:\n",
    "            continue\n",
    "\n",
    "        if doc_id in pos_index[term][1]:\n",
    "            del pos_index[term][1][doc_id]\n",
    "            if len(pos_index[term][1]) == 0:\n",
    "                del pos_index[term]\n",
    "                delete_term_from_bigram_index(term)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "delete_document_from_indexes(data_path, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    global pos_index, doc_ids, bigram_index\n",
    "    with open(destination + '.pkl', 'wb') as f:\n",
    "        pickle.dump(pos_index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(destination + '_ids.pkl', 'wb') as f1:\n",
    "        pickle.dump(doc_ids, f1, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(destination + '_bigram.pkl', 'wb') as f2:\n",
    "        pickle.dump(bigram_index, f2, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    global pos_index, doc_ids, bigram_index\n",
    "    with open(source + '.pkl', 'rb') as f:\n",
    "        pos_index = pickle.load(f)\n",
    "    with open(source + '_ids.pkl', 'rb') as f1:\n",
    "        doc_ids = pickle.load(f1)\n",
    "    with open(source + '_bigram.pkl', 'rb') as f2:\n",
    "        bigram_index = pickle.load(f2)\n",
    "    \n",
    "load_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [],
   "source": [
    "def jaccard_coefficient(t1, t2):\n",
    "    global bigram_index\n",
    "    chars = '$' + t1 + '$'\n",
    "    \n",
    "    terms_intersection = 0\n",
    "    for i in range(len(chars) - 1):\n",
    "        if t2 in bigram_index[chars[i:i + 2]]:\n",
    "            terms_intersection += 1\n",
    "            \n",
    "    terms_union = len(t1) + 1 + len(t2) + 1 - terms_intersection\n",
    "    \n",
    "    return terms_intersection / terms_union\n",
    "\n",
    "\n",
    "def find_candidates(term):\n",
    "    global bigram_index\n",
    "    chars = '$' + term + '$'\n",
    "    \n",
    "    candidates = set()\n",
    "    bigram_words = set()\n",
    "    for i in range(len(chars) - 1):\n",
    "        for t2 in bigram_index[chars[i:i + 2]]:\n",
    "            bigram_words.add(t2)\n",
    "    \n",
    "    for t2 in bigram_words:\n",
    "        if jaccard_coefficient(term, t2) > 0.4:\n",
    "            candidates.add(t2)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    n = len(s1)\n",
    "    m = len(s2)\n",
    "    d = [[0 for i in range(m)] for j in range(n)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        d[i][0] = i\n",
    "    \n",
    "    for j in range(m):\n",
    "        d[0][j] = j\n",
    "        \n",
    "    for i in range(1, n):\n",
    "        for j in range(1, m):\n",
    "            score = 1\n",
    "            if s1[i] == s2[j]:\n",
    "                score = 0\n",
    "            d[i][j] = min(d[i-1][j-1] + score, d[i-1][j] + 1, d[i][j-1] + 1)\n",
    "    \n",
    "    return d[n-1][m-1]\n",
    "        \n",
    "\n",
    "def correct_query(query):\n",
    "    prepared_query = prepare_text(query)\n",
    "    for i in range(len(prepared_query)):\n",
    "        term = prepared_query[i]\n",
    "        if term not in pos_index:\n",
    "            bigram_candidates = find_candidates(term)\n",
    "            best_candidate = term\n",
    "            best_distance = math.inf\n",
    "            for can in bigram_candidates:\n",
    "                distance = edit_distance(term, can)\n",
    "                if distance < best_distance:\n",
    "                    best_distance = distance\n",
    "                    best_candidate = can\n",
    "            prepared_query[i] = best_candidate\n",
    "    \n",
    "    correct_query = ''\n",
    "    for w in prepared_query:\n",
    "        correct_query += w + ' '\n",
    "    \n",
    "    correct_query = correct_query[:-1]\n",
    "                \n",
    "#     correct_query = \"سلام حالا پرسمان درست شد.\"\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "\n",
    "tf_doc_title = {}\n",
    "tf_doc_text = {}\n",
    "idf_doc_title = {}\n",
    "idf_doc_text = {}\n",
    "\n",
    "def construct_tf_idf():\n",
    "    global pos_index, tf_doc_title, tf_doc_text, idf_doc_title, idf_doc_text\n",
    "    df_title = {}\n",
    "    df_text = {}\n",
    "    for term in pos_index:\n",
    "        tf_doc_title[term] = {}\n",
    "        tf_doc_text[term] = {}\n",
    "        df_title[term] = len([1 for doc in pos_index[term][1] if 'title' in pos_index[term][1][doc]])\n",
    "        df_text[term] = len([1 for doc in pos_index[term][1] if 'text' in pos_index[term][1][doc]])\n",
    "        for doc in pos_index[term][1]:\n",
    "            tf_doc_title[term][doc] = len(pos_index[term][1][doc]['title']) if 'title' in pos_index[term][1][\n",
    "                doc] else 0\n",
    "            tf_doc_text[term][doc] = len(pos_index[term][1][doc]['text']) if 'text' in pos_index[term][1][\n",
    "                doc] else 0\n",
    "\n",
    "    total_doc_num = len(doc_ids)\n",
    "    idf_doc_title = {i: total_doc_num / (1 + df_title[i]) for i in df_title.keys()}\n",
    "    idf_doc_text = {i: total_doc_num / (1 + df_text[i]) for i in df_text.keys()}\n",
    "\n",
    "construct_tf_idf()\n",
    "\n",
    "\n",
    "def log_tf(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1 + math.log10(x)\n",
    "\n",
    "\n",
    "def log_idf(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return math.log10(x)\n",
    "\n",
    "\n",
    "def tf_query(term, query):\n",
    "    tf = 0\n",
    "    for w in query:\n",
    "        if w == term:\n",
    "            tf += 1\n",
    "    return tf\n",
    "\n",
    "\n",
    "def vectorize_query(query, normalization=False):\n",
    "    query_vec = {}\n",
    "    norm = 0\n",
    "\n",
    "    for term in query:\n",
    "        query_vec[term] = log_tf(tf_query(term, query))\n",
    "        if normalization:\n",
    "            norm += query_vec[term] ** 2\n",
    "\n",
    "    if normalization:\n",
    "        query_vec = {term: query_vec[term] / math.sqrt(norm) for term in query}\n",
    "\n",
    "    return query_vec\n",
    "\n",
    "\n",
    "def vectorize_doc(query, doc, tf, idf, normalization=False):\n",
    "    doc_vec = {}\n",
    "    norm = 0\n",
    "\n",
    "    for term in query:\n",
    "        if term in tf and term in idf and doc in tf[term]:\n",
    "            doc_vec[term] = log_tf(tf[term][doc]) * log_idf(idf[term])\n",
    "        else:\n",
    "            doc_vec[term] = 0\n",
    "        if normalization:\n",
    "            norm += doc_vec[term] ** 2\n",
    "\n",
    "    if normalization and norm > 0:\n",
    "        doc_vec = {term: doc_vec[term] / math.sqrt(norm) for term in query}\n",
    "\n",
    "    return doc_vec\n",
    "\n",
    "\n",
    "\n",
    "def find_exact_phrases(query):\n",
    "    return re.findall(r'\"(.*?)\"', query)\n",
    "\n",
    "\n",
    "def phrase_query_search(phrase, where=None):\n",
    "    global pos_index\n",
    "    phrase_docs = set()\n",
    "    term_freqs = []\n",
    "    for term in phrase:\n",
    "        term_freqs.append(pos_index[term][0])\n",
    "\n",
    "    terms = [w for _, w in sorted(zip(term_freqs, phrase))]\n",
    "\n",
    "    terms_intersect = pos_index[terms[0]][1].keys()\n",
    "    for i in range(1, len(terms)):\n",
    "        terms_intersect = terms_intersect & pos_index[terms[i]][1].keys()\n",
    "\n",
    "    for doc in terms_intersect:\n",
    "        found = True\n",
    "        for i in range(len(phrase) - 1):\n",
    "            positions1 = pos_index[phrase[i]][1][doc]\n",
    "            positions2 = pos_index[phrase[i + 1]][1][doc]\n",
    "            if not position_search(positions1, positions2, where):\n",
    "                found = False\n",
    "                break\n",
    "        if found:\n",
    "            phrase_docs.add(doc)\n",
    "\n",
    "    return phrase_docs\n",
    "\n",
    "\n",
    "def position_search(positions1, positions2, where=None):\n",
    "    if where is None or where == 'text':\n",
    "        if 'text' in positions1 and 'text' in positions2:\n",
    "            pp1 = positions1['text']\n",
    "            pp2 = positions2['text']\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(pp1) and j < len(pp2):\n",
    "                if pp1[i] + 1 == pp2[j]:\n",
    "                    return True\n",
    "                if pp1[i] > pp2[j]:\n",
    "                    j += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    if where is None or where == 'title':\n",
    "        if 'title' in positions1 and 'title' in positions2:\n",
    "            pp1 = positions1['title']\n",
    "            pp2 = positions2['title']\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(pp1) and j < len(pp2):\n",
    "                if pp1[i] + 1 == pp2[j]:\n",
    "                    return True\n",
    "                if pp1[i] > pp2[j]:\n",
    "                    j += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return False\n",
    "\n",
    "                \n",
    "def filter_docs(exact_phrases, where=None):\n",
    "    if len(exact_phrases) == 0:\n",
    "        return doc_ids\n",
    "\n",
    "    docs = set()\n",
    "    for i in range(len(exact_phrases)):\n",
    "        correct_phrase = correct_query(exact_phrases[i])\n",
    "        prepared_phrase = prepare_text(correct_phrase)\n",
    "        phrase_docs = phrase_query_search(prepared_phrase, where)\n",
    "        if i == 0:\n",
    "            docs = phrase_docs\n",
    "        else:\n",
    "            docs = docs.intersection(phrase_docs)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    global doc_ids, tf_doc_title, tf_doc_text, idf_doc_title, idf_doc_text\n",
    "\n",
    "    normalization = False\n",
    "    if method == \"ltc-lnc\":\n",
    "        normalization = True\n",
    "\n",
    "    exact_phrases = find_exact_phrases(query)\n",
    "\n",
    "    docs = filter_docs(exact_phrases, where=None)\n",
    "\n",
    "    corr_query = correct_query(query)\n",
    "    prepared_query = prepare_text(corr_query)\n",
    "    query_vec = vectorize_query(prepared_query, normalization)\n",
    "\n",
    "    score = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        # title weight\n",
    "        doc_vec_title = vectorize_doc(prepared_query, doc, tf_doc_title, idf_doc_title, normalization)\n",
    "\n",
    "        # text weight\n",
    "        doc_vec_text = vectorize_doc(prepared_query, doc, tf_doc_text, idf_doc_text, normalization)\n",
    "\n",
    "        score[doc] = sum(\n",
    "            [query_vec[term] * doc_vec_title[term] * weight + query_vec[term] * doc_vec_text[term] for term in\n",
    "             prepared_query])\n",
    "    \n",
    "    heap = [(value, key) for key,value in score.items()]\n",
    "    largest = heapq.nlargest(15, heap)\n",
    "#     sorted_score = sorted(score.items(), key=itemgetter(1), reverse=True)\n",
    "    relevant_docs = []\n",
    "\n",
    "    for score, doc in largest:\n",
    "        relevant_docs.append(doc)\n",
    "        \n",
    "#     if len(relevant_docs) > 15:\n",
    "#         relevant_docs = relevant_docs[:15]\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    "search('مطالعه \"علوم اجتماعی\" در دانشگاه', method=\"ltc-lnc\", weight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    global doc_ids, tf_doc_title, tf_doc_text, idf_doc_title, idf_doc_text\n",
    "\n",
    "    normalization = False\n",
    "    if method == \"ltc-lnc\":\n",
    "        normalization = True\n",
    "\n",
    "    detailed_phrases_title = find_exact_phrases(title_query)\n",
    "    detailed_phrases_text = find_exact_phrases(text_query)\n",
    "\n",
    "    docs_title = filter_docs(detailed_phrases_title, where='title')\n",
    "    docs_text = filter_docs(detailed_phrases_text, where='text')\n",
    "\n",
    "    docs = docs_title.intersection(docs_text)\n",
    "\n",
    "    corr_title_query = correct_query(title_query)\n",
    "    corr_text_query = correct_query(text_query)\n",
    "    \n",
    "    prepared_title_query = prepare_text(corr_title_query)\n",
    "    prepared_text_query = prepare_text(corr_text_query)\n",
    "\n",
    "    title_query_vec = vectorize_query(prepared_title_query, normalization)\n",
    "    text_query_vec = vectorize_query(prepared_text_query, normalization)\n",
    "\n",
    "    score = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        # title weight\n",
    "        doc_vec_title = vectorize_doc(prepared_title_query, doc, tf_doc_title, idf_doc_title, normalization)\n",
    "\n",
    "        # text weight\n",
    "        doc_vec_text = vectorize_doc(prepared_text_query, doc, tf_doc_text, idf_doc_text, normalization)\n",
    "\n",
    "        score[doc] = sum(\n",
    "            [title_query_vec[term] * doc_vec_title[term] for term in prepared_title_query] + [\n",
    "                text_query_vec[term] * doc_vec_text[term] for term in\n",
    "                prepared_text_query])\n",
    "\n",
    "#     sorted_score = sorted(score.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    heap = [(value, key) for key,value in score.items()]\n",
    "    largest = heapq.nlargest(15, heap)\n",
    "    \n",
    "    relevant_docs = []\n",
    "\n",
    "    for score, doc in largest:\n",
    "        relevant_docs.append(doc)\n",
    "\n",
    "#     relevant_docs = []\n",
    "#     for doc, score in sorted_score:\n",
    "#         if score == 0:\n",
    "#             break\n",
    "#         relevant_docs.append(doc)\n",
    "    \n",
    "#     if len(relevant_docs) > 15:\n",
    "#         relevant_docs = relevant_docs[:15]\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    "detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def read_files(data_type, query_id='all'):\n",
    "    all_files = []\n",
    "    if query_id == 'all':\n",
    "        files = glob.glob('data/' + data_type + '/*.txt')\n",
    "        files = sorted(files)\n",
    "        for file in files:\n",
    "            with open(file, 'r') as target_file:\n",
    "                all_files.append([l for l in (line.strip() for line in target_file) if l])\n",
    "    else:\n",
    "        with open('data/'+ data_type + '/%s.txt' % (query_id,)) as target_file:\n",
    "            all_files.append(target_file_file.read())\n",
    "    return all_files\n",
    "\n",
    "def text_to_list(doc_list):\n",
    "    res = []\n",
    "    for doc in doc_list:\n",
    "        l = list(map(int, doc[0].split(',')))\n",
    "        res.append(l)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def R_Precision(query_id='all'):\n",
    "    queries = read_files('queries', query_id)\n",
    "    relevant_docs = read_files('relevance', query_id)\n",
    "    relevant_docs = text_to_list(relevant_docs)\n",
    "    \n",
    "    retrieved = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        if len(queries[i]) > 1:\n",
    "            title_query, text_query = queries[i][:2] \n",
    "            retrieved.append(detailed_search(title_query, text_query))\n",
    "        else:\n",
    "            retrieved.append(search(queries[i][0]))\n",
    "    \n",
    "    r_precision = []\n",
    "    \n",
    "    for i in range(len(retrieved)):\n",
    "        num_relevant = 0\n",
    "        l = len(relevant_docs[i])\n",
    "        if len(retrieved[i]) < len(relevant_docs[i]):\n",
    "            l = len(retrieved[i])\n",
    "        for j in range(l):\n",
    "            if retrieved[i][j] in relevant_docs[i]:\n",
    "                num_relevant += 1\n",
    "                \n",
    "        r_precision.append(num_relevant / len(relevant_docs[i]))\n",
    "    \n",
    "    result = sum(r_precision) / len(r_precision)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    queries = read_files('queries', query_id)\n",
    "    relevant_docs = read_files('relevance', query_id)\n",
    "    relevant_docs = text_to_list(relevant_docs)\n",
    "    \n",
    "    retrieved = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        if len(queries[i]) > 1:\n",
    "            title_query, text_query = queries[i][:2] \n",
    "            retrieved.append(detailed_search(title_query, text_query))\n",
    "        else:\n",
    "            retrieved.append(search(queries[i][0]))\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(retrieved)):\n",
    "        precision.append(len(set(retrieved[i]).intersection(relevant_docs[i])) / len(retrieved[i]))\n",
    "        recall.append(len(set(retrieved[i]).intersection(relevant_docs[i])) / len(relevant_docs[i]))\n",
    "    \n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    \n",
    "    result = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    queries = read_files('queries', query_id)\n",
    "    relevant_docs = read_files('relevance', query_id)\n",
    "    relevant_docs = text_to_list(relevant_docs)\n",
    "    \n",
    "    retrieved = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        if len(queries[i]) > 1:\n",
    "            title_query, text_query = queries[i][:2] \n",
    "            retrieved.append(detailed_search(title_query, text_query))\n",
    "        else:\n",
    "            retrieved.append(search(queries[i][0]))\n",
    "    \n",
    "    precision = []\n",
    "    \n",
    "    for i in range(len(retrieved)):\n",
    "        num_relevant = 0\n",
    "        precision.append([])\n",
    "        for j in range(len(retrieved[i])):\n",
    "            if retrieved[i][j] in relevant_docs[i]:\n",
    "                num_relevant += 1\n",
    "                precision[i].append(num_relevant / (i + 1))\n",
    "        precision[i] = sum(precision[i]) / len(precision[i])\n",
    "    \n",
    "    result = sum(precision) / len(precision)\n",
    "\n",
    "    return result\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    queries = read_files('queries', query_id)\n",
    "    relevant_docs = read_files('relevance', query_id)\n",
    "    relevant_docs = text_to_list(relevant_docs)\n",
    "    \n",
    "    retrieved = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        if len(queries[i]) > 1:\n",
    "            title_query, text_query = queries[i][:2] \n",
    "            retrieved.append(detailed_search(title_query, text_query))\n",
    "        else:\n",
    "            retrieved.append(search(queries[i][0]))\n",
    "    \n",
    "    max_length = max(len(r_list) for r_list in relevant_docs)\n",
    "    ideal_dcg_vals = [1 for i in range(max_length)]\n",
    "    \n",
    "    for i in range(1, len(ideal_dcg_vals)):\n",
    "        ideal_dcg_vals[i] = ideal_dcg_vals[i - 1] + ideal_dcg_vals[i] / math.log2(i + 1)\n",
    "    \n",
    "    ndcg = []\n",
    "    \n",
    "    for i in range(len(retrieved)):\n",
    "        num_relevant = 0\n",
    "        ideal_dcg = ideal_dcg_vals[:len(relevant_docs[i])]\n",
    "        actual_dcg = []\n",
    "        for j in range(len(retrieved[i])):\n",
    "            if j >= len(relevant_docs[i]):\n",
    "                break\n",
    "            if retrieved[i][j] in relevant_docs[i]:\n",
    "                if j == 0:\n",
    "                    actual_dcg.append(1)\n",
    "                else:\n",
    "                    actual_dcg.append(1 / math.log2(j + 1))\n",
    "            else:\n",
    "                actual_dcg.append(0)\n",
    "                \n",
    "        for j in range(1, len(actual_dcg)):\n",
    "            actual_dcg[j] += actual_dcg[j - 1]\n",
    "            \n",
    "        ndcg.append(sum(actual_dcg) / sum(ideal_dcg))\n",
    "    \n",
    "    result = sum(ndcg) / len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"R_Precision:\", R_Precision())\n",
    "print(\"F_measure:\", F_measure())\n",
    "print(\"MAP:\", MAP())\n",
    "print(\"NDCG:\", NDCG())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
