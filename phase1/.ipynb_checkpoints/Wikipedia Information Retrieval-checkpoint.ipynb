{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System for Persian Wikipedia \n",
    "\n",
    "In this project, I will implement a simple information retrieval system for a dataset consisting of Persian Wikipedia web pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data preparation and Text Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import heapq\n",
    "import random\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "# Preparing text for use in index\n",
    "# Note: Bigram index doesn't use stemming\n",
    "\n",
    "def prepare_text(raw_text, stem_flag=True):\n",
    "    prepared_text = raw_text\n",
    "    \n",
    "    # 1. Remove Punctuation marks\n",
    "    punctuation = '[^۰-۹ آ-ی a-z 0-9 \\u200c]'\n",
    "    prepared_text = re.sub(punctuation, ' ', prepared_text)\n",
    "    \n",
    "    # 2. Normalization\n",
    "    normalizer = Normalizer()\n",
    "    prepared_text = normalizer.normalize(prepared_text) \n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(prepared_text) \n",
    "    \n",
    "    # 4. Stemming\n",
    "    if stem_flag:\n",
    "        stemmer = Stemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens] \n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# 2- Index Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1- Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "doc_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses an xml document and extracts id, title and text\n",
    "\n",
    "def parse_next_doc(corpus):\n",
    "    content = ''\n",
    "    for line in corpus:\n",
    "        content += line\n",
    "        if '</page>' in line:\n",
    "            break\n",
    "    try:\n",
    "        id = re.search('<id>(.*?)</id>', content, re.DOTALL).group(1)\n",
    "        title = re.search('<title>(.*?)</title>', content, re.DOTALL).group(1) \n",
    "        text = re.search('<text(.*?)</text>', content, re.DOTALL).group(1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    doc_dict = {'id': id, 'title': title, 'text': text}\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds content of a zone(title, text) for a doc to index\n",
    "\n",
    "def add_zone_to_index(doc_dict, zone):\n",
    "    doc_id = doc_dict.get('id')\n",
    "    prepared_zone = prepare_text(doc_dict.get(zone), True)\n",
    "    for pos, term in enumerate(prepared_zone):\n",
    "        if term in index:\n",
    "            try:\n",
    "                index[term][doc_id][zone].append(pos)\n",
    "            except:\n",
    "                try:\n",
    "                    index[term][doc_id][zone] = [pos]\n",
    "                except:\n",
    "                    index[term][doc_id] = {}\n",
    "                    index[term][doc_id][zone] = [pos]    \n",
    "                 \n",
    "        else:\n",
    "            index[term] = {}\n",
    "            index[term][doc_id] = {}\n",
    "            index[term][doc_id][zone] = [pos]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is build completely!\n"
     ]
    }
   ],
   "source": [
    "# Building positional index\n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    global index\n",
    "    corpus = open(docs_path, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(corpus)\n",
    "    while doc_dict:\n",
    "        doc_ids.append(doc_dict.get('id'))\n",
    "        add_zone_to_index(doc_dict, 'title')                \n",
    "        add_zone_to_index(doc_dict, 'text')\n",
    "        doc_dict = parse_next_doc(corpus)\n",
    "    print(\"Index is build completely!\")    \n",
    "        \n",
    "construct_positional_indexes('data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [],
   "source": [
    "def get_posting_list(word):\n",
    "    try:\n",
    "        posting_list = index[word]\n",
    "    except:\n",
    "        posting_list = []\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2- Bigram Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_index = {}\n",
    "bigram_dict = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Index is build completely!\n"
     ]
    }
   ],
   "source": [
    "# Building bigram index\n",
    "\n",
    "def construct_bigram_index(docs_path):\n",
    "    global bigram_index\n",
    "    corpus = open(docs_path, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(corpus)\n",
    "    while doc_dict:\n",
    "        doc_id = doc_dict['id']\n",
    "        words = prepare_text(doc_dict['title'], stem_flag=False) + prepare_text(doc_dict['text'], stem_flag=False)\n",
    "        for word in words:\n",
    "            bigram_dict.add(word)\n",
    "            marked_word = '$' + word + '$'\n",
    "            bigrams = [marked_word[i:i + 2] for i in range(0, len(marked_word) - 1)]\n",
    "            for bigram in bigrams:\n",
    "                try:\n",
    "                    bigram_index[bigram].add(word)\n",
    "                except:\n",
    "                    bigram_index[bigram] = set()\n",
    "                    bigram_index[bigram].add(word)\n",
    "        doc_dict = parse_next_doc(corpus)\n",
    "    print(\"Bigram Index is build completely!\") \n",
    "\n",
    "construct_bigram_index('data/Persian.xml')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this function after construct bigram index\n",
    "\n",
    "def remove_bigram_index(docs_path):\n",
    "    global bigram_index\n",
    "    corpus = open(docs_path, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(corpus)\n",
    "    while doc_dict:\n",
    "        doc_id = doc_dict['id']\n",
    "        words = prepare_text(doc_dict['title'], stem_flag=False) + prepare_text(doc_dict['text'], stem_flag=False)\n",
    "        for word in words:\n",
    "            bigram_dict.add(word)\n",
    "            marked_word = '$' + word + '$'\n",
    "            bigrams = [marked_word[i:i + 2] for i in range(0, len(marked_word) - 1)]\n",
    "            for bigram in bigrams:\n",
    "                #try:\n",
    "                #    bigram_index[bigram].add(word)\n",
    "                #except:\n",
    "                #    bigram_index[bigram] = set()\n",
    "                #    bigram_index[bigram].add(word)\n",
    "                if not prepare_text(word)[0] in index:\n",
    "                    try:\n",
    "                        bigram_index[bigram].remove(word) \n",
    "                    except:\n",
    "                        pass\n",
    "                                     \n",
    "        doc_dict = parse_next_doc(corpus)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ورثرغنه',\n",
       " 'ثروت\\u200cها',\n",
       " 'ثروت\\u200cهای',\n",
       " 'دراکثرشهرهای',\n",
       " 'منثره',\n",
       " 'تأثرش',\n",
       " 'یثرب',\n",
       " 'مؤثر\\u200cتر',\n",
       " 'ثریایی',\n",
       " 'اکثرأ',\n",
       " 'اثرگذارترین',\n",
       " 'اثرش',\n",
       " 'اثربخش',\n",
       " 'مفقودالاثرهای',\n",
       " 'ناموثر',\n",
       " 'نثری',\n",
       " 'حداکثری',\n",
       " 'نثر',\n",
       " 'اثرگذار',\n",
       " 'حداکثر',\n",
       " 'اکثریتی',\n",
       " 'تکثر',\n",
       " 'مؤثرتر',\n",
       " 'نثرنویسی',\n",
       " 'اکثرا',\n",
       " 'اثرهایی',\n",
       " 'ثروت\\u200cمندان',\n",
       " 'مؤثرترین',\n",
       " 'میثر',\n",
       " 'ثروتمند',\n",
       " 'ثروتمندان',\n",
       " 'اثری',\n",
       " 'اثربخشی',\n",
       " 'موثرترین',\n",
       " 'اثره',\n",
       " 'اثر',\n",
       " 'مؤثرند',\n",
       " 'اثرگذاری',\n",
       " 'ثروتی',\n",
       " 'اثرها',\n",
       " 'اکثرنفاط',\n",
       " 'تأثر',\n",
       " 'براثر',\n",
       " 'ثروتمندترین\\u200cها',\n",
       " 'میثرا',\n",
       " 'گوثری',\n",
       " 'اثراتی',\n",
       " 'ثروت',\n",
       " 'کثرة',\n",
       " 'موثرتری',\n",
       " 'نام\\u200cاثر',\n",
       " 'اثرهای',\n",
       " 'ثروتهای',\n",
       " 'دراثر',\n",
       " 'موثری',\n",
       " 'کوثر',\n",
       " 'اکثر',\n",
       " 'ثریتونه',\n",
       " 'دراکثر',\n",
       " 'موثرند',\n",
       " 'متأثر',\n",
       " 'تأثری',\n",
       " 'ثروتمندترین',\n",
       " 'خشثریه',\n",
       " 'اثرات',\n",
       " 'بی\\u200cاثر',\n",
       " 'کثرت',\n",
       " 'کوثری',\n",
       " 'أکثرهم',\n",
       " 'موثرتر',\n",
       " 'متاثر',\n",
       " 'مؤثر',\n",
       " 'مؤثری',\n",
       " 'متکثر',\n",
       " 'هیثرو',\n",
       " 'اثر\\u200cها',\n",
       " 'اکثریت',\n",
       " 'میثره',\n",
       " 'تکثرگرایی',\n",
       " 'ثریا',\n",
       " 'حدکثر',\n",
       " 'پرثروت']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    try:\n",
    "        return list(bigram_index[bigram])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "get_words_with_bigram('ثر')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93786\n",
      "{'3415': {'text': [2400, 2410]}, '4058': {'text': [944]}, '4659': {'text': [1745]}, '5237': {'text': [4137]}, '6054': {'text': [1612]}, '7013': {'text': [797, 2202]}}\n"
     ]
    }
   ],
   "source": [
    "# Testing index size and index for 'Newton'\n",
    "\n",
    "print(len(index))\n",
    "print(index['نیوتون'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    file_name = docs_path + '/' + str(doc_num) + '.xml'\n",
    "    global index\n",
    "    doc = open(file_name, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(doc)\n",
    "        \n",
    "    if doc_dict:\n",
    "        if doc_dict.get('id') in doc_ids:\n",
    "            return\n",
    "        add_zone_to_index(doc_dict, 'title')                \n",
    "        add_zone_to_index(doc_dict, 'text')\n",
    "        doc_ids.append(doc_dict.get('id'))\n",
    "\n",
    "add_document_to_indexes('data/wiki', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93951\n",
      "{'3415': {'text': [2400, 2410]}, '4058': {'text': [944]}, '4659': {'text': [1745]}, '5237': {'text': [4137]}, '6054': {'text': [1612]}, '7013': {'text': [797, 2202]}, '650': {'text': [8, 34, 132, 136, 209, 2778, 3508, 3546, 3573, 3617, 3664, 3698, 3796, 3852]}}\n"
     ]
    }
   ],
   "source": [
    "# Doc 650 added to newton index\n",
    "\n",
    "print(len(index))\n",
    "print(index['نیوتون'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zone_from_index(doc_dict, zone):\n",
    "    doc_id = doc_dict.get('id')\n",
    "    prepared_zone = prepare_text(doc_dict.get(zone), True)\n",
    "    for term in prepared_zone:\n",
    "            try:\n",
    "                del index[term][doc_id]\n",
    "                if not bool(index[term]):\n",
    "                    del index[term]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    file_name = docs_path + '/' + str(doc_num) + '.xml'\n",
    "    global index\n",
    "    doc = open(file_name, encoding = 'utf8')\n",
    "    doc_dict = parse_next_doc(doc)\n",
    "    \n",
    "    if doc_dict:\n",
    "        if doc_dict.get('id') not in doc_ids:\n",
    "            return\n",
    "        remove_zone_from_index(doc_dict, 'title')\n",
    "        remove_zone_from_index(doc_dict, 'text')\n",
    "        doc_ids.remove(doc_dict.get('id'))\n",
    "\n",
    "delete_document_from_indexes('data/wiki', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93786\n",
      "{'3415': {'text': [2400, 2410]}, '4058': {'text': [944]}, '4659': {'text': [1745]}, '5237': {'text': [4137]}, '6054': {'text': [1612]}, '7013': {'text': [797, 2202]}}\n"
     ]
    }
   ],
   "source": [
    "# Newton index back to normal\n",
    "\n",
    "print(len(index))\n",
    "print(index['نیوتون'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    file_name = destination + '/index.json'\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(index, json_file)\n",
    "\n",
    "save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    if not os.path.exists(source):\n",
    "        return \"No such directory\"\n",
    "    file_name = source + '/index.json'\n",
    "    global index\n",
    "    with open(file_name) as json_file:\n",
    "        index = json.load(json_file)\n",
    "\n",
    "load_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93786\n",
      "{'3415': {'text': [2400, 2410]}, '4058': {'text': [944]}, '4659': {'text': [1745]}, '5237': {'text': [4137]}, '6054': {'text': [1612]}, '7013': {'text': [797, 2202]}}\n"
     ]
    }
   ],
   "source": [
    "# Testing newton again to see if index is the same...\n",
    "\n",
    "print(len(index))\n",
    "print(index['نیوتون'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Document retreival, Scoring and VSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1- Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coeff(token_set1, token_set2):\n",
    "    intersection = [b for b in token_set1 if b in token_set2]\n",
    "    union = [b for b in token_set1 or b in token_set2]\n",
    "    return  len(intersection) / len(union) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['شلام', 'حالا', 'برسهان', 'درسک', 'شد']\n",
      "شلام ['غلام', 'آلام', 'شلال', 'لام', 'شلقم', 'کلام', 'سلام', 'شلاق', 'شلیم', 'بلام', 'شرام', 'شلغم', 'شام']\n",
      "برسهان ['برسلان', 'برسیان', 'برهان']\n",
      "درسک ['درسی', 'ارسک', 'دریک', 'درسر', 'درسبک', 'دیسک', 'درک', 'دراک', 'درست', 'پرسک', 'درس', 'دسک', 'درسش']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'شلاق حالا برهان پرسک شد'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-word (non-context sensitive) query correction based on jaccard coeeficient and edit distance\n",
    "\n",
    "def correct_query(query):\n",
    "    query_terms = prepare_text(query, stem_flag = False)\n",
    "    print(query_terms)\n",
    "    for term in query_terms:\n",
    "        if term not in bigram_dict:\n",
    "            marked_term = '$' + term + '$'\n",
    "            term_bigrams = [marked_term[i:i + 2] for i in range(0, len(marked_term) - 1)]\n",
    "            candidate_set = []\n",
    "\n",
    "            for b in term_bigrams:\n",
    "                candidate_set.extend(list(bigram_index[b]))\n",
    "            candidate_set = list(set(candidate_set))\n",
    "            \n",
    "            # filtering based on jaccard coefficient\n",
    "            jaccard_candidates = []\n",
    "            threshold = 0.5\n",
    "            for j in range(len(candidate_set)):\n",
    "                marked_c = '$' + candidate_set[j] + '$'\n",
    "                candidate_bigrams = [marked_c[i:i + 2] for i in range(0, len(marked_c) - 1)]\n",
    "                if jaccard_coeff(candidate_bigrams, term_bigrams) > threshold:\n",
    "                      jaccard_candidates.append(candidate_set[j]) \n",
    "                        \n",
    "            # filtering based on edit distance  \n",
    "            edit_distances = [editdistance.eval(jaccard_candidates[j], term) for j in range(len(jaccard_candidates))]\n",
    "            final_candidates = [jaccard_candidates[j] for j in range(len(jaccard_candidates)) if edit_distances[j] == min(edit_distances)]\n",
    "            idx = query_terms.index(term)\n",
    "            print(term, final_candidates)\n",
    "            query_terms[idx] = random.choice(final_candidates)\n",
    "            \n",
    "            \n",
    "    correct_query = ' '.join(query_terms)\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2- Phrasal (Exact) Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6824']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phrasal search based on zone(s)\n",
    "\n",
    "def phrasal_search(phrase, zones = ['title', 'text']):\n",
    "    phrase_terms = prepare_text(phrase)\n",
    "    if len(phrase_terms) == 1:\n",
    "        posting_list = get_posting_list(phrase)\n",
    "        if len(posting_list) > 0:\n",
    "            return list(posting_list.keys())\n",
    "        return []\n",
    "        \n",
    "    common_docs = get_posting_list(phrase_terms[0]).keys()\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for term in phrase_terms:\n",
    "        posting_list = index[term]\n",
    "        common_docs = list(set(common_docs) & set(index[term].keys()))\n",
    "    \n",
    "    for zone in zones:  \n",
    "        for doc in common_docs:\n",
    "            try:\n",
    "                for pos in index[phrase_terms[0]][doc][zone]:\n",
    "                    flag = True\n",
    "                    for i in range(1, len(phrase_terms)):\n",
    "                        if not (pos +  i) in index[phrase_terms[i]][doc][zone]:\n",
    "                            flag = False\n",
    "                            break\n",
    "                    if flag and doc not in relevant_docs:\n",
    "                        relevant_docs.append(doc)\n",
    "                        break           \n",
    "            except:\n",
    "                pass\n",
    "    return relevant_docs        \n",
    "            \n",
    "phrasal_search('نظرخواهی انجام شده توسط دانشگاه') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3- tf-idf Scoring and vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building static tf-idf matrixes\n",
    "\n",
    "def build_df_tf():\n",
    "    tf = {}\n",
    "    df = {}\n",
    "    for term, posting in index.items():\n",
    "        tf[term] = {}\n",
    "        df[term] = {}\n",
    "        df[term]['title'] = 0\n",
    "        df[term]['text'] = 0\n",
    "        \n",
    "        for doc in posting:\n",
    "            tf[term][doc] = {}\n",
    "            try:\n",
    "                tf[term][doc]['title'] = len(index[term][doc]['title'])\n",
    "                df[term]['title'] += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:    \n",
    "                tf[term][doc]['text'] = len(index[term][doc]['text'])\n",
    "                df[term]['text'] += 1\n",
    "            except:\n",
    "                pass\n",
    "    return tf, df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "df = {}\n",
    "tf, df = build_df_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3415': {'text': [2400, 2410]}, '4058': {'text': [944]}, '4659': {'text': [1745]}, '5237': {'text': [4137]}, '6054': {'text': [1612]}, '7013': {'text': [797, 2202]}}\n",
      "6\n",
      "{'text': 2}\n",
      "{'3415': {'text': 2}, '4058': {'text': 1}, '4659': {'text': 1}, '5237': {'text': 1}, '6054': {'text': 1}, '7013': {'text': 2}}\n",
      "{'title': 0, 'text': 6}\n"
     ]
    }
   ],
   "source": [
    "# tf-idf test\n",
    "\n",
    "word = 'نیوتون'\n",
    "print(index[word])\n",
    "print(len(index[word]))\n",
    "print(tf[word]['3415'])\n",
    "print(tf[word])\n",
    "print(df[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3197,\n",
       " 5509,\n",
       " 3039,\n",
       " 3099,\n",
       " 3747,\n",
       " 6694,\n",
       " 6749,\n",
       " 4321,\n",
       " 5293,\n",
       " 6915,\n",
       " 6824,\n",
       " 4838,\n",
       " 6772,\n",
       " 3777,\n",
       " 5508]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding |max_retrieved| docs which are most relevant to query\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2, max_retrieved=15):\n",
    "    relevant_docs = [] \n",
    "    scores = []\n",
    "    phrases = re.findall(r'\"(.*?)\"', query)\n",
    "    non_phrase = re.sub(r'\"(.*?)\"', ' ', query)\n",
    "    query_terms = prepare_text(query)\n",
    "    non_phrase_terms = prepare_text(non_phrase)\n",
    "    \n",
    "    for phrase in phrases + non_phrase_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase))\n",
    "            \n",
    "    relevant_docs = list(set(relevant_docs))\n",
    "    scores = [0] * len(relevant_docs)\n",
    "        \n",
    "    w_q = [0] * len(query_terms)\n",
    "    \n",
    "    # compute query weight vector\n",
    "    for i in range(len(query_terms)):\n",
    "        term = query_terms[i]\n",
    "        w_q[i] = math.log10(query_terms.count(term)) + 1\n",
    "\n",
    "    # cosine normalization\n",
    "    if method[2] == 'c': \n",
    "        w_q = w_q / np.linalg.norm(w_q)\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        w_d = [0] * len(query_terms)\n",
    "        for i in range(len(query_terms)):\n",
    "            term = query_terms[i]\n",
    "            w_td_title, w_td_text = 0, 0\n",
    "            n = len(doc_ids)\n",
    "            try:\n",
    "                w_td_title = (math.log10(tf[term][doc]['title']) + 1) * math.log10(n / df[term]['title'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                w_td_text = (math.log10(tf[term][doc]['text']) + 1) * math.log10(n / df[term]['text'])\n",
    "            except:\n",
    "                pass\n",
    "            w_d[i] = w_td_title * 2 + w_td_text\n",
    "            \n",
    "        doc_idx = relevant_docs.index(doc)\n",
    "        # cosine normalization\n",
    "        if method[2] == 'c':\n",
    "            w_d = w_d / np.linalg.norm(w_d)\n",
    "        scores[doc_idx] = np.dot(w_d, w_q)\n",
    "    \n",
    "    # find k top docs according to score with a heap\n",
    "    relevant_docs = heapq.nlargest(min(len(relevant_docs), max_retrieved), zip(scores, relevant_docs))\n",
    "    relevant_docs = [int(x) for (y, x) in relevant_docs]\n",
    "    return relevant_docs\n",
    "\n",
    "search('\"نظرخواهی انجام شده توسط دانشگاه\" شهر نیویورک', \"ltc-lnc\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3260,\n",
       " 4530,\n",
       " 6969,\n",
       " 6752,\n",
       " 5967,\n",
       " 7143,\n",
       " 6949,\n",
       " 5293,\n",
       " 3666,\n",
       " 5192,\n",
       " 4094,\n",
       " 3938,\n",
       " 3120,\n",
       " 3874,\n",
       " 6917]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching is done seperately in each zone\n",
    "\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\", max_retrieved=15):\n",
    "    relevant_docs = []\n",
    "    scores = []\n",
    "    title_phrases = re.findall(r'\"(.*?)\"', title_query)\n",
    "    text_phrases = re.findall(r'\"(.*?)\"', text_query)\n",
    "    title_non_phrase = re.sub(r'\"(.*?)\"', ' ', title_query)\n",
    "    text_non_phrase = re.sub(r'\"(.*?)\"', ' ', text_query)\n",
    "    non_phrase_title_terms = prepare_text(title_non_phrase)\n",
    "    non_phrase_text_terms = prepare_text(text_non_phrase)\n",
    "    \n",
    "    for phrase in title_phrases + non_phrase_title_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase, zones=['title']))\n",
    "        \n",
    "    for phrase in text_phrases + non_phrase_text_terms:\n",
    "        relevant_docs.extend(phrasal_search(phrase, zones=['text']))\n",
    "        \n",
    "    query_terms = prepare_text(title_query) + prepare_text(text_query)\n",
    "            \n",
    "    relevant_docs = list(set(relevant_docs))\n",
    "    scores = [0] * len(relevant_docs)\n",
    "    w_q = [0] * len(query_terms)\n",
    "    \n",
    "    # compute query weight vector\n",
    "    for i in range(len(query_terms)):\n",
    "        term = query_terms[i]\n",
    "        w_q[i] = math.log10(query_terms.count(term)) + 1\n",
    "\n",
    "    # cosine normalization\n",
    "    if method[2] == 'c': \n",
    "        w_q = w_q / np.linalg.norm(w_q)\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        w_d = [0] * len(query_terms)\n",
    "        for i in range(len(query_terms)):\n",
    "            term = query_terms[i]\n",
    "            w_td_title, w_td_text = 0, 0\n",
    "            n = len(doc_ids)\n",
    "            try:\n",
    "                w_td_title = (math.log10(tf[term][doc]['title']) + 1) * math.log10(n / df[term]['title'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                w_td_text = (math.log10(tf[term][doc]['text']) + 1) * math.log10(n / df[term]['text'])\n",
    "            except:\n",
    "                pass\n",
    "            w_d[i] = w_td_title * 2 + w_td_text\n",
    "            \n",
    "        doc_idx = relevant_docs.index(doc)\n",
    "        # cosine normalization\n",
    "        if method[2] == 'c':\n",
    "            w_d = w_d / np.linalg.norm(w_d)\n",
    "        scores[doc_idx] = np.dot(w_d, w_q)\n",
    "                  \n",
    "  \n",
    "    # find k top docs according to score with a heap\n",
    "    relevant_docs = heapq.nlargest(min(len(relevant_docs), max_retrieved), zip(scores, relevant_docs))\n",
    "    relevant_docs = [int(x) for (y, x) in relevant_docs]\n",
    "    return relevant_docs\n",
    "\n",
    "detailed_search('عجایب \"هفت‌گانه\"', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- IR System Evaluation with R-precision, F-measure, MAP and NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading test queries and their relevant documents\n",
    "\n",
    "def read_queries(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevance = [], []\n",
    "        for file in glob.glob('data/queries/*.txt'):\n",
    "            query = []\n",
    "            with open(file, encoding = 'utf8') as query_file:\n",
    "                for line in query_file:\n",
    "                    query.append(line)   \n",
    "            queries.append(query)  \n",
    "        for file in glob.glob('data/relevance/*.txt'):\n",
    "            with open(file, encoding = 'utf8') as relevance_file:\n",
    "                relevance.append([int(x) for x in relevance_file.read().split(',')]) \n",
    "        return queries, relevance   \n",
    "    else:\n",
    "        query = []\n",
    "        with open('data/queries/%s.txt'%(query_id,), encoding = 'utf8') as query_file:\n",
    "            for line in query_file:\n",
    "                query.append(line)\n",
    "        with open('data/relevance/%s.txt'%(query_id,)) as relevance_file:   \n",
    "            relevance = [int(x) for x in relevance_file.read().split(',')]\n",
    "        return query, relevance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for related documents\n",
    "\n",
    "def get_related_docs(query, max_retrieved=15):\n",
    "    if len(query) == 1:\n",
    "        return search(query[0], \"ltc-lnc\", max_retrieved=max_retrieved)\n",
    "    elif len(query) == 2:\n",
    "        return detailed_search(query[0], query[1], \"ltn-lnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the f-measure for relevant and retrieved docs\n",
    "\n",
    "def compute_f_measure(retrieved, relevant):\n",
    "    alpha = 0.5\n",
    "    p = len(list(set(retrieved) & set(relevant))) / len(retrieved)\n",
    "    r = len(list(set(retrieved) & set(relevant))) / len(relevant)\n",
    "    beta2 = (1 - alpha) / alpha\n",
    "    f_measure = (beta2 + 1) * p * r / (beta2 * p + r)\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the average precision for relevant and retrieved docs\n",
    "\n",
    "def compute_avg_precision(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1   \n",
    "    p = np.divide(np.cumsum(p) * p, np.arange(1, len(retrieved) + 1))\n",
    "    p = p[p > 0]\n",
    "    return np.mean(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the NDCG for relevant and retrieved docs\n",
    "\n",
    "def compute_ndcg(retrieved, relevant):\n",
    "    indices = [i for i in range(len(retrieved)) if retrieved[i] in relevant]\n",
    "    p = np.zeros(len(retrieved))\n",
    "    p[indices] = 1 \n",
    "    ideal = np.array([1] * int(np.sum(p)) + [0] * (len(p) - int(np.sum(p))))\n",
    "    dcg_rf = p[0] + sum(np.divide(p[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    dcg_gt = ideal[0] + sum(np.divide(ideal[1:], np.log2(np.arange(2, len(retrieved) + 1))))\n",
    "    ndcg = dcg_rf / dcg_gt\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6046041053192083\n",
      "0.5495403840954625\n",
      "0.706255989361921\n",
      "0.8119605069912865\n"
     ]
    }
   ],
   "source": [
    "def R_Precision(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        r_precision = []\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        for i in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[i], max_retrieved=len(relevants[i]))\n",
    "            r_precision.append(len(list(set(retrieved) & set(relevants[i]))) / len(relevants[i]))\n",
    "        r_precision = np.mean(r_precision)\n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query, max_retrieved=len(relevant))\n",
    "        r_precision = len(list(set(retrieved) & set(relevant))) / len(relevant)\n",
    "\n",
    "    return r_precision\n",
    "\n",
    "\n",
    "def F_measure(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        f_measures = []\n",
    "        for i in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[i])\n",
    "            f_measures.append(compute_f_measure(retrieved, relevants[i]))\n",
    "        f_measure = np.mean(f_measures)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        f_measure = compute_f_measure(retrieved, relevant)\n",
    "    \n",
    "    return f_measure\n",
    "  \n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        average_p = []\n",
    "        for j in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[j])\n",
    "            average_p.append(compute_avg_precision(retrieved, relevants[j]))            \n",
    "        map_value = np.mean(average_p)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        map_value = compute_avg_precision(retrieved, relevant)\n",
    "        \n",
    "    return map_value\n",
    "\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        queries, relevants = read_queries(query_id)\n",
    "        ndcg_values = []\n",
    "        for j in range(len(queries)):\n",
    "            retrieved = get_related_docs(queries[j])\n",
    "            ndcg_values.append(compute_ndcg(retrieved, relevants[j]))\n",
    "        ndcg_value = np.mean(ndcg_values)    \n",
    "    else:\n",
    "        query, relevant = read_queries(query_id)\n",
    "        retrieved = get_related_docs(query)\n",
    "        ndcg_value = compute_ndcg(retrieved, relevant)\n",
    "        \n",
    "    return ndcg_value\n",
    "\n",
    "\n",
    "print(R_Precision())\n",
    "print(F_measure())\n",
    "print(MAP())\n",
    "print(NDCG())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
