{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nMeRVcDMxQC"
   },
   "source": [
    "# Classification and Clustering of Text articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kLwEkjWMxQS"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_b1eQD1MxQW"
   },
   "outputs": [],
   "source": [
    "# importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import t\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rWbQSOjMxQw"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json('data/train.json')\n",
    "validation_data = pd.read_json('data/validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2056,
     "status": "ok",
     "timestamp": 1589546309331,
     "user": {
      "displayName": "Yasamin Tabatabaee",
      "photoUrl": "",
      "userId": "07445355406834081164"
     },
     "user_tz": -270
    },
    "id": "81N4v2NDMxQ7",
    "outputId": "48b84dbc-af30-4f9d-8c5e-3cd011e7318d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every day, cubicle-dwellers get up from their ...</td>\n",
       "      <td>4</td>\n",
       "      <td>MobileAccess Networks Strengthens Signals for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New 1.8-inch hard drives may boost battery lif...</td>\n",
       "      <td>4</td>\n",
       "      <td>Hitachi Drives Consumer Storage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A hearing into allegations of racism against t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cricket: Zim race probe halted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The prospect that a tropical storm and a hurri...</td>\n",
       "      <td>4</td>\n",
       "      <td>Simultaneous Tropical Storms are Very Rare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Second seed Jiri Novak and number three Guille...</td>\n",
       "      <td>2</td>\n",
       "      <td>NOVAK AND CANAS SET UP SHOWDOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  category  \\\n",
       "0  Every day, cubicle-dwellers get up from their ...         4   \n",
       "1  New 1.8-inch hard drives may boost battery lif...         4   \n",
       "2  A hearing into allegations of racism against t...         1   \n",
       "3  The prospect that a tropical storm and a hurri...         4   \n",
       "4  Second seed Jiri Novak and number three Guille...         2   \n",
       "\n",
       "                                               title  \n",
       "0  MobileAccess Networks Strengthens Signals for ...  \n",
       "1                    Hitachi Drives Consumer Storage  \n",
       "2                     Cricket: Zim race probe halted  \n",
       "3         Simultaneous Tropical Storms are Very Rare  \n",
       "4                    NOVAK AND CANAS SET UP SHOWDOWN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2018,
     "status": "ok",
     "timestamp": 1589546309335,
     "user": {
      "displayName": "Yasamin Tabatabaee",
      "photoUrl": "",
      "userId": "07445355406834081164"
     },
     "user_tz": -270
    },
    "id": "3CPsv_kJMxRH",
    "outputId": "b81c0785-517a-499e-b266-da4a978f6144"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The first targeted flyby of Titan occurs on Tu...</td>\n",
       "      <td>Titan flyby overview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Officials can not estimate all casualties as s...</td>\n",
       "      <td>Dubai terminal construction collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>A patch has been issued for the JpegOfDeath ho...</td>\n",
       "      <td>Will JpegOfDeath Help Slay Microsoft?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marsh  amp; McLennan Companies Inc., the insur...</td>\n",
       "      <td>Update 2: Marsh Seeks Incentive Fees for Settl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A Singaporean IT products distributor is intro...</td>\n",
       "      <td>PC distributor puts RFID tags in goods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               body  \\\n",
       "0         4  The first targeted flyby of Titan occurs on Tu...   \n",
       "1         1  Officials can not estimate all casualties as s...   \n",
       "2         4  A patch has been issued for the JpegOfDeath ho...   \n",
       "3         3  Marsh  amp; McLennan Companies Inc., the insur...   \n",
       "4         4  A Singaporean IT products distributor is intro...   \n",
       "\n",
       "                                               title  \n",
       "0                               Titan flyby overview  \n",
       "1               Dubai terminal construction collapse  \n",
       "2              Will JpegOfDeath Help Slay Microsoft?  \n",
       "3  Update 2: Marsh Seeks Incentive Fees for Settl...  \n",
       "4             PC distributor puts RFID tags in goods  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBKkh_AKMxRl"
   },
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    replace_punctuation = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return text.translate(replace_punctuation).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of terms in data set\n",
    "\n",
    "def get_vocabulary(data):\n",
    "    terms = set()\n",
    "    for doc_idx in data.index:\n",
    "        body_words = text_to_words(data.loc[doc_idx].body)\n",
    "        title_words = text_to_words(data.loc[doc_idx].title)\n",
    "        terms.update(body_words + title_words)\n",
    "    return list(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = get_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2Jf7diWR92k"
   },
   "outputs": [],
   "source": [
    "# Building static tf-idf dicts\n",
    "\n",
    "def build_tf(data, terms): \n",
    "    n = data.shape[0]\n",
    "    v = len(terms)\n",
    "    weight_matrix = np.zeros((n, v))\n",
    "    \n",
    "    for doc_idx in data.index:\n",
    "        doc = data.loc[doc_idx]\n",
    "        body_words = text_to_words(doc.body)\n",
    "        title_words = text_to_words(doc.title)\n",
    "        weight_matrix[doc_idx] = np.zeros(v)\n",
    "        for w in set(body_words + title_words):\n",
    "            if w in terms:\n",
    "                weight_matrix[doc_idx][terms.index(w)] = body_words.count(w) + title_words.count(w)\n",
    "    \n",
    "    return pd.DataFrame(weight_matrix, index=data.index, columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = build_tf(train_data[:100], terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat['target_category'] = train_data[:100]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solar</th>\n",
       "      <th>spls</th>\n",
       "      <th>kidney</th>\n",
       "      <th>odm</th>\n",
       "      <th>ultimatum</th>\n",
       "      <th>react</th>\n",
       "      <th>nanometer</th>\n",
       "      <th>beachfront</th>\n",
       "      <th>atypically</th>\n",
       "      <th>norway</th>\n",
       "      <th>...</th>\n",
       "      <th>filmmaker</th>\n",
       "      <th>starring</th>\n",
       "      <th>agencies</th>\n",
       "      <th>involvement</th>\n",
       "      <th>unmitigated</th>\n",
       "      <th>frontbridge</th>\n",
       "      <th>soon</th>\n",
       "      <th>snohomish</th>\n",
       "      <th>karadma</th>\n",
       "      <th>target_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 35611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    solar  spls  kidney  odm  ultimatum  react  nanometer  beachfront  \\\n",
       "0     0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "1     0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "2     0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "3     0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "4     0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "..    ...   ...     ...  ...        ...    ...        ...         ...   \n",
       "95    0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "96    0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "97    0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "98    0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "99    0.0   0.0     0.0  0.0        0.0    0.0        0.0         0.0   \n",
       "\n",
       "    atypically  norway  ...  filmmaker  starring  agencies  involvement  \\\n",
       "0          0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "1          0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "2          0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "3          0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "4          0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "..         ...     ...  ...        ...       ...       ...          ...   \n",
       "95         0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "96         0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "97         0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "98         0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "99         0.0     0.0  ...        0.0       0.0       0.0          0.0   \n",
       "\n",
       "    unmitigated  frontbridge  soon  snohomish  karadma  target_category  \n",
       "0           0.0          0.0   0.0        0.0      0.0                4  \n",
       "1           0.0          0.0   0.0        0.0      0.0                4  \n",
       "2           0.0          0.0   0.0        0.0      0.0                1  \n",
       "3           0.0          0.0   0.0        0.0      0.0                4  \n",
       "4           0.0          0.0   0.0        0.0      0.0                2  \n",
       "..          ...          ...   ...        ...      ...              ...  \n",
       "95          0.0          0.0   0.0        0.0      0.0                1  \n",
       "96          0.0          0.0   0.0        0.0      0.0                3  \n",
       "97          0.0          0.0   0.0        0.0      0.0                1  \n",
       "98          0.0          0.0   0.0        0.0      0.0                4  \n",
       "99          0.0          0.0   0.0        0.0      0.0                1  \n",
       "\n",
       "[100 rows x 35611 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = mat.groupby(['target_category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_category\n",
       "1     918.0\n",
       "2    1169.0\n",
       "3     868.0\n",
       "4    1064.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = build_tf(validation_data, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document frequency per term\n",
    "df = mat.astype(bool).sum(axis=0)\n",
    "df = np.log10(train_data.shape[0] / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqM1YOG2Cq_m"
   },
   "outputs": [],
   "source": [
    "x_train = df * x_train\n",
    "y_train = train_data['category']\n",
    "\n",
    "x_val = df * x_val\n",
    "y_val = validation_data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPXvODFxMxR7"
   },
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-LDfz6QalAB"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred, categories):\n",
    "    conf_mat = np.zeros((len(categories), len(categories)))\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(len(categories)):\n",
    "            conf_mat[i][j] = np.count_nonzero((y_pred == i) & (y_true == j).to_numpy())    \n",
    "    return pd.DataFrame(conf_mat, index=['Predicted ' + cat for cat in categories], columns=['True ' + cat for cat in categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    diff = (y_true == y_pred).to_numpy()\n",
    "    return np.count_nonzero(diff) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRHkMUm1aldr"
   },
   "outputs": [],
   "source": [
    "def precision_recall(y_true, y_pred, categories):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    recall = np.zeros(len(categories))\n",
    "    precision = np.zeros(len(categories))\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        precision[i] = conf_mat[i][i] / conf_mat.sum(axis=0)[i]\n",
    "        recall[i] = conf_mat[i][i] / conf_mat.sum(axis=1)[i]\n",
    "        \n",
    "    return recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9Qm8er8al2U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2FbSWowamGU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnAXq9ZmZ4EC"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehdWhnBX5irF"
   },
   "source": [
    "### Naive-Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Opl7jk3VHjq"
   },
   "source": [
    "#### Class Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZ1fLk6m575b"
   },
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "  def __init__(self, alpha):\n",
    "      self.alpha = alpha\n",
    "    \n",
    "  def predict(self, x_test):\n",
    "      test_pred = pd.DataFrame(columns = ['category'], index = x_test.index)\n",
    "      for doc_idx in x_test.index:\n",
    "        print(doc_idx)\n",
    "        doc = x_test.loc[doc_idx]\n",
    "        terms = list(set(text_to_words(doc.body) + text_to_words(doc.title)))         \n",
    "        scores = pd.DataFrame(index = self.class_probs.index)        \n",
    "        for c in self.class_probs.index:\n",
    "            scores.loc[c] = self.class_probs.loc[c] + self.term_probs[terms].sum(axis=1).loc[c]                      \n",
    "      return test_pred\n",
    "\n",
    "  def fit(self, tf_mat):\n",
    "      tf_mat.loc[:, tf_mat.columns != 'target_category'] = tf_mat.loc[:, tf_mat.columns != 'target_category'] + self.alpha  \n",
    "      self.class_probs = np.log10(tf_mat['target_category'].value_counts() / tf_mat['target_category'].shape[0]) \n",
    "      self.term_probs = np.log10(tf_mat.groupby(['target_category']).sum())\n",
    "      print(self.term_probs.head())\n",
    "      self.term_probs = self.term_probs.sub(pd.Series(np.log10(tf_mat.groupby(['target_category']).sum().sum(axis=1)).tolist(), index=self.term_probs.index))\n",
    "      print(self.term_probs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2q_d1yfTWQ3T"
   },
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = Naive_Bayes(alpha = 0.1)\n",
    "nb_model.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSvPO5OT5yIq"
   },
   "source": [
    "### K Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Opl7jk3VHjq"
   },
   "source": [
    "#### Class Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZ1fLk6m575b"
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "  def __init__(self, k, method):\n",
    "      self.k = k\n",
    "      self.method = method\n",
    "    \n",
    "\n",
    "  def predict(self, x_test):\n",
    "      test_pred = pd.DataFrame(columns = ['category'], index = x_test.index)\n",
    "      dist_matrix = self.distance_matrix(x_test)\n",
    "      \n",
    "      for i in range(0, len(x_test)):\n",
    "          neighbor_indexes = dist_matrix[i, :].argsort()[0:self.k] \n",
    "          majority = self.y.iloc[neighbor_indexes].mode()\n",
    "          test_pred.at[x_test.index[i], 'category'] = majority.at[0]\n",
    "\n",
    "      return test_pred\n",
    "\n",
    "\n",
    "  def distance_matrix(self, x_test):\n",
    "      if self.method == 'c':\n",
    "          return np.dot(self.x / np.linalg.norm(self.x), (x_test / np.linalg.norm(x_test)).T)\n",
    "\n",
    "      if self.method == 'e':\n",
    "          dist_matrix = np.zeros((x_test.shape[0], self.x.shape[0]))\n",
    "          dist_matrix = - 2 * np.dot(test_x, self.x.T).T\n",
    "          dist_matrix += np.diag((np.dot(x_test, x_test.T)))\n",
    "          dist_matrix = dist_matrix.T\n",
    "          dist_matrix += np.diag((np.dot(self.x, self.x.T)))\n",
    "          return np.sqrt(dist_matrix)\n",
    "\n",
    "\n",
    "  def fit(self, x, y):\n",
    "      self.x = x\n",
    "      self.y = y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2q_d1yfTWQ3T"
   },
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26FIrG5bVBjI"
   },
   "outputs": [],
   "source": [
    "knn_model = KNN(k=3, method='c')\n",
    "knn_model.fit(x_val, y_val)\n",
    "y_pred = knn_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 866,
     "status": "ok",
     "timestamp": 1589547290053,
     "user": {
      "displayName": "Yasamin Tabatabaee",
      "photoUrl": "",
      "userId": "07445355406834081164"
     },
     "user_tz": -270
    },
    "id": "NuYsS5kAZrlQ",
    "outputId": "ccdfbd60-981a-4046-8916-511aaebdb0e0"
   },
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSvPO5OT5yIq"
   },
   "source": [
    "### Preprocessing Effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [t for t in tokens if not t in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = PorterStemmer() \n",
    "    return [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = remove_stopwords(text_to_words(train_data.loc[0].body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\y.tabatabaee/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\y.tabatabaee\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\y.tabatabaee/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\y.tabatabaee\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c689edf5b138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-53298771d323>\u001b[0m in \u001b[0;36mlemmatize_text\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\y.tabatabaee/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\y.tabatabaee\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\y.tabatabaee\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatize_text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSvPO5OT5yIq"
   },
   "source": [
    "### SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyper-parameter tuning with GridSearchCV\n",
    "\n",
    "params = {'C': [1, 10]}\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model = GridSearchCV(estimator = svm_model, param_grid = params, cv = 5)\n",
    "\n",
    "svm_model.fit(x_train, y_train)\n",
    "print(\"Validation accuracy = \", svm_model.score(x_val, y_val))\n",
    "print(\"Best params = \", svm_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSvPO5OT5yIq"
   },
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = [100, 200, 300]\n",
    "max_depth = [5, 10, 15]\n",
    "\n",
    "params = {'n_estimators': n_estimators, \n",
    "          'max_depth': max_depth,}\n",
    "\n",
    "rf_m### Random Forest\n",
    "odel = RandomForestClassifier()\n",
    "rf_model = GridSearchCV(estimator = rf_model, param_grid = params, cv = 5)\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Validation accuracy = \", rf_model.score(x_val, y_val))\n",
    "print(\"Best params = \", rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(window=2, size=300)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification_Clustering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
